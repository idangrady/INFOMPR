{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras \n",
    "from tensorflow.keras.layers import Input, LSTM, Attention, Embedding, Dense, Concatenate, TimeDistributed   #Layers required to implement the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np   #Package for scientific computing and dealing with arrays\n",
    "import pandas as pd  #Package providing fast, flexible and expressive data structures\n",
    "import re            #re stands for RegularExpression providing full support for Perl-like Regular Expressions in Python\n",
    "from bs4 import BeautifulSoup   #Package for pulling data out of HTML and XML files\n",
    "from keras.preprocessing.sequence import pad_sequences  #For Padding the seqences to same length\n",
    "from nltk.corpus import stopwords   #For removing filler words\n",
    "from tensorflow.keras.layers import Input, LSTM, Attention, Embedding, Dense, Concatenate, TimeDistributed   #Layers required to implement the model\n",
    "from tensorflow.keras.models import Model  #Helps in grouping the layers into an object with training and inference features\n",
    "from tensorflow.keras.callbacks import EarlyStopping  #Allows training the model on large no. of training epochs & stop once the performance stops improving on validation dataset\n",
    "from os import listdir\n",
    "import string\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer \n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import RepeatVector\n",
    "\n",
    "from pickle import dump\n",
    "from pickle import load\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "\n",
    "from keras import backend as K \n",
    "from easydict import EasyDict as edict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded = False\n",
    "save = False\n",
    "clean = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded Stories 92578\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# load doc into memory\n",
    "def load_doc(filename):\n",
    "\t# open the file as read only\n",
    "\tfile = open(filename, encoding='utf-8')\n",
    "\t# read all text\n",
    "\ttext = file.read()\n",
    "\t# close the file\n",
    "\tfile.close()\n",
    "\treturn text\n",
    "\n",
    "# split a document into news story and highlights\n",
    "def split_story(doc):\n",
    "\t# find first highlight\n",
    "\tindex = doc.find('@highlight')\n",
    "\t# split into story and highlights\n",
    "\tstory, highlights = doc[:index], doc[index:].split('@highlight')\n",
    "\t# strip extra white space around each highlight\n",
    "\thighlights = [h.strip() for h in highlights if len(h) > 0]\n",
    "\treturn story, highlights\n",
    "\n",
    "# load all stories in a directory\n",
    "def load_stories(directory):\n",
    "\tstories = list()\n",
    "\tfor name in listdir(directory):\n",
    "\t\tfilename = directory + '/' + name\n",
    "\t\t# load document\n",
    "\t\tdoc = load_doc(filename)\n",
    "\t\t# split into story and highlights\n",
    "\t\tstory, highlights = split_story(doc)\n",
    "\t\t# store\n",
    "\t\tstories.append({'story':story, 'highlights':highlights})\n",
    "\treturn stories\n",
    "\n",
    "# clean a list of lines\n",
    "def clean_lines(lines):\n",
    "\tcleaned = list()\n",
    "\t# prepare a translation table to remove punctuation\n",
    "\ttable = str.maketrans('', '', string.punctuation)\n",
    "\tfor line in lines:\n",
    "\t\t# strip source cnn office if it exists\n",
    "\t\tindex = line.find('(CNN) -- ')\n",
    "\t\tif index > -1:\n",
    "\t\t\tline = line[index+len('(CNN)'):]\n",
    "\t\t# tokenize on white space\n",
    "\t\tline = line.split()\n",
    "\t\t# convert to lower case\n",
    "\t\tline = [word.lower() for word in line]\n",
    "\t\t# remove punctuation from each token\n",
    "\t\tline = [w.translate(table) for w in line]\n",
    "\t\t# remove tokens with numbers in them\n",
    "\t\tline = [word for word in line if word.isalpha()]\n",
    "\t\t# store as string\n",
    "\t\tcleaned.append(' '.join(line))\n",
    "\t# remove empty strings\n",
    "\tcleaned = [c for c in cleaned if len(c) > 0]\n",
    "\treturn cleaned\n",
    "\n",
    "\n",
    "if clean:\n",
    "# load stories\n",
    "\tdirectory = 'cnn/'\n",
    "\tstories = load_stories(directory)\n",
    "\tprint('Loaded Stories %d' % len(stories))\n",
    "\n",
    "\t# clean stories\n",
    "\tfor example in stories:\n",
    "\t\texample['story'] = clean_lines(example['story'].split('\\n'))\n",
    "\t\texample['highlights'] = clean_lines(example['highlights'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded Stories 92578\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if save:\n",
    "    dump(stories, open('cnn_dataset.pkl', 'wb'))\n",
    "\n",
    "\n",
    "# load from file\n",
    "if loaded:\n",
    "    stories = load(open('cnn_dataset.pkl', 'rb'))\n",
    "    print('Loaded Stories %d' % len(stories))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preprocessing\n",
    "\n",
    "#This the dictionary used for expanding contractions\n",
    "contraction_mapping = {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"'cause\": \"because\", \"could've\": \"could have\", \"couldn't\": \"could not\",\n",
    "\n",
    "                           \"didn't\": \"did not\", \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \"hasn't\": \"has not\", \"haven't\": \"have not\",\n",
    "\n",
    "                           \"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\",\n",
    "\n",
    "                           \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \"i'd\": \"i would\",\n",
    "\n",
    "                           \"i'd've\": \"i would have\", \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\",\n",
    "\n",
    "                           \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\",\n",
    "\n",
    "                           \"mayn't\": \"may not\", \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\", \"must've\": \"must have\",\n",
    "\n",
    "                           \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\",\n",
    "\n",
    "                           \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\",\n",
    "\n",
    "                           \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\", \"she's\": \"she is\",\n",
    "\n",
    "                           \"should've\": \"should have\", \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\"so's\": \"so as\",\n",
    "\n",
    "                           \"this's\": \"this is\",\"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\", \"there'd\": \"there would\",\n",
    "\n",
    "                           \"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\",\"they'd\": \"they would\", \"they'd've\": \"they would have\",\n",
    "\n",
    "                           \"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\",\n",
    "\n",
    "                           \"wasn't\": \"was not\", \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\",\n",
    "\n",
    "                           \"we've\": \"we have\", \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\",\n",
    "\n",
    "                           \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\",\n",
    "\n",
    "                           \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\",\n",
    "\n",
    "                           \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \"won't've\": \"will not have\",\n",
    "\n",
    "                           \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \"y'all\": \"you all\",\n",
    "\n",
    "                           \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\n",
    "\n",
    "                           \"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\",\n",
    "\n",
    "                           \"you're\": \"you are\", \"you've\": \"you have\"}\n",
    "\n",
    "#Text Cleaning\n",
    "\n",
    "stop_words = set(stopwords.words('english')) \n",
    "def text_cleaner(text,num):\n",
    "    newString = text.lower()  #converts all uppercase characters in the string into lowercase characters and returns it\n",
    "    newString = BeautifulSoup(newString, \"lxml\").text #parses the string into an lxml.html \n",
    "    newString = re.sub(r'\\([^)]*\\)', '', newString) #used to replace a string that matches a regular expression instead of perfect match\n",
    "    newString = re.sub('\"','', newString)           \n",
    "    newString = ' '.join([contraction_mapping[t] if t in contraction_mapping else t for t in newString.split(\" \")]) #for expanding contractions using the contraction_mapping dictionary    \n",
    "    newString = re.sub(r\"'s\\b\",\"\",newString)\n",
    "    newString = re.sub(\"[^a-zA-Z]\", \" \", newString)\n",
    "    if(num==0): \n",
    "      tokens = [w for w in newString.split() if not w in stop_words]  #converting the strings into tokens\n",
    "    else :\n",
    "      tokens = newString.split()\n",
    "    long_words=[]\n",
    "    for i in tokens:\n",
    "        if len(i)>1:                  #removing short words\n",
    "            long_words.append(i)   \n",
    "    return (\" \".join(long_words)).strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean = True\n",
    "if clean:\n",
    "#Calling the function\n",
    "    x_vocab = edict()\n",
    "    y_vocab = edict()\n",
    "\n",
    "    article_word_count = []\n",
    "    abstract_word_count = []\n",
    "\n",
    "    cleaned_text = []\n",
    "    highlights = []\n",
    "\n",
    "    max_ar_length = 0\n",
    "    max_high_lengh = 0\n",
    "\n",
    "    for file in stories:\n",
    "        t = file['story']\n",
    "        story =\"\"\n",
    "        highligh= \"\"\n",
    "        h = file['highlights']\n",
    "        for line in t:\n",
    "            story= story+line\n",
    "            # for word in line: \n",
    "            #     if word not in x_vocab.keys():\n",
    "            #         x_vocab[word] =1\n",
    "            #     else: \n",
    "            #         x_vocab[word]+=1\n",
    "\n",
    "        if len(story.split())> max_ar_length: max_ar_length =len(story.split())\n",
    "        article_word_count.append(len(story.split()))\n",
    "        cleaned_text.append(text_cleaner(story,0))\n",
    "        for line in h:\n",
    "            highligh= highligh+line\n",
    "            # if word not in y_vocab.keys():\n",
    "            #     y_vocab[word] =1\n",
    "            # else: \n",
    "            #     y_vocab[word]+=1\n",
    "\n",
    "        if len(highligh.split())> max_high_lengh: max_high_lengh =len(highligh.split())\n",
    "        abstract_word_count.append(len(highligh.split()))\n",
    "        highlights.append(highligh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Max length sequence story: 2050\n",
      " Max length sequence story: 103\n"
     ]
    }
   ],
   "source": [
    "# print summer: \n",
    "print(f\" Max length sequence story: {max_ar_length}\")\n",
    "print(f\" Max length sequence story: {max_high_lengh}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if save:\n",
    "    dump([x_vocab, y_vocab,article_word_count,abstract_word_count ,cleaned_text,  highlights, max_ar_length, max_high_lengh], open('cleaned_data_set.pkl', 'wb'))\n",
    "\n",
    "# load from file\n",
    "if loaded:\n",
    "    #stories = load(open('cnn_dataset.pkl', 'rb'))\n",
    "    #print('Loaded Stories %d' % len(stories))\n",
    "    x_vocab, y_vocab,article_word_count,abstract_word_count ,cleaned_text,  highlights, max_ar_length, max_high_lengh =  load(open('cleaned_data_set.pkl', 'rb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "876"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len((cleaned_text[0].split()))+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tokenizer = Tokenizer(len(cleaned_text[0].split())+1)\n",
    "X_tokenizer.fit_on_texts(cleaned_text[0].split())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(highlights[0].split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_tokenizer = Tokenizer(len(set(highlights[0].split()))) \n",
    "Y_tokenizer.fit_on_texts(highlights[0].split())\n",
    "Y_voc = Y_tokenizer.num_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_to_idx(data: list) -> dict:\n",
    "    \"\"\"\n",
    "    Function that maps the data and return a dictionary of words corresponding to their index\n",
    "    it gets a list\n",
    "    return:\n",
    "        dict 1 idx to word\n",
    "        dict 2 word to idx\n",
    "    \"\"\"\n",
    "    total_letters = [letters for sublist in data for subsublist in sublist for letters in subsublist]\n",
    "    unique_letters =set(total_letters)\n",
    "    total_words = [word.replace(',','') for sublist in data for subsublist in sublist for word in subsublist.split()]\n",
    "    unique_words =list(set(total_words))\n",
    "\n",
    "    w_2_i = {unique_words[i]:i for i in range(len(unique_words))}\n",
    "    i_2_w= {i: unique_words[i] for i in range(len(unique_words))}\n",
    "    print(w_2_i)\n",
    "    input()\n",
    "    return (w_2_i, i_2_w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train= X_tokenizer.texts_to_sequences(cleaned_text[0].split()) \n",
    "y_train  = Y_tokenizer.texts_to_sequences(highlights[0])\n",
    "X_voc = X_tokenizer.num_words\n",
    "\n",
    "type(highlights[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_to_array(list_, shape):\n",
    "    return np.array([x for y in list_ for x in y]).reshape(shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = flatten_to_array(X_train, (1,-1))\n",
    "y =flatten_to_array(y_train, (1,-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_voc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 875)"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 875)]        0           []                               \n",
      "                                                                                                  \n",
      " input_2 (InputLayer)           [(None, 228)]        0           []                               \n",
      "                                                                                                  \n",
      " embedding (Embedding)          (None, 875, 100)     100000      ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " embedding_1 (Embedding)        (None, 228, 100)     4300        ['input_2[0][0]']                \n",
      "                                                                                                  \n",
      " lstm (LSTM)                    [(None, 875, 100),   80400       ['embedding[0][0]']              \n",
      "                                 (None, 100),                                                     \n",
      "                                 (None, 100)]                                                     \n",
      "                                                                                                  \n",
      " lstm_1 (LSTM)                  [(None, 228, 100),   80400       ['embedding_1[0][0]',            \n",
      "                                 (None, 100),                     'lstm[0][1]',                   \n",
      "                                 (None, 100)]                     'lstm[0][2]']                   \n",
      "                                                                                                  \n",
      " time_distributed (TimeDistribu  (None, 228, 42)     4242        ['lstm_1[0][0]']                 \n",
      " ted)                                                                                             \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 269,342\n",
      "Trainable params: 269,342\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "('You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) ', 'for plot_model/model_to_dot to work.')\n",
      "Epoch 1/200\n",
      "1/1 [==============================] - 7s 7s/step - loss: 3.7394 - accuracy: 0.0439\n",
      "Epoch 2/200\n",
      "1/1 [==============================] - 0s 340ms/step - loss: 3.7118 - accuracy: 0.2193\n",
      "Epoch 3/200\n",
      "1/1 [==============================] - 0s 358ms/step - loss: 3.6834 - accuracy: 0.2105\n",
      "Epoch 4/200\n",
      "1/1 [==============================] - 0s 365ms/step - loss: 3.6236 - accuracy: 0.2105\n",
      "Epoch 5/200\n",
      "1/1 [==============================] - 0s 369ms/step - loss: 3.1504 - accuracy: 0.1404\n",
      "Epoch 6/200\n",
      "1/1 [==============================] - 0s 346ms/step - loss: 2.9815 - accuracy: 0.1316\n",
      "Epoch 7/200\n",
      "1/1 [==============================] - 0s 361ms/step - loss: 2.8846 - accuracy: 0.1140\n",
      "Epoch 8/200\n",
      "1/1 [==============================] - 0s 357ms/step - loss: 2.8438 - accuracy: 0.1404\n",
      "Epoch 9/200\n",
      "1/1 [==============================] - 0s 335ms/step - loss: 2.8146 - accuracy: 0.2149\n",
      "Epoch 10/200\n",
      "1/1 [==============================] - 0s 363ms/step - loss: 2.7906 - accuracy: 0.1535\n",
      "Epoch 11/200\n",
      "1/1 [==============================] - 0s 334ms/step - loss: 2.7700 - accuracy: 0.1491\n",
      "Epoch 12/200\n",
      "1/1 [==============================] - 0s 330ms/step - loss: 2.7526 - accuracy: 0.1623\n",
      "Epoch 13/200\n",
      "1/1 [==============================] - 0s 340ms/step - loss: 2.7380 - accuracy: 0.1579\n",
      "Epoch 14/200\n",
      "1/1 [==============================] - 0s 372ms/step - loss: 2.7273 - accuracy: 0.2456\n",
      "Epoch 15/200\n",
      "1/1 [==============================] - 0s 331ms/step - loss: 2.7107 - accuracy: 0.2456\n",
      "Epoch 16/200\n",
      "1/1 [==============================] - 0s 334ms/step - loss: 2.7013 - accuracy: 0.1535\n",
      "Epoch 17/200\n",
      "1/1 [==============================] - 0s 338ms/step - loss: 2.6970 - accuracy: 0.1316\n",
      "Epoch 18/200\n",
      "1/1 [==============================] - 0s 349ms/step - loss: 2.6810 - accuracy: 0.1535\n",
      "Epoch 19/200\n",
      "1/1 [==============================] - 0s 332ms/step - loss: 2.6626 - accuracy: 0.2281\n",
      "Epoch 20/200\n",
      "1/1 [==============================] - 0s 334ms/step - loss: 2.6408 - accuracy: 0.1535\n",
      "Epoch 21/200\n",
      "1/1 [==============================] - 0s 330ms/step - loss: 2.6247 - accuracy: 0.3421\n",
      "Epoch 22/200\n",
      "1/1 [==============================] - 0s 332ms/step - loss: 2.6149 - accuracy: 0.1798\n",
      "Epoch 23/200\n",
      "1/1 [==============================] - 0s 329ms/step - loss: 2.6108 - accuracy: 0.3026\n",
      "Epoch 24/200\n",
      "1/1 [==============================] - 0s 330ms/step - loss: 2.6042 - accuracy: 0.2105\n",
      "Epoch 25/200\n",
      "1/1 [==============================] - 0s 397ms/step - loss: 2.5716 - accuracy: 0.3860\n",
      "Epoch 26/200\n",
      "1/1 [==============================] - 0s 330ms/step - loss: 2.5455 - accuracy: 0.2675\n",
      "Epoch 27/200\n",
      "1/1 [==============================] - 0s 347ms/step - loss: 2.5220 - accuracy: 0.3904\n",
      "Epoch 28/200\n",
      "1/1 [==============================] - 0s 330ms/step - loss: 2.5133 - accuracy: 0.2412\n",
      "Epoch 29/200\n",
      "1/1 [==============================] - 0s 332ms/step - loss: 2.5527 - accuracy: 0.2193\n",
      "Epoch 30/200\n",
      "1/1 [==============================] - 0s 329ms/step - loss: 2.5005 - accuracy: 0.2982\n",
      "Epoch 31/200\n",
      "1/1 [==============================] - 0s 330ms/step - loss: 2.4746 - accuracy: 0.3553\n",
      "Epoch 32/200\n",
      "1/1 [==============================] - 0s 328ms/step - loss: 2.4414 - accuracy: 0.3553\n",
      "Epoch 33/200\n",
      "1/1 [==============================] - 0s 332ms/step - loss: 2.4168 - accuracy: 0.3947\n",
      "Epoch 34/200\n",
      "1/1 [==============================] - 0s 331ms/step - loss: 2.4058 - accuracy: 0.3246\n",
      "Epoch 35/200\n",
      "1/1 [==============================] - 0s 329ms/step - loss: 2.4059 - accuracy: 0.3289\n",
      "Epoch 36/200\n",
      "1/1 [==============================] - 0s 349ms/step - loss: 2.4448 - accuracy: 0.2719\n",
      "Epoch 37/200\n",
      "1/1 [==============================] - 0s 332ms/step - loss: 2.4186 - accuracy: 0.3509\n",
      "Epoch 38/200\n",
      "1/1 [==============================] - 0s 329ms/step - loss: 2.3292 - accuracy: 0.3728\n",
      "Epoch 39/200\n",
      "1/1 [==============================] - 0s 330ms/step - loss: 2.3132 - accuracy: 0.3596\n",
      "Epoch 40/200\n",
      "1/1 [==============================] - 0s 330ms/step - loss: 2.3096 - accuracy: 0.3596\n",
      "Epoch 41/200\n",
      "1/1 [==============================] - 0s 329ms/step - loss: 2.3018 - accuracy: 0.3684\n",
      "Epoch 42/200\n",
      "1/1 [==============================] - 0s 330ms/step - loss: 2.2580 - accuracy: 0.4211\n",
      "Epoch 43/200\n",
      "1/1 [==============================] - 0s 333ms/step - loss: 2.2280 - accuracy: 0.4211\n",
      "Epoch 44/200\n",
      "1/1 [==============================] - 0s 328ms/step - loss: 2.2065 - accuracy: 0.4167\n",
      "Epoch 45/200\n",
      "1/1 [==============================] - 0s 349ms/step - loss: 2.1878 - accuracy: 0.4430\n",
      "Epoch 46/200\n",
      "1/1 [==============================] - 0s 330ms/step - loss: 2.1777 - accuracy: 0.4035\n",
      "Epoch 47/200\n",
      "1/1 [==============================] - 0s 329ms/step - loss: 2.1765 - accuracy: 0.4342\n",
      "Epoch 48/200\n",
      "1/1 [==============================] - 0s 339ms/step - loss: 2.1499 - accuracy: 0.4123\n",
      "Epoch 49/200\n",
      "1/1 [==============================] - 0s 332ms/step - loss: 2.1189 - accuracy: 0.4693\n",
      "Epoch 50/200\n",
      "1/1 [==============================] - 0s 331ms/step - loss: 2.0911 - accuracy: 0.4430\n",
      "Epoch 51/200\n",
      "1/1 [==============================] - 0s 330ms/step - loss: 2.0663 - accuracy: 0.4825\n",
      "Epoch 52/200\n",
      "1/1 [==============================] - 0s 338ms/step - loss: 2.0439 - accuracy: 0.4605\n",
      "Epoch 53/200\n",
      "1/1 [==============================] - 0s 335ms/step - loss: 2.0238 - accuracy: 0.5088\n",
      "Epoch 54/200\n",
      "1/1 [==============================] - 0s 349ms/step - loss: 2.0118 - accuracy: 0.4868\n",
      "Epoch 55/200\n",
      "1/1 [==============================] - 0s 332ms/step - loss: 2.0041 - accuracy: 0.5088\n",
      "Epoch 56/200\n",
      "1/1 [==============================] - 0s 333ms/step - loss: 1.9826 - accuracy: 0.4956\n",
      "Epoch 57/200\n",
      "1/1 [==============================] - 0s 332ms/step - loss: 1.9395 - accuracy: 0.5132\n",
      "Epoch 58/200\n",
      "1/1 [==============================] - 0s 328ms/step - loss: 1.9245 - accuracy: 0.5263\n",
      "Epoch 59/200\n",
      "1/1 [==============================] - 0s 329ms/step - loss: 1.8981 - accuracy: 0.5088\n",
      "Epoch 60/200\n",
      "1/1 [==============================] - 0s 331ms/step - loss: 1.8880 - accuracy: 0.5746\n",
      "Epoch 61/200\n",
      "1/1 [==============================] - 0s 338ms/step - loss: 1.8606 - accuracy: 0.5044\n",
      "Epoch 62/200\n",
      "1/1 [==============================] - 0s 368ms/step - loss: 1.8340 - accuracy: 0.6096\n",
      "Epoch 63/200\n",
      "1/1 [==============================] - 0s 382ms/step - loss: 1.7962 - accuracy: 0.5526\n",
      "Epoch 64/200\n",
      "1/1 [==============================] - 0s 332ms/step - loss: 1.7692 - accuracy: 0.6184\n",
      "Epoch 65/200\n",
      "1/1 [==============================] - 0s 331ms/step - loss: 1.7461 - accuracy: 0.5702\n",
      "Epoch 66/200\n",
      "1/1 [==============================] - 0s 336ms/step - loss: 1.7271 - accuracy: 0.6447\n",
      "Epoch 67/200\n",
      "1/1 [==============================] - 0s 333ms/step - loss: 1.7096 - accuracy: 0.5789\n",
      "Epoch 68/200\n",
      "1/1 [==============================] - 0s 354ms/step - loss: 1.6894 - accuracy: 0.6623\n",
      "Epoch 69/200\n",
      "1/1 [==============================] - 0s 354ms/step - loss: 1.6628 - accuracy: 0.6009\n",
      "Epoch 70/200\n",
      "1/1 [==============================] - 0s 396ms/step - loss: 1.6343 - accuracy: 0.6623\n",
      "Epoch 71/200\n",
      "1/1 [==============================] - 0s 348ms/step - loss: 1.6058 - accuracy: 0.6272\n",
      "Epoch 72/200\n",
      "1/1 [==============================] - 0s 329ms/step - loss: 1.5801 - accuracy: 0.6535\n",
      "Epoch 73/200\n",
      "1/1 [==============================] - 0s 331ms/step - loss: 1.5504 - accuracy: 0.6491\n",
      "Epoch 74/200\n",
      "1/1 [==============================] - 0s 332ms/step - loss: 1.5272 - accuracy: 0.6579\n",
      "Epoch 75/200\n",
      "1/1 [==============================] - 0s 328ms/step - loss: 1.4970 - accuracy: 0.6711\n",
      "Epoch 76/200\n",
      "1/1 [==============================] - 0s 331ms/step - loss: 1.4702 - accuracy: 0.6842\n",
      "Epoch 77/200\n",
      "1/1 [==============================] - 0s 329ms/step - loss: 1.4427 - accuracy: 0.6842\n",
      "Epoch 78/200\n",
      "1/1 [==============================] - 0s 330ms/step - loss: 1.4169 - accuracy: 0.6930\n",
      "Epoch 79/200\n",
      "1/1 [==============================] - 0s 331ms/step - loss: 1.3912 - accuracy: 0.7149\n",
      "Epoch 80/200\n",
      "1/1 [==============================] - 0s 343ms/step - loss: 1.3691 - accuracy: 0.7105\n",
      "Epoch 81/200\n",
      "1/1 [==============================] - 0s 329ms/step - loss: 1.3511 - accuracy: 0.7193\n",
      "Epoch 82/200\n",
      "1/1 [==============================] - 0s 331ms/step - loss: 1.3354 - accuracy: 0.7193\n",
      "Epoch 83/200\n",
      "1/1 [==============================] - 0s 330ms/step - loss: 1.3176 - accuracy: 0.7237\n",
      "Epoch 84/200\n",
      "1/1 [==============================] - 0s 333ms/step - loss: 1.2726 - accuracy: 0.7325\n",
      "Epoch 85/200\n",
      "1/1 [==============================] - 0s 332ms/step - loss: 1.2400 - accuracy: 0.7544\n",
      "Epoch 86/200\n",
      "1/1 [==============================] - 0s 330ms/step - loss: 1.2135 - accuracy: 0.7675\n",
      "Epoch 87/200\n",
      "1/1 [==============================] - 0s 329ms/step - loss: 1.1908 - accuracy: 0.7544\n",
      "Epoch 88/200\n",
      "1/1 [==============================] - 0s 332ms/step - loss: 1.1812 - accuracy: 0.8158\n",
      "Epoch 89/200\n",
      "1/1 [==============================] - 0s 346ms/step - loss: 1.1598 - accuracy: 0.7368\n",
      "Epoch 90/200\n",
      "1/1 [==============================] - 0s 333ms/step - loss: 1.1393 - accuracy: 0.8202\n",
      "Epoch 91/200\n",
      "1/1 [==============================] - 0s 332ms/step - loss: 1.1023 - accuracy: 0.7895\n",
      "Epoch 92/200\n",
      "1/1 [==============================] - 0s 334ms/step - loss: 1.0756 - accuracy: 0.8158\n",
      "Epoch 93/200\n",
      "1/1 [==============================] - 0s 331ms/step - loss: 1.0513 - accuracy: 0.8246\n",
      "Epoch 94/200\n",
      "1/1 [==============================] - 0s 331ms/step - loss: 1.0244 - accuracy: 0.8377\n",
      "Epoch 95/200\n",
      "1/1 [==============================] - 0s 332ms/step - loss: 1.0010 - accuracy: 0.8596\n",
      "Epoch 96/200\n",
      "1/1 [==============================] - 0s 333ms/step - loss: 0.9807 - accuracy: 0.8465\n",
      "Epoch 97/200\n",
      "1/1 [==============================] - 0s 331ms/step - loss: 0.9661 - accuracy: 0.8728\n",
      "Epoch 98/200\n",
      "1/1 [==============================] - 0s 350ms/step - loss: 0.9485 - accuracy: 0.8377\n",
      "Epoch 99/200\n",
      "1/1 [==============================] - 0s 330ms/step - loss: 0.9192 - accuracy: 0.8904\n",
      "Epoch 100/200\n",
      "1/1 [==============================] - 0s 329ms/step - loss: 0.8937 - accuracy: 0.8509\n",
      "Epoch 101/200\n",
      "1/1 [==============================] - 0s 329ms/step - loss: 0.8690 - accuracy: 0.8947\n",
      "Epoch 102/200\n",
      "1/1 [==============================] - 0s 330ms/step - loss: 0.8486 - accuracy: 0.8728\n",
      "Epoch 103/200\n",
      "1/1 [==============================] - 0s 331ms/step - loss: 0.8270 - accuracy: 0.8904\n",
      "Epoch 104/200\n",
      "1/1 [==============================] - 0s 329ms/step - loss: 0.8069 - accuracy: 0.8860\n",
      "Epoch 105/200\n",
      "1/1 [==============================] - 0s 332ms/step - loss: 0.7859 - accuracy: 0.8991\n",
      "Epoch 106/200\n",
      "1/1 [==============================] - 0s 329ms/step - loss: 0.7643 - accuracy: 0.8947\n",
      "Epoch 107/200\n",
      "1/1 [==============================] - 0s 349ms/step - loss: 0.7453 - accuracy: 0.9123\n",
      "Epoch 108/200\n",
      "1/1 [==============================] - 0s 332ms/step - loss: 0.7279 - accuracy: 0.9211\n",
      "Epoch 109/200\n",
      "1/1 [==============================] - 0s 333ms/step - loss: 0.7153 - accuracy: 0.9211\n",
      "Epoch 110/200\n",
      "1/1 [==============================] - 0s 329ms/step - loss: 0.6955 - accuracy: 0.9254\n",
      "Epoch 111/200\n",
      "1/1 [==============================] - 0s 330ms/step - loss: 0.6706 - accuracy: 0.9254\n",
      "Epoch 112/200\n",
      "1/1 [==============================] - 0s 329ms/step - loss: 0.6478 - accuracy: 0.9386\n",
      "Epoch 113/200\n",
      "1/1 [==============================] - 0s 329ms/step - loss: 0.6300 - accuracy: 0.9386\n",
      "Epoch 114/200\n",
      "1/1 [==============================] - 0s 328ms/step - loss: 0.6128 - accuracy: 0.9474\n",
      "Epoch 115/200\n",
      "1/1 [==============================] - 0s 331ms/step - loss: 0.5981 - accuracy: 0.9430\n",
      "Epoch 116/200\n",
      "1/1 [==============================] - 0s 348ms/step - loss: 0.5784 - accuracy: 0.9561\n",
      "Epoch 117/200\n",
      "1/1 [==============================] - 0s 332ms/step - loss: 0.5617 - accuracy: 0.9430\n",
      "Epoch 118/200\n",
      "1/1 [==============================] - 0s 331ms/step - loss: 0.5451 - accuracy: 0.9605\n",
      "Epoch 119/200\n",
      "1/1 [==============================] - 0s 355ms/step - loss: 0.5297 - accuracy: 0.9605\n",
      "Epoch 120/200\n",
      "1/1 [==============================] - 0s 333ms/step - loss: 0.5134 - accuracy: 0.9693\n",
      "Epoch 121/200\n",
      "1/1 [==============================] - 0s 329ms/step - loss: 0.4969 - accuracy: 0.9649\n",
      "Epoch 122/200\n",
      "1/1 [==============================] - 0s 335ms/step - loss: 0.4804 - accuracy: 0.9693\n",
      "Epoch 123/200\n",
      "1/1 [==============================] - 0s 332ms/step - loss: 0.4650 - accuracy: 0.9737\n",
      "Epoch 124/200\n",
      "1/1 [==============================] - 0s 331ms/step - loss: 0.4527 - accuracy: 0.9693\n",
      "Epoch 125/200\n",
      "1/1 [==============================] - 0s 352ms/step - loss: 0.4476 - accuracy: 0.9781\n",
      "Epoch 126/200\n",
      "1/1 [==============================] - 0s 328ms/step - loss: 0.4283 - accuracy: 0.9737\n",
      "Epoch 127/200\n",
      "1/1 [==============================] - 0s 330ms/step - loss: 0.4085 - accuracy: 0.9868\n",
      "Epoch 128/200\n",
      "1/1 [==============================] - 0s 332ms/step - loss: 0.3943 - accuracy: 0.9912\n",
      "Epoch 129/200\n",
      "1/1 [==============================] - 0s 328ms/step - loss: 0.3803 - accuracy: 0.9825\n",
      "Epoch 130/200\n",
      "1/1 [==============================] - 0s 393ms/step - loss: 0.3670 - accuracy: 0.9912\n",
      "Epoch 131/200\n",
      "1/1 [==============================] - 0s 338ms/step - loss: 0.3556 - accuracy: 0.9825\n",
      "Epoch 132/200\n",
      "1/1 [==============================] - 0s 336ms/step - loss: 0.3446 - accuracy: 0.9912\n",
      "Epoch 133/200\n",
      "1/1 [==============================] - 0s 330ms/step - loss: 0.3346 - accuracy: 0.9868\n",
      "Epoch 134/200\n",
      "1/1 [==============================] - 0s 350ms/step - loss: 0.3223 - accuracy: 1.0000\n",
      "Epoch 135/200\n",
      "1/1 [==============================] - 0s 329ms/step - loss: 0.3111 - accuracy: 1.0000\n",
      "Epoch 136/200\n",
      "1/1 [==============================] - 0s 333ms/step - loss: 0.2990 - accuracy: 1.0000\n",
      "Epoch 137/200\n",
      "1/1 [==============================] - 0s 333ms/step - loss: 0.2875 - accuracy: 1.0000\n",
      "Epoch 138/200\n",
      "1/1 [==============================] - 0s 330ms/step - loss: 0.2768 - accuracy: 1.0000\n",
      "Epoch 139/200\n",
      "1/1 [==============================] - 0s 331ms/step - loss: 0.2666 - accuracy: 1.0000\n",
      "Epoch 140/200\n",
      "1/1 [==============================] - 0s 331ms/step - loss: 0.2571 - accuracy: 1.0000\n",
      "Epoch 141/200\n",
      "1/1 [==============================] - 0s 333ms/step - loss: 0.2482 - accuracy: 1.0000\n",
      "Epoch 142/200\n",
      "1/1 [==============================] - 0s 334ms/step - loss: 0.2401 - accuracy: 1.0000\n",
      "Epoch 143/200\n",
      "1/1 [==============================] - 0s 344ms/step - loss: 0.2324 - accuracy: 1.0000\n",
      "Epoch 144/200\n",
      "1/1 [==============================] - 0s 333ms/step - loss: 0.2240 - accuracy: 1.0000\n",
      "Epoch 145/200\n",
      "1/1 [==============================] - 0s 333ms/step - loss: 0.2157 - accuracy: 1.0000\n",
      "Epoch 146/200\n",
      "1/1 [==============================] - 0s 330ms/step - loss: 0.2072 - accuracy: 1.0000\n",
      "Epoch 147/200\n",
      "1/1 [==============================] - 0s 331ms/step - loss: 0.2004 - accuracy: 1.0000\n",
      "Epoch 148/200\n",
      "1/1 [==============================] - 0s 331ms/step - loss: 0.1933 - accuracy: 1.0000\n",
      "Epoch 149/200\n",
      "1/1 [==============================] - 0s 328ms/step - loss: 0.1868 - accuracy: 1.0000\n",
      "Epoch 150/200\n",
      "1/1 [==============================] - 0s 328ms/step - loss: 0.1779 - accuracy: 1.0000\n",
      "Epoch 151/200\n",
      "1/1 [==============================] - 0s 337ms/step - loss: 0.1699 - accuracy: 1.0000\n",
      "Epoch 152/200\n",
      "1/1 [==============================] - 0s 345ms/step - loss: 0.1628 - accuracy: 1.0000\n",
      "Epoch 153/200\n",
      "1/1 [==============================] - 0s 329ms/step - loss: 0.1561 - accuracy: 1.0000\n",
      "Epoch 154/200\n",
      "1/1 [==============================] - 0s 330ms/step - loss: 0.1502 - accuracy: 1.0000\n",
      "Epoch 155/200\n",
      "1/1 [==============================] - 0s 329ms/step - loss: 0.1443 - accuracy: 1.0000\n",
      "Epoch 156/200\n",
      "1/1 [==============================] - 0s 329ms/step - loss: 0.1390 - accuracy: 1.0000\n",
      "Epoch 157/200\n",
      "1/1 [==============================] - 0s 330ms/step - loss: 0.1337 - accuracy: 1.0000\n",
      "Epoch 158/200\n",
      "1/1 [==============================] - 0s 326ms/step - loss: 0.1288 - accuracy: 1.0000\n",
      "Epoch 159/200\n",
      "1/1 [==============================] - 0s 330ms/step - loss: 0.1238 - accuracy: 1.0000\n",
      "Epoch 160/200\n",
      "1/1 [==============================] - 0s 340ms/step - loss: 0.1190 - accuracy: 1.0000\n",
      "Epoch 161/200\n",
      "1/1 [==============================] - 0s 342ms/step - loss: 0.1142 - accuracy: 1.0000\n",
      "Epoch 162/200\n",
      "1/1 [==============================] - 0s 330ms/step - loss: 0.1095 - accuracy: 1.0000\n",
      "Epoch 163/200\n",
      "1/1 [==============================] - 0s 330ms/step - loss: 0.1051 - accuracy: 1.0000\n",
      "Epoch 164/200\n",
      "1/1 [==============================] - 0s 328ms/step - loss: 0.1008 - accuracy: 1.0000\n",
      "Epoch 165/200\n",
      "1/1 [==============================] - 0s 332ms/step - loss: 0.0969 - accuracy: 1.0000\n",
      "Epoch 166/200\n",
      "1/1 [==============================] - 0s 326ms/step - loss: 0.0931 - accuracy: 1.0000\n",
      "Epoch 167/200\n",
      "1/1 [==============================] - 0s 332ms/step - loss: 0.0896 - accuracy: 1.0000\n",
      "Epoch 168/200\n",
      "1/1 [==============================] - 0s 332ms/step - loss: 0.0863 - accuracy: 1.0000\n",
      "Epoch 169/200\n",
      "1/1 [==============================] - 0s 338ms/step - loss: 0.0835 - accuracy: 1.0000\n",
      "Epoch 170/200\n",
      "1/1 [==============================] - 0s 344ms/step - loss: 0.0808 - accuracy: 1.0000\n",
      "Epoch 171/200\n",
      "1/1 [==============================] - 0s 331ms/step - loss: 0.0781 - accuracy: 1.0000\n",
      "Epoch 172/200\n",
      "1/1 [==============================] - 0s 328ms/step - loss: 0.0742 - accuracy: 1.0000\n",
      "Epoch 173/200\n",
      "1/1 [==============================] - 0s 331ms/step - loss: 0.0707 - accuracy: 1.0000\n",
      "Epoch 174/200\n",
      "1/1 [==============================] - 0s 330ms/step - loss: 0.0673 - accuracy: 1.0000\n",
      "Epoch 175/200\n",
      "1/1 [==============================] - 0s 338ms/step - loss: 0.0645 - accuracy: 1.0000\n",
      "Epoch 176/200\n",
      "1/1 [==============================] - 0s 333ms/step - loss: 0.0618 - accuracy: 1.0000\n",
      "Epoch 177/200\n",
      "1/1 [==============================] - 0s 329ms/step - loss: 0.0593 - accuracy: 1.0000\n",
      "Epoch 178/200\n",
      "1/1 [==============================] - 0s 335ms/step - loss: 0.0570 - accuracy: 1.0000\n",
      "Epoch 179/200\n",
      "1/1 [==============================] - 0s 341ms/step - loss: 0.0547 - accuracy: 1.0000\n",
      "Epoch 180/200\n",
      "1/1 [==============================] - 0s 332ms/step - loss: 0.0525 - accuracy: 1.0000\n",
      "Epoch 181/200\n",
      "1/1 [==============================] - 0s 331ms/step - loss: 0.0505 - accuracy: 1.0000\n",
      "Epoch 182/200\n",
      "1/1 [==============================] - 0s 332ms/step - loss: 0.0485 - accuracy: 1.0000\n",
      "Epoch 183/200\n",
      "1/1 [==============================] - 0s 330ms/step - loss: 0.0466 - accuracy: 1.0000\n",
      "Epoch 184/200\n",
      "1/1 [==============================] - 0s 331ms/step - loss: 0.0448 - accuracy: 1.0000\n",
      "Epoch 185/200\n",
      "1/1 [==============================] - 0s 331ms/step - loss: 0.0430 - accuracy: 1.0000\n",
      "Epoch 186/200\n",
      "1/1 [==============================] - 0s 334ms/step - loss: 0.0413 - accuracy: 1.0000\n",
      "Epoch 187/200\n",
      "1/1 [==============================] - 0s 337ms/step - loss: 0.0396 - accuracy: 1.0000\n",
      "Epoch 188/200\n",
      "1/1 [==============================] - 0s 340ms/step - loss: 0.0380 - accuracy: 1.0000\n",
      "Epoch 189/200\n",
      "1/1 [==============================] - 0s 329ms/step - loss: 0.0365 - accuracy: 1.0000\n",
      "Epoch 190/200\n",
      "1/1 [==============================] - 0s 370ms/step - loss: 0.0350 - accuracy: 1.0000\n",
      "Epoch 191/200\n",
      "1/1 [==============================] - 0s 378ms/step - loss: 0.0336 - accuracy: 1.0000\n",
      "Epoch 192/200\n",
      "1/1 [==============================] - 0s 373ms/step - loss: 0.0322 - accuracy: 1.0000\n",
      "Epoch 193/200\n",
      "1/1 [==============================] - 0s 366ms/step - loss: 0.0309 - accuracy: 1.0000\n",
      "Epoch 194/200\n",
      "1/1 [==============================] - 0s 347ms/step - loss: 0.0297 - accuracy: 1.0000\n",
      "Epoch 195/200\n",
      "1/1 [==============================] - 0s 355ms/step - loss: 0.0285 - accuracy: 1.0000\n",
      "Epoch 196/200\n",
      "1/1 [==============================] - 0s 374ms/step - loss: 0.0274 - accuracy: 1.0000\n",
      "Epoch 197/200\n",
      "1/1 [==============================] - 0s 336ms/step - loss: 0.0262 - accuracy: 1.0000\n",
      "Epoch 198/200\n",
      "1/1 [==============================] - 0s 354ms/step - loss: 0.0251 - accuracy: 1.0000\n",
      "Epoch 199/200\n",
      "1/1 [==============================] - 0s 354ms/step - loss: 0.0241 - accuracy: 1.0000\n",
      "Epoch 200/200\n",
      "1/1 [==============================] - 0s 329ms/step - loss: 0.0231 - accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "from keras import backend as K\n",
    "\n",
    "K.clear_session()  #Resets all state generated by Keras\n",
    "\n",
    "latent_dim = 100\n",
    "\n",
    "embedding_dim = 100\n",
    "\n",
    "# Encoder\n",
    "\n",
    "encoder_inputs = Input(shape=(x.shape[1],))\n",
    "\n",
    "\n",
    "\n",
    "enc_emb =  Embedding(1000, embedding_dim,trainable=True, mask_zero = True)(encoder_inputs)\n",
    "\n",
    "encoder_lstm = LSTM(latent_dim,return_sequences=True,return_state=True) #add dropout\n",
    "\n",
    "encoder_outputs, state_h, state_c = encoder_lstm(enc_emb)\n",
    "\n",
    "\n",
    "\n",
    "#Setting up the Decoder using 'encoder_states' as initial state\n",
    "\n",
    "# TODO: figure out why the shape is None? because we also use it later for our decoding when we only give one\n",
    "\n",
    "decoder_inputs = Input(shape=(y.shape[1],))\n",
    "\n",
    "#Embedding layer\n",
    "\n",
    "dec_emb_layer = Embedding(Y_voc+1, embedding_dim,trainable=True, mask_zero = True)\n",
    "\n",
    "dec_emb = dec_emb_layer(decoder_inputs)\n",
    "\n",
    "\n",
    "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True) # ,dropout=0.4,recurrent_dropout=0.2\n",
    "\n",
    "# TODO: is the LSTM bidirectional? why do we even need those two states?\n",
    "\n",
    "# TODO: why do the graphs say (None, 256) where 256 is the latent_dim (the length of the state vectors); shouldn't it be clear that it is (1, 256)?\n",
    "\n",
    "decoder_outputs ,decoder_fwd_state, decoder_back_state = decoder_lstm(dec_emb,initial_state=[state_h, state_c])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# TODO: figure out what TimeDistributed does\n",
    "\n",
    "decoder_dense =  TimeDistributed(Dense(units = Y_voc, activation='softmax'))\n",
    "\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "#Defining the model\n",
    "\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "\n",
    "model.summary()\n",
    "\n",
    "#Visualize the Model\n",
    "\n",
    "from tensorflow.keras.utils import plot_model\n",
    "\n",
    "plot_model(model, to_file='training_model_plot.png', show_shapes=True, show_layer_names=True)\n",
    "\n",
    "# TODO: understand this\n",
    "\n",
    "#Adding Metrics\n",
    "\n",
    "model.compile(optimizer='rmsprop' , loss='sparse_categorical_crossentropy' , metrics=['accuracy'])\n",
    "\n",
    "\n",
    "\n",
    "# TODO: fit only on one example and check if model actually works\n",
    "\n",
    "history = model.fit(x = [x,y], y = y.reshape(1,-1,1) ,epochs=200,batch_size = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict = model.predict([x,y])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_index_word=Y_tokenizer.index_word \n",
    "target_word_index=Y_tokenizer.word_index\n",
    "\n",
    "source_index_word=X_tokenizer.index_word \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pre = np.squeeze(predict, axis = 0)\n",
    "pre.shape\n",
    "\n",
    "np.argmax(pre[2,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'to'"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_index_word[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'syrian official obama climbed to the top of the tree does nt know how to get downobama sends a letter to the heads of the house and senateobama to seek congressional approval on military action against syriaaim is to determine whether cw were used not by whom says un spokesman'"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "highlights[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "official\n",
      "get\n",
      "top\n",
      "climbed\n",
      "of\n",
      "obama\n",
      "the\n",
      "a\n",
      "a\n",
      "climbed\n",
      "downobama\n",
      "climbed\n",
      "of\n",
      "how\n",
      "the\n",
      "sends\n",
      "of\n",
      "does\n",
      "of\n",
      "downobama\n",
      "how\n",
      "climbed\n",
      "does\n",
      "sends\n",
      "to\n",
      "nt\n",
      "syrian\n",
      "the\n",
      "syrian\n",
      "tree\n",
      "to\n",
      "syrian\n",
      "the\n",
      "letter\n",
      "the\n",
      "a\n",
      "syrian\n",
      "tree\n",
      "to\n",
      "syrian\n",
      "top\n",
      "to\n"
     ]
    }
   ],
   "source": [
    "for i in range(predict.shape[2]):\n",
    "    print(target_index_word[np.argmax(pre[i,:])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 100)\n",
      "(1000, 100)\n",
      "(100, 42)\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "l = i+4\n",
    "d = i if i <4 else 3\n",
    "print((model.get_weights()[i]).shape)\n",
    "\n",
    "print(encoder_model.get_weights()[d].shape)\n",
    "print(decoder_model.get_weights()[l].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Layer model_25 weight shape (43, 100) is not compatible with provided weight shape (400,).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-166-8143a166e530>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[1;31m#target_model.set_weights(model.get_weights())\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 31\u001b[1;33m \u001b[0mdecoder_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_weights\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_weights\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     32\u001b[0m \u001b[1;31m#Function defining the implementation of inference process\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mdecode_sequence\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_seq\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda_2\\lib\\site-packages\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36mset_weights\u001b[1;34m(self, weights)\u001b[0m\n\u001b[0;32m   1849\u001b[0m         \u001b[0mref_shape\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparam\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1850\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mref_shape\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_compatible_with\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweight_shape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1851\u001b[1;33m           raise ValueError(\n\u001b[0m\u001b[0;32m   1852\u001b[0m               \u001b[1;34mf'Layer {self.name} weight shape {ref_shape} '\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1853\u001b[0m               \u001b[1;34m'is not compatible with provided weight '\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Layer model_25 weight shape (43, 100) is not compatible with provided weight shape (400,)."
     ]
    }
   ],
   "source": [
    "\n",
    "encoder_model = Model(inputs=encoder_inputs,outputs=[encoder_outputs, state_h, state_c])\n",
    "\n",
    "#encoder_model.set_weights(model.get_weights()[:4])\n",
    "\n",
    "decoder_state_input_h = Input(shape=(latent_dim,))\n",
    "decoder_state_input_c = Input(shape=(latent_dim,))\n",
    "\n",
    "# we use the same embedding from the trained model\n",
    "dec_emb2= dec_emb_layer(decoder_inputs) \n",
    "\n",
    "#Setting the initial states to the states from the previous time step for better prediction\n",
    "decoder_outputs2, state_h2, state_c2 = decoder_lstm(dec_emb2, initial_state=[decoder_state_input_h, decoder_state_input_c])\n",
    "\n",
    "# TODO: remove this\n",
    "# #Attention inference\n",
    "# attn_out_inf, attn_states_inf = attn_layer([decoder_hidden_state_input, decoder_outputs2])\n",
    "# decoder_inf_concat = Concatenate(axis=-1, name='concat')([decoder_outputs2, attn_out_inf])\n",
    "\n",
    "#Adding Dense softmax layer to generate proability distribution over the target vocabulary\n",
    "decoder_outputs2 = decoder_dense(decoder_outputs2) \n",
    "\n",
    "#Final Decoder model\n",
    "# TODO: how is this all linked to what we trained before? where does the parameter sharing happen?\n",
    "# TODO: why are we appending the lists instead of just writing it in one list\n",
    "decoder_model = Model(\n",
    "    [decoder_inputs] + [decoder_state_input_h, decoder_state_input_c],\n",
    "    [decoder_outputs2] + [state_h2, state_c2])\n",
    "\n",
    "#target_model.set_weights(model.get_weights()) \n",
    "\n",
    "decoder_model.set_weights(model.get_weights()[4:])\n",
    "#Function defining the implementation of inference process\n",
    "def decode_sequence(input_seq):\n",
    "    \n",
    "    e_out, e_h, e_c = encoder_model.predict(input_seq)\n",
    "\n",
    "    #Generating empty target sequence of length 1\n",
    "    target_seq = np.zeros((1,1))\n",
    "    \n",
    "    #Populating the first word of target sequence with the start word\n",
    "\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "    while not stop_condition:\n",
    "        \n",
    "        output_tokens, h, c = decoder_model.predict([target_seq] + [e_h, e_c])\n",
    "\n",
    "        #Sampling a token\n",
    "        # TODO: understand the indexing: why -1 instead of 0 as well? should the output tokens be of shape (1, 1, num_words)?\n",
    "        # TODO: I added the plus one because I think the neurons in the dense layer start at 0 but our dictionary starts at 1, despite a one-on-one mapping from\n",
    "        # dictionary index numbers to neurons if I understand correctly\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        print(sampled_token_index)\n",
    "        #print(sampled_token)\n",
    "        input()\n",
    "        sampled_token = target_index_word[sampled_token_index+1]\n",
    "        \n",
    "\n",
    "        target_seq = np.zeros((1,1))\n",
    "        target_seq[0, 0] = sampled_token_index\n",
    "\n",
    "        #Updating internal states\n",
    "        e_h, e_c = h, c\n",
    "\n",
    "    return decoded_sentence\n",
    "\n",
    "#Functions to convert an integer sequence to a word sequence for summary as well as reviews \n",
    "def seq2summary(input_seq):\n",
    "    newString=''\n",
    "    # for i in input_seq:\n",
    "    #     if((i!=0 and i!=target_word_index['sostok']) and i!=target_word_index['eostok']):\n",
    "    newString=newString+target_index_word[i]+' '\n",
    "    return newString\n",
    "\n",
    "def seq2text(input_seq):\n",
    "    newString=''\n",
    "    # for i in input_seq:\n",
    "    #     if(i!=0):\n",
    "    newString=newString+source_index_word[i]+' '\n",
    "    return newString\n",
    "\n",
    "#Summaries generated by the model\n",
    "\n",
    "# TODO: change this, will crash when there is less than 20\n",
    "#for i in range(0,20):\n",
    "#print(\"Article:\",seq2text(X_train[1]))\n",
    "print(\"Original summary:\",seq2summary(y))\n",
    "print(\"article to be decoded:\")\n",
    "#print(X_train[i].reshape(1,MAX_ARTICLE_LEN))\n",
    "print(\"Predicted summary:\",decode_sequence(x))\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-32-2313bfa3cb78>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'abstract'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'abstract'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m:\u001b[0m \u001b[1;34m'sostok '\u001b[0m\u001b[1;33m+\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' eostok'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m#Splitting the Dataset twice to get 80% training data, 10% of validation data and 10% of test data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mX_val\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_val\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrain_test_split\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'article'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'abstract'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtest_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "df['abstract'] = df['abstract'].apply(lambda x : 'sostok '+ x + ' eostok')\n",
    "\n",
    "#Splitting the Dataset twice to get 80% training data, 10% of validation data and 10% of test data\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train,X_val,y_train,y_val=train_test_split(np.array(df['article']),np.array(df['abstract']),test_size=0.2,random_state=0,shuffle=True)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_val, y_val, test_size = 0.5, random_state = 0, shuffle = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size =200\n",
    "src_txt_length =200\n",
    "sum_txt_length = 200\n",
    "# encoder input model\n",
    "inputs = Input(shape=(src_txt_length,))\n",
    "encoder1 = Embedding(vocab_size, 128)(inputs)\n",
    "encoder2 = LSTM(128)(encoder1)\n",
    "encoder3 = RepeatVector(sum_txt_length)(encoder2)\n",
    "# decoder output model\n",
    "decoder1 = LSTM(128, return_sequences=True)(encoder3)\n",
    "outputs = TimeDistributed(Dense(vocab_size, activation='softmax'))(decoder1)\n",
    "# tie it together\n",
    "model = Model(inputs=inputs, outputs=outputs)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        embedded = self.embedding(input).view(1, 1, -1)\n",
    "        output = embedded\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        output = self.embedding(input).view(1, 1, -1)\n",
    "        output = F.relu(output)\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        output = self.softmax(self.out(output[0]))\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'MAX_LENGTH' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-37-0b46da400ec9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mclass\u001b[0m \u001b[0mAttnDecoderRNN\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhidden_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdropout_p\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mMAX_LENGTH\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m         \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mAttnDecoderRNN\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhidden_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhidden_size\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutput_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moutput_size\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-37-0b46da400ec9>\u001b[0m in \u001b[0;36mAttnDecoderRNN\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mAttnDecoderRNN\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[1;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhidden_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdropout_p\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mMAX_LENGTH\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m         \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mAttnDecoderRNN\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhidden_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhidden_size\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutput_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moutput_size\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'MAX_LENGTH' is not defined"
     ]
    }
   ],
   "source": [
    "class AttnDecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size, dropout_p=0.1, max_length=MAX_LENGTH):\n",
    "        super(AttnDecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.dropout_p = dropout_p\n",
    "        self.max_length = max_length\n",
    "\n",
    "        self.embedding = nn.Embedding(self.output_size, self.hidden_size)\n",
    "        self.attn = nn.Linear(self.hidden_size * 2, self.max_length)\n",
    "        self.attn_combine = nn.Linear(self.hidden_size * 2, self.hidden_size)\n",
    "        self.dropout = nn.Dropout(self.dropout_p)\n",
    "        self.gru = nn.GRU(self.hidden_size, self.hidden_size)\n",
    "        self.out = nn.Linear(self.hidden_size, self.output_size)\n",
    "\n",
    "    def forward(self, input, hidden, encoder_outputs):\n",
    "        embedded = self.embedding(input).view(1, 1, -1)\n",
    "        embedded = self.dropout(embedded)\n",
    "\n",
    "        attn_weights = F.softmax(\n",
    "            self.attn(torch.cat((embedded[0], hidden[0]), 1)), dim=1)\n",
    "        attn_applied = torch.bmm(attn_weights.unsqueeze(0),\n",
    "                                 encoder_outputs.unsqueeze(0))\n",
    "\n",
    "        output = torch.cat((embedded[0], attn_applied[0]), 1)\n",
    "        output = self.attn_combine(output).unsqueeze(0)\n",
    "\n",
    "        output = F.relu(output)\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "\n",
    "        output = F.log_softmax(self.out(output[0]), dim=1)\n",
    "        return output, hidden, attn_weights\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e26081387c8990b949948d3d3fd7f7a4bc3bad723d6b8652b3d4ab339298fb31"
  },
  "kernelspec": {
   "display_name": "Python 3.7.9 64-bit (conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
