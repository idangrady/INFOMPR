{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras \n",
    "from tensorflow.keras.layers import Input, LSTM, Attention, Embedding, Dense, Concatenate, TimeDistributed   #Layers required to implement the model\n",
    "# Try out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np   #Package for scientific computing and dealing with arrays\n",
    "import pandas as pd  #Package providing fast, flexible and expressive data structures\n",
    "import re            #re stands for RegularExpression providing full support for Perl-like Regular Expressions in Python\n",
    "from bs4 import BeautifulSoup   #Package for pulling data out of HTML and XML files\n",
    "from keras.preprocessing.sequence import pad_sequences  #For Padding the seqences to same length\n",
    "from nltk.corpus import stopwords   #For removing filler words\n",
    "from tensorflow.keras.layers import Input, LSTM, Attention, Embedding, Dense, Concatenate, TimeDistributed   #Layers required to implement the model\n",
    "from tensorflow.keras.models import Model  #Helps in grouping the layers into an object with training and inference features\n",
    "from tensorflow.keras.callbacks import EarlyStopping  #Allows training the model on large no. of training epochs & stop once the performance stops improving on validation dataset\n",
    "from os import listdir\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Loaded Stories 23\n"
    }
   ],
   "source": [
    "\n",
    "# load doc into memory\n",
    "def load_doc(filename):\n",
    "\t# open the file as read only\n",
    "\tfile = open(filename, encoding='utf-8')\n",
    "\t# read all text\n",
    "\ttext = file.read()\n",
    "\t# close the file\n",
    "\tfile.close()\n",
    "\treturn text\n",
    "\n",
    "# split a document into news story and highlights\n",
    "def split_story(doc):\n",
    "\t# find first highlight\n",
    "\tindex = doc.find('@highlight')\n",
    "\t# split into story and highlights\n",
    "\tstory, highlights = doc[:index], doc[index:].split('@highlight')\n",
    "\t# strip extra white space around each highlight\n",
    "\thighlights = [h.strip() for h in highlights if len(h) > 0]\n",
    "\treturn story, highlights\n",
    "\n",
    "# load all stories in a directory\n",
    "def load_stories(directory):\n",
    "\tstories = list()\n",
    "\tfor name in listdir(directory):\n",
    "\t\tfilename = directory + '/' + name\n",
    "\t\t# load document\n",
    "\t\tdoc = load_doc(filename)\n",
    "\t\t# split into story and highlights\n",
    "\t\tstory, highlights = split_story(doc)\n",
    "\t\t# store\n",
    "\t\tstories.append({'story':story, 'highlights':highlights})\n",
    "\treturn stories\n",
    "\n",
    "# clean a list of lines\n",
    "def clean_lines(lines):\n",
    "\tcleaned = list()\n",
    "\t# prepare a translation table to remove punctuation\n",
    "\ttable = str.maketrans('', '', string.punctuation)\n",
    "\tfor line in lines:\n",
    "\t\t# strip source cnn office if it exists\n",
    "\t\tindex = line.find('(CNN) -- ')\n",
    "\t\tif index > -1:\n",
    "\t\t\tline = line[index+len('(CNN)'):]\n",
    "\t\t# tokenize on white space\n",
    "\t\tline = line.split()\n",
    "\t\t# convert to lower case\n",
    "\t\tline = [word.lower() for word in line]\n",
    "\t\t# remove punctuation from each token\n",
    "\t\tline = [w.translate(table) for w in line]\n",
    "\t\t# remove tokens with numbers in them\n",
    "\t\tline = [word for word in line if word.isalpha()]\n",
    "\t\t# store as string\n",
    "\t\tcleaned.append(' '.join(line))\n",
    "\t# remove empty strings\n",
    "\tcleaned = [c for c in cleaned if len(c) > 0]\n",
    "\treturn cleaned\n",
    "\n",
    "# load stories\n",
    "directory = 'cnn/'\n",
    "stories = load_stories(directory)\n",
    "print('Loaded Stories %d' % len(stories))\n",
    "\n",
    "# clean stories\n",
    "for example in stories:\n",
    "\texample['story'] = clean_lines(example['story'].split('\\n'))\n",
    "\texample['highlights'] = clean_lines(example['highlights'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save to file\n",
    "from pickle import dump\n",
    "dump(stories, open('cnn_dataset.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Loaded Stories 23\n"
    }
   ],
   "source": [
    "from pickle import load\n",
    "\n",
    "# load from file\n",
    "stories = load(open('cnn_dataset.pkl', 'rb'))\n",
    "print('Loaded Stories %d' % len(stories))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer \n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import RepeatVector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "23"
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": [
    "len(stories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "['after a weekend of intense investigation authorities are piecing together more details about friday s fatal shooting at los angeles international airport including the suspect s behavior earlier in the week and a warning from his family that may have come minutes too late',\n 'officers sent to check on paul ciancia s welfare arrived at his apartment less than an hour after the shooting started police said monday',\n 'here is a rundown to get you up to speed',\n 'the suspect',\n 'ciancia of los angeles is charged with murder of a federal officer and commission of violence in an international airport',\n 'he was shot by officers friday and was in critical condition at ronald reagan ucla medical center on sunday',\n 'a source said ciancia was unable to speak to investigators',\n 'clues about a motive',\n 'attorney general eric holder said monday that more investigation is necessary to uncover a motive for the attack',\n 'but a note found on ciancia indicated that he wanted to kill transportation security administration employees to instill fear in what the suspect called the agents traitorous minds fbi special agent in charge david bowdich said',\n 'according to someone who knew ciancia and his three roommates well ciancia began asking for a ride to the airport days before the shooting he claimed he needed to fly to new jersey to help his sick father but he never said what day he needed to leave the source said',\n 'on friday ciancia burst into a roommate s room and demanded a ride to the airport immediately said the source who spoke to cnn on the condition of anonymity',\n 'the roommate obliged investigators do nt think the roommate had any idea of ciancia s plans',\n 'the nearsave',\n 'around the same time ciancia was sending text messages to family members in pennsville new jersey',\n 'one suggested that something bad would happen',\n 'although ciancia has no known history of mental illness he said in the texts that he was unhappy and the messages were alarming enough that ciancia s father decided to call police',\n 'i felt that it was pretty serious it sounded as if paul ciancia in california was thinking about harming himself so obviously i knew i needed to make a phone call to the lapd pennsville police chief allen cummings told cnn s jake tapper on monday',\n 'cummings spoke with a lieutenant there who told him the department was in the middle of responding to a shooting at lax',\n 'at this point we were nt connecting the dots he said they did later when a reporter called the police chief asking him to comment on the shooting',\n 'los angeles police department cmdr andy smith says police were first called to check on ciancia at am officers arrived at his apartment six minutes later according to smith',\n 'ciancia was already gone',\n 'the timeline provided monday by police differed from that offered earlier by rep michael mccaul rtexas chairman of the house homeland security committee he said police had arrived at ciancia s apartment about minutes after the suspect had left for the airport',\n 'according to the lapd account they arrived minutes after the shooting which began about am according to police it was not immediately clear when ciancia left for the airport',\n 'the attack',\n 'about am friday ciancia walked up to a transportation security administration checkpoint in terminal he pulled a caliber assault rifle from a bag and shot tsa officer gerardo hernandez at pointblank range according to a court document filed by an fbi agent',\n 'ciancia then went up an escalator but returned to shoot hernandez again apparently after seeing him move',\n 'he continued walking and shooting witnesses said he went from person to person asking are you tsa',\n 'i just shook my head traveler leon saryan told cnn s anderson cooper and he kept going',\n 'chaos and terror inside lax terminal',\n 'the victims',\n 'hernandez was the first tsa officer to die in the line of duty since the agency was created in',\n 'he took pride in his duty for the american public and for the tsa mission said his wife ana hernandez',\n 'the couple who married in have two children',\n 'two other tsa officers james speer and tony grigsby were wounded but were released from the hospital',\n 'grigsby who was shot in the foot told reporters monday he was injured while helping an elderly man move to a safe area',\n 'i turned around and there was a gunman he said shot me twice',\n 'a traveler who was shot in the leg brian ludmer of lake forest illinois was in fair condition sunday',\n 'the police response',\n 'tsa officers are unarmed so it was airport police officers who eventually shot ciancia multiple times in the chest also striking him in the face and neck',\n 'airport police chief patrick gannon said the fbi told him that his officers were seconds behind ciancia he praised their response even though he acknowledged that he had moved his officers away from positions inside the checkpoints during the past year',\n 'the threat at the airport does not exist behind security at that podium the threat exists from the curbline on gannon said so we have our people stationed throughout the airport',\n 'holder said monday that the investigation will include a review of security measures at lax and other airports',\n 'the responsibility for protecting airport security is not a tsa function but something that i think we need to certainly examine given what happened in los angeles he said',\n 'travel delays',\n 'the incident forced authorities to shut down parts of the airport evacuate travelers and put a temporary hold on some departures and landings',\n 'more than airline passengers were affected by the incident friday as a result of cancellations delays or diversions to other airports according to lax one airline jetblue temporarily moved its operations to long beach airport',\n 'on saturday an additional flights were affected including that were canceled involving about passengers according to los angeles international airport',\n 'according to flightaware a flight tracking website airlines canceled flights into or out of lax after the incident friday morning and more saturday',\n 'an additional flights were delayed over the two days flightaware said',\n 'some of those cancellations and delays may have been caused by problems other than the shooting however',\n 'the airport was operating normally monday morning',\n 'suspect s family responds',\n 'ciancia s family in a statement read monday afternoon by attorney john jordan in new jersey said they were shocked and numbed by the tragic events of last friday',\n 'it is most important for us as a family to express our deep and sincere sympathy to the hernandez family the ciancia family said lrb by rrb all accounts officer hernandez was an exemplary member of the law enforcement community and a good family man our hearts go out to his family and many others who grieve his passing',\n 'we wish to convey too our hopes that those who were wounded during this incident will experience quick and full recoveries we also regret the inconvenience experienced by thousands of travelers as well as the administration and the employees of the los angeles airport',\n 'the ciancia family said they would continue to love and care for paul',\n 'we will support him during the difficult times ahead while we do not mean to minimize the grief and distress experienced by many other families we hope that the public will understand that this is a very difficult time for our family too the family said',\n 'what s next',\n 'if convicted ciancia could face the death penalty or life in prison without parole the us attorney general would decide whether to pursue a death sentence',\n 'tsa administrator john pistole said the shooting has prompted a review of security protocol with partner agencies',\n 'mccaul said better coordination with local law enforcement could improve security at checkpoints',\n 'but the congressman acknowledged that it s very difficult to stop these types of attacks',\n 'it s almost like an open shopping mall he said',\n 'opinion do nt arm the tsa']"
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "(stories[0]['story'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preprocessing\n",
    "\n",
    "#This the dictionary used for expanding contractions\n",
    "contraction_mapping = {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"'cause\": \"because\", \"could've\": \"could have\", \"couldn't\": \"could not\",\n",
    "\n",
    "                           \"didn't\": \"did not\", \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \"hasn't\": \"has not\", \"haven't\": \"have not\",\n",
    "\n",
    "                           \"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\",\n",
    "\n",
    "                           \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \"i'd\": \"i would\",\n",
    "\n",
    "                           \"i'd've\": \"i would have\", \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\",\n",
    "\n",
    "                           \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\",\n",
    "\n",
    "                           \"mayn't\": \"may not\", \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\", \"must've\": \"must have\",\n",
    "\n",
    "                           \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\",\n",
    "\n",
    "                           \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\",\n",
    "\n",
    "                           \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\", \"she's\": \"she is\",\n",
    "\n",
    "                           \"should've\": \"should have\", \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\"so's\": \"so as\",\n",
    "\n",
    "                           \"this's\": \"this is\",\"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\", \"there'd\": \"there would\",\n",
    "\n",
    "                           \"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\",\"they'd\": \"they would\", \"they'd've\": \"they would have\",\n",
    "\n",
    "                           \"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\",\n",
    "\n",
    "                           \"wasn't\": \"was not\", \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\",\n",
    "\n",
    "                           \"we've\": \"we have\", \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\",\n",
    "\n",
    "                           \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\",\n",
    "\n",
    "                           \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\",\n",
    "\n",
    "                           \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \"won't've\": \"will not have\",\n",
    "\n",
    "                           \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \"y'all\": \"you all\",\n",
    "\n",
    "                           \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\n",
    "\n",
    "                           \"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\",\n",
    "\n",
    "                           \"you're\": \"you are\", \"you've\": \"you have\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "[nltk_data] Error loading stopwords: <urlopen error [SSL:\n[nltk_data]     CERTIFICATE_VERIFY_FAILED] certificate verify failed:\n[nltk_data]     unable to get local issuer certificate (_ssl.c:1108)>\n"
    }
   ],
   "source": [
    "#Text Cleaning\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "\n",
    "stop_words = set(stopwords.words('english')) \n",
    "def text_cleaner(text,num):\n",
    "    newString = text.lower()  #converts all uppercase characters in the string into lowercase characters and returns it\n",
    "    newString = BeautifulSoup(newString, \"lxml\").text #parses the string into an lxml.html \n",
    "    newString = re.sub(r'\\([^)]*\\)', '', newString) #used to replace a string that matches a regular expression instead of perfect match\n",
    "    newString = re.sub('\"','', newString)           \n",
    "    newString = ' '.join([contraction_mapping[t] if t in contraction_mapping else t for t in newString.split(\" \")]) #for expanding contractions using the contraction_mapping dictionary    \n",
    "    newString = re.sub(r\"'s\\b\",\"\",newString)\n",
    "    newString = re.sub(\"[^a-zA-Z]\", \" \", newString)\n",
    "    if(num==0): \n",
    "      tokens = [w for w in newString.split() if not w in stop_words]  #converting the strings into tokens\n",
    "    else :\n",
    "      tokens = newString.split()\n",
    "    long_words=[]\n",
    "    for i in tokens:\n",
    "        if len(i)>1:                  #removing short words\n",
    "            long_words.append(i)   \n",
    "    return (\" \".join(long_words)).strip()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'edict' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [12]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0m ss  \u001b[38;5;241m=\u001b[39m \u001b[43medict\u001b[49m()\n\u001b[1;32m      2\u001b[0m ss[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124midan\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m      3\u001b[0m ss[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124midan\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'edict' is not defined"
     ]
    }
   ],
   "source": [
    "ss  = edict()\n",
    "ss[\"idan\"] = 1\n",
    "ss[\"idan\"]+1\n",
    "ss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from easydict import EasyDict as edict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calling the function\n",
    "x_vocab = edict()\n",
    "y_vocab = edict()\n",
    "\n",
    "article_word_count = []\n",
    "abstract_word_count = []\n",
    "\n",
    "cleaned_text = []\n",
    "highlights = []\n",
    "\n",
    "max_ar_length = 0\n",
    "max_high_lengh = 0\n",
    "\n",
    "for file in stories:\n",
    "    t = file['story']\n",
    "    story =\"\"\n",
    "    highligh= \"\"\n",
    "    h = file['highlights']\n",
    "    for line in t:\n",
    "        story= story+line\n",
    "        for word in line: \n",
    "            if word not in x_vocab.keys():\n",
    "                x_vocab[word] =1\n",
    "            else: \n",
    "                x_vocab[word]+=1\n",
    "\n",
    "    if len(story.split())> max_ar_length: max_ar_length =len(story.split())\n",
    "    article_word_count.append(len(story.split()))\n",
    "    cleaned_text.append(text_cleaner(story,0))\n",
    "    for line in h:\n",
    "        highligh= highligh+line\n",
    "        if word not in y_vocab.keys():\n",
    "            y_vocab[word] =1\n",
    "        else: \n",
    "            y_vocab[word]+=1\n",
    "\n",
    "    if len(highligh.split())> max_high_lengh: max_high_lengh =len(highligh.split())\n",
    "    abstract_word_count.append(len(highligh.split()))\n",
    "    highlights.append(highligh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Max length sequence story: 1329\n Max length sequence highlight: 56\n"
    }
   ],
   "source": [
    "# print summer: \n",
    "print(f\" Max length sequence story: {max_ar_length}\")\n",
    "print(f\" Max length sequence highlight: {max_high_lengh}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "int"
     },
     "metadata": {},
     "execution_count": 16
    }
   ],
   "source": [
    "type(len(set(cleaned_text[1].split())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_text = cleaned_text[0]\n",
    "cleaned_text = cleaned_text[1:]\n",
    "test_highlight = highlights[0]\n",
    "highlights = highlights[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "22"
     },
     "metadata": {},
     "execution_count": 18
    }
   ],
   "source": [
    "len(cleaned_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "'lrb cnn rrb men boys rescued found chained week islamic religious school pakistan reunited families placed shelters authorities saidthe group discovered underground room heavy chains linking togetherthe school alarabiya aloom jamia masjid zikirya also drug rehab clinic sohrab goth suburb gadap karachiall boys returned families senior police official ahsanullah marwat told cnnof adults released families seven handed shelter homeless saidthree people worked facility arrested four men ran place still large marwat saidofficials said facility part madrassa part drugrehab facility captives chained night apparently prevent escapethe operation successful plan continuing work ensure places like shut marwat saidmany captives told police families sent recovering drug addicts day worked religious studiesbut future rescued children unclearone woman told local television station willing pay police keep troublesome child said would rather facility remain open regardless treated childrenmany others however said shock disbelief allegationsone man complained deep debt paying school large amount money board son'"
     },
     "metadata": {},
     "execution_count": 19
    }
   ],
   "source": [
    "# TODO: some words are not properly split; maybe beginning and ending of sentences?\n",
    "cleaned_text[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "'sos captive boys and men were rescued from an islamic religious school in pakistanthey were reunited with their families this weekthe facility was a school and drug rehab clinicauthorities say they re searching for the owners three others arrested at the facility eos'"
     },
     "metadata": {},
     "execution_count": 20
    }
   ],
   "source": [
    "new_highlights = []\n",
    "for highlight in highlights:\n",
    "    highlight = \"sos \" + highlight + \" eos\"\n",
    "    new_highlights.append(highlight)\n",
    "highlights = new_highlights\n",
    "highlights[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tokenizer = Tokenizer(num_words = 3000)\n",
    "X_tokenizer.fit_on_texts(cleaned_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "   ('adding', 4),\n             ('collection', 2),\n             ('titlesthe', 1),\n             ('stage', 1),\n             ('career', 4),\n             ('spent', 2),\n             ('means', 5),\n             ('dream', 1),\n             ('lifetime', 1),\n             ('come', 3),\n             ('true', 3),\n             ('saidwhen', 1),\n             ('kid', 1),\n             ('michael', 2),\n             ('schumacher', 1),\n             ('greatest', 2),\n             ('idol', 1),\n             ('incredible', 1),\n             ('honor', 10),\n             ('get', 17),\n             ('drive', 3),\n             ('ferrarii', 1),\n             ('extremely', 2),\n             ('motivated', 1),\n             ('help', 5),\n             ('back', 18),\n             ('top', 4),\n             ('put', 7),\n             ('heart', 6),\n             ('soul', 2),\n             ('making', 5),\n             ('happenvettel', 1),\n             ('partner', 2),\n             ('kimi', 1),\n             ('raikkonen', 1),\n             ('last', 11),\n             ('title', 5),\n             ('match', 7),\n             ('might', 5),\n             ('mercedeswho', 1),\n             ('mclarenmclaren', 1),\n             ('tried', 2),\n             ('dampen', 1),\n             ('speculation', 1),\n             ('cars', 1),\n             ('buildup', 1),\n             ('weekend', 4),\n             ('seasonending', 1),\n             ('racewe', 1),\n             ('know', 19),\n             ('awaiting', 1),\n             ('news', 7),\n             ('lineup', 1),\n             ('announce', 1),\n             ('december', 2),\n             ('hear', 2),\n             ('twitteralonso', 1),\n             ('redhot', 1),\n             ('favorite', 1),\n             ('take', 12),\n             ('current', 4),\n             ('jenson', 1),\n             ('button', 1),\n             ('kevin', 1),\n             ('magnussen', 2),\n             ('unclearbutton', 1),\n             ('brawn', 1),\n             ('since', 8),\n             ('morphed', 1),\n             ('mercedes', 1),\n             ('sanguine', 1),\n             ('explored', 2),\n             ('idea', 2),\n             ('moving', 3),\n             ('racingdanish', 1),\n             ('passionate', 1),\n             ('staying', 1),\n             ('gave', 2),\n             ('inthere', 1),\n             ('dane', 1),\n             ('asked', 6),\n             ('options', 1),\n             ('nt', 35),\n             ('retained', 2),\n             ('eighttime', 1),\n             ('championswhich', 1),\n             ('teams', 6),\n             ('fillmercedes', 1),\n             ('williams', 1),\n             ('sauber', 1),\n             ('confirmed', 2),\n             ('pairings', 1),\n             ('offer', 7),\n             ('rossoforce', 1),\n             ('already', 2),\n             ('announced', 9),\n             ('retain', 2),\n             ('hulkenberg', 1),\n             ('mexican', 1),\n             ('sergio', 1),\n             ('perez', 1),\n             ('position', 2),\n             ('confirmedthere', 1),\n             ('seat', 2),\n             ('grabs', 1),\n             ('blood', 5),\n             ('max', 1),\n             ('verstappen', 1),\n             ('youngest', 1),\n             ('history', 4),\n             ('seasonjunior', 1),\n             ('carlos', 1),\n             ('sainz', 1),\n             ('jr', 2),\n             ('pole', 1),\n             ('second', 3),\n             ('named', 2),\n             ('test', 4),\n             ('end', 6),\n             ('french', 1),\n             ('racer', 1),\n             ('jeaneric', 1),\n             ('vergnesauber', 1),\n             ('swede', 1),\n             ('marcus', 1),\n             ('ericsson', 1),\n             ('brazilian', 1),\n             ('felipe', 1),\n             ('nasr', 2),\n             ('november', 2),\n             ('much', 8),\n             ('chagrin', 1),\n             ('adrian', 1),\n             ('sutil', 1),\n             ('believed', 2),\n             ('seasonericsson', 1),\n             ('backed', 1),\n             ('sponsors', 1),\n             ('bring', 1),\n             ('estimated', 6),\n             ('funding', 2),\n             ('swiss', 1),\n             ('teamgrid', 1),\n             ('shrinks', 1),\n             ('inthe', 1),\n             ('flooded', 1),\n             ('eager', 1),\n             ('racers', 1),\n             ('number', 10),\n             ('squeezedthe', 1),\n             ('financial', 1),\n             ('pressures', 1),\n             ('caterham', 3),\n             ('marussia', 2),\n             ('went', 5),\n             ('administration', 8),\n             ('october', 4),\n             ('theory', 1),\n             ('spots', 1),\n             ('grid', 1),\n             ('inafter', 1),\n             ('missing', 1),\n             ('us', 32),\n             ('brazil', 1),\n             ('grands', 1),\n             ('used', 6),\n             ('crowdfunding', 1),\n             ('finance', 1),\n             ('dhabijapan', 1),\n             ('kamui', 1),\n             ('kobayashi', 1),\n             ('briton', 1),\n             ('former', 5),\n             ('tester', 1),\n             ('stevens', 1),\n             ('debutboth', 1),\n             ('entry', 2),\n             ('list', 4),\n             ('dependent', 1),\n             ('hard', 5),\n             ('winter', 1),\n             ('drumming', 1),\n             ('stay', 2),\n             ('sportwith', 1),\n             ('spending', 2),\n             ('minimum', 1),\n             ('per', 2),\n             ('employing', 1),\n             ('offers', 2),\n             ('pace', 1),\n             ('performance', 1),\n             ('pot', 1),\n             ('personal', 5),\n             ('talent', 1),\n             ('important', 10),\n             ('ever', 1),\n             ('ca', 5),\n             ('afford', 1),\n             ('supertalents', 1),\n             ('kyle', 2),\n             ('white', 29),\n             ('pieces', 1),\n             ('metal', 3),\n             ('wear', 4),\n             ('bracelet', 2),\n             ('inscribed', 1),\n             ('names', 1),\n             ('six', 3),\n             ('comrades', 2),\n             ('killed', 15),\n             ('ambush', 3),\n             ('afghanistan', 14),\n             ('medal', 9),\n             ('given', 3),\n             ('valor', 1),\n             ('ensured', 1),\n             ('death', 11),\n             ('toll', 1),\n             ('higherspeaking', 1),\n             ('minutes', 3),\n             ('president', 17),\n             ('barack', 2),\n             ('obama', 13),\n             ('highest', 2),\n             ('military', 14),\n             ('insisted', 2),\n             ('emblems', 1),\n             ('equally', 1),\n             ('represent', 2),\n             ('family', 5),\n             ('ago', 2),\n             ('survived', 4),\n             ('well', 6),\n             ('notthe', 1),\n             ('army', 5),\n             ('sergeant', 1),\n             ('tuesday', 5),\n             ('owes', 1),\n             ('calls', 1),\n             ('heroes', 1),\n             ('live', 4),\n             ('life', 4),\n             ('honorthough', 1),\n             ('uncomfortable', 1),\n             ('hearing', 1),\n             ('name', 3),\n             ('word', 1),\n             ('hero', 1),\n             ('sentence', 1),\n             ('ready', 1),\n             ('challenge', 4),\n             ('proudly', 1),\n             ('wearing', 4),\n             ('piece', 1),\n             ('blue', 1),\n             ('fabric', 2),\n             ('carved', 1),\n             ('reverence', 1),\n             ('vow', 1),\n             ('responsibility', 5),\n             ('saidnot', 1),\n             ('long', 2),\n             ('recalled', 2),\n             ('bravery', 1),\n             ('colleaguesthe', 1),\n             ('paid', 1),\n             ('tribute', 3),\n             ('died', 2),\n             ('done', 2),\n             ('everything', 3),\n             ('country', 10),\n             ('could', 27),\n             ('ask', 1),\n             ('morekyle', 1),\n             ('members', 9),\n             ('chosen', 2),\n             ('company', 9),\n             ('duty', 2),\n             ('time', 16),\n             ('america', 3),\n             ('ourswhite', 1),\n             ('really', 12),\n             ('individual', 1),\n             ('award', 3),\n             ('calling', 2),\n             ('testament', 3),\n             ('trust', 1),\n             ('leadersstill', 1),\n             ('deserved', 1),\n             ('singled', 1),\n             ('freshman', 1),\n             ('twin', 1),\n             ('towers', 1),\n             ('fell', 4),\n             ('september', 2),\n             ('joined', 2),\n             ('old', 1),\n             ('months', 3),\n             ('service', 8),\n             ('faced', 1),\n             ('ultimate', 1),\n             ('testhe', 1),\n             ('aced', 1),\n             ('represented', 1),\n             ('called', 11),\n             ('generation', 2),\n             ('proven', 2),\n             ('greatesttoday', 1),\n             ('crowd', 1),\n             ('included', 1),\n             ('parents', 3),\n             ('soldier', 6),\n             ('embodies', 2),\n             ('courage', 1),\n             ('generationattacked', 1),\n             ('alleyon', 1),\n             ('dressed', 2),\n             ('uniform', 1),\n             ('weekdays', 1),\n             ('wears', 1),\n             ('suit', 1),\n             ('job', 5),\n             ('investment', 1),\n             ('analyst', 2),\n             ('bank', 2),\n             ('charlotte', 1),\n             ('north', 12),\n             ('carolina', 1),\n             ('admitted', 1),\n             ('laugh', 1),\n             ('less', 3),\n             ('exciting', 1),\n             ('previous', 1),\n             ('armythe', 1),\n             ('washington', 8),\n             ('state', 8),\n             ('native', 1),\n             ('following', 3),\n             ('father', 2),\n             ('special', 4),\n             ('member', 3),\n             ('earned', 5),\n             ('ticket', 1),\n             ('platoon', 2),\n             ('radio', 4),\n             ('telephone', 1),\n             ('operatorhe', 1),\n             ('walking', 6),\n             ('meeting', 2),\n             ('elders', 1),\n             ('unit', 3),\n             ('squad', 1),\n             ('afghan', 2),\n             ('soldiersthey', 1),\n             ('knew', 2),\n             ('stop', 3),\n             ('singlefile', 1),\n             ('cliff', 2),\n             ('steep', 2),\n             ('rocky', 1),\n             ('slope', 1),\n             ('heading', 1),\n             ('area', 7),\n             ('known', 6),\n             ('alleyin', 1),\n             ('interview', 2),\n             ('prior', 3),\n             ('ceremony', 3),\n             ('walked', 1),\n             ('little', 5),\n             ('incline', 1),\n             ('looking', 1),\n             ('valley', 1),\n             ('shot', 2),\n             ('shots', 1),\n             ('echo', 1),\n             ('fully', 2),\n             ('automatic', 1),\n             ('gunfiretaking', 1),\n             ('fire', 7),\n             ('patrol', 2),\n             ('separated', 2),\n             ('cover', 4),\n             ('finishing', 1),\n             ('magazine', 3),\n             ('beginning', 2),\n             ('load', 1),\n             ('another', 12),\n             ('rocketpropelled', 1),\n             ('grenade', 1),\n             ('exploded', 3),\n             ('knocking', 1),\n             ('unconsciousmoments', 1),\n             ('came', 7),\n             ('enemy', 4),\n             ('round', 3),\n             ('hit', 5),\n             ('rock', 4),\n             ('inches', 1),\n             ('head', 9),\n             ('shrapnel', 1),\n             ('fragments', 1),\n             ('cut', 4),\n             ('facedazed', 1),\n             ('struggled', 1),\n             ('happening', 1),\n             ('soldiers', 3),\n             ('jumped', 1),\n             ('administered', 1),\n             ('aid', 5),\n             ('wounded', 5),\n             ('using', 1),\n             ('available', 2),\n             ('tree', 2),\n             ('surviveit', 1),\n             ('point', 1),\n             ('attack', 7),\n             ('realized', 1),\n             ('workinghe', 1),\n             ('looked', 3),\n             ('saw', 3),\n             ('feet', 1),\n             ('away', 7),\n             ('whose', 3),\n             ('wounds', 1),\n             ('bad', 10),\n             ('toward', 2),\n             ('braving', 1),\n             ('firewhite', 1),\n             ('able', 2),\n             ('drag', 1),\n             ('treebut', 1),\n             ('injuries', 2),\n             ('severe', 2),\n             ('diedrisking', 1),\n             ('againwhite', 1),\n             ('continued', 5),\n             ('risk', 7),\n             ('fellow', 3),\n             ('warriors', 1),\n             ('running', 3),\n             ('reach', 6),\n             ('leader', 5),\n             ('publication', 1),\n             ('stars', 2),\n             ('stripes', 1),\n             ('see', 8),\n             ('helmet', 1),\n             ('assault', 1),\n             ('pack', 2),\n             ('tell', 4),\n             ('whether', 4),\n             ('alive', 1),\n             ('saidwhite', 1),\n             ('crawled', 1),\n             ('late', 4),\n             ('deadwhite', 1),\n             ('figured', 1),\n             ('trained', 3),\n             ('carry', 1),\n             ('dutyit', 1),\n             ('never', 6),\n             ('choice', 1),\n             ('explained', 1),\n             ('gon', 1),\n             ('na', 1),\n             ('make', 3),\n             ('thisbut', 1),\n             ('focused', 2),\n             ('dragged', 2),\n             ('earlier', 4),\n             ('knee', 1),\n             ('wrapped', 1),\n             ('belt', 1),\n             ('around', 6),\n             ('leg', 1),\n             ('creating', 1),\n             ('tourniquetthen', 1),\n             ('working', 5),\n             ('deceased', 1),\n             ('comrade', 1),\n             ('artillery', 1),\n             ('helicopter', 1),\n             ('gunships', 1),\n             ('helpfinally', 1),\n             ('hope', 1),\n             ('friendly', 1),\n             ('mortar', 1),\n             ('landed', 3),\n             ('near', 5),\n             ('whitei', 1),\n             ('remember', 2),\n             ('hot', 1),\n             ('chunks', 1),\n             ('size', 1),\n             ('palm', 1),\n             ('flinging', 1),\n             ('stripessuffering', 1),\n             ('concussion', 1),\n             ('managed', 3),\n             ('hang', 1),\n             ('waiting', 1),\n             ('helicopters', 2),\n             ('evacuate', 1),\n             ('arrived', 1),\n             ('rescuers', 1),\n             ('aboard', 2),\n             ('firsta', 1),\n             ('changedspeaking', 1),\n             ('national', 8),\n             ('public', 6),\n             ('experience', 3),\n             ('violence', 5),\n             ('wait', 2),\n             ('seemed', 1),\n             ('forever', 1),\n             ('entirely', 2),\n             ('gone', 2),\n             ('laterit', 1),\n             ('something', 8),\n             ('think', 17),\n             ('every', 8),\n             ('images', 1),\n             ('burned', 1),\n             ('goes', 1),\n             ('gets', 2),\n             ('easierbut', 1),\n             ('inside', 2),\n             ('changed', 1),\n             ('saideven', 1),\n             ('say', 12),\n             ('good', 6),\n             ('npr', 1),\n             ('pretty', 1),\n             ('reason', 1),\n             ('decided', 3),\n             ('armywhite', 1),\n             ('home', 14),\n             ('paratrooopers', 1),\n             ('reenlist', 1),\n             ('thought', 6),\n             ('felt', 2),\n             ('doubted', 1),\n             ('devote', 1),\n             ('complete', 3),\n             ('mind', 1),\n             ('nprit', 1),\n             ('unacceptable', 1),\n             ('continue', 2),\n             ('perhaps', 1),\n             ('deployed', 1),\n             ('deserve', 1),\n             ('explainedobama', 1),\n             ('february', 2),\n             ('recipient', 2),\n             ('actions', 4),\n             ('seventh', 1),\n             ('surviving', 1),\n             ('received', 5),\n             ('posthumously', 1),\n             ('war', 2),\n             ('iraq', 11),\n             ('according', 14),\n             ('congressional', 3),\n             ('societyin', 1),\n             ('brief', 1),\n             ('statement', 11),\n             ('reporters', 2),\n             ('symbol', 1),\n             ('knowingly', 1),\n             ('face', 4),\n             ('depart', 1),\n             ('distant', 2),\n             ('lands', 1),\n             ('defense', 7),\n             ('nation', 5),\n             ('locks', 1),\n             ('bonds', 1),\n             ('brotherhoodas', 1),\n             ('brothers', 2),\n             ('armswithout', 1),\n             ('teamread', 1),\n             ('transcript', 1),\n             ('house', 7),\n             ('ceremonyminority', 1),\n             ('veterans', 1),\n             ('receive', 1),\n             ('overdue', 1),\n             ('honorsee', 1),\n             ('profilecnn', 1),\n             ('barbara', 1),\n             ('starr', 1),\n             ('contributed', 5),\n             ('report', 9),\n             ('turkey', 12),\n             ('main', 5),\n             ('political', 5),\n             ('parties', 2),\n             ('pushing', 1),\n             ('constitutional', 2),\n             ('amendment', 1),\n             ('lift', 1),\n             ('bans', 1),\n             ('headscarves', 5),\n             ('universities', 2),\n             ('caused', 4),\n             ('concern', 4),\n             ('among', 4),\n             ('secular', 6),\n             ('populationthe', 1),\n             ('lifting', 1),\n             ('ban', 2),\n             ('populationprime', 1),\n             ('minister', 3),\n             ('recep', 1),\n             ('tayyip', 1),\n             ('erdogan', 1),\n             ('initiated', 1),\n             ('saying', 4),\n             ('create', 3),\n             ('equality', 1),\n             ('higher', 4),\n             ('educationthe', 1),\n             ('commission', 5),\n             ('discuss', 2),\n             ('proposal', 6),\n             ('submitted', 2),\n             ('akp', 1),\n             ('mhp', 1),\n             ('coming', 1),\n             ('days', 13),\n             ('sending', 4),\n             ('floor', 1),\n             ('voteif', 1),\n             ('approved', 1),\n             ('need', 3),\n             ('abdullah', 1),\n             ('gul', 1),\n             ('approval', 2),\n             ('expectedunder', 1),\n             ('veils', 1),\n             ('burqas', 1),\n             ('chaddors', 1),\n             ('allowedbans', 1),\n             ('headcoverings', 1),\n             ('imposed', 2),\n             ('early', 7),\n             ('symbols', 1),\n             ('conflicted', 1),\n             ('governing', 1),\n             ('systemthe', 1),\n             ('change', 2),\n             ('constitution', 1),\n             ('chills', 1),\n             ('population', 3),\n             ('women', 4),\n             ('groups', 8),\n             ('parliament', 1),\n             ('voice', 2),\n             ('rejectionthis', 1),\n             ('direct', 1),\n             ('threat', 2),\n             ('republic', 1),\n             ('foundations', 1),\n             ('deniz', 1),\n             ('baykal', 1),\n             ('party', 5),\n             ('chpanother', 1),\n             ('chp', 1),\n             ('lawmaker', 1),\n             ('fears', 2),\n             ('enacted', 1),\n             ('feel', 4),\n             ('pressure', 1),\n             ('daughters', 4),\n             ('elementary', 1),\n             ('schoolmustafa', 1),\n             ('akaydin', 2),\n             ('education', 2),\n             ('allowing', 1),\n             ('rejection', 1),\n             ('system', 2),\n             ('governmentit', 1),\n             ('attempt', 1),\n             ('counterrevolution', 1),\n             ('breaking', 1),\n             ('pointhe', 1),\n             ('majority', 2),\n             ('female', 1),\n             ('students', 3),\n             ...])"
     },
     "metadata": {},
     "execution_count": 22
    }
   ],
   "source": [
    "X_tokenizer.word_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: why 90? this will probably fuck up the embedding layer because we only have 51 words in the abstract and the embedding thinks our vocabulary has size 90\n",
    "Y_tokenizer = Tokenizer(num_words = 3000)\n",
    "Y_tokenizer.fit_on_texts(highlights)\n",
    "Y_voc = Y_tokenizer.num_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "OrderedDict([('sos', 22),\n             ('captive', 1),\n             ('boys', 1),\n             ('and', 11),\n             ('men', 1),\n             ('were', 4),\n             ('rescued', 1),\n             ('from', 5),\n             ('an', 4),\n             ('islamic', 1),\n             ('religious', 1),\n             ('school', 2),\n             ('in', 24),\n             ('pakistanthey', 1),\n             ('reunited', 1),\n             ('with', 7),\n             ('their', 1),\n             ('families', 1),\n             ('this', 3),\n             ('weekthe', 1),\n             ('facility', 2),\n             ('was', 6),\n             ('a', 14),\n             ('drug', 1),\n             ('rehab', 1),\n             ('clinicauthorities', 1),\n             ('say', 2),\n             ('they', 2),\n             ('re', 1),\n             ('searching', 1),\n             ('for', 6),\n             ('the', 33),\n             ('owners', 1),\n             ('three', 1),\n             ('others', 1),\n             ('arrested', 1),\n             ('at', 4),\n             ('eos', 22),\n             ('sebastian', 1),\n             ('vettel', 1),\n             ('signs', 1),\n             ('threeyear', 1),\n             ('contract', 1),\n             ('ferrari', 1),\n             ('fromred', 1),\n             ('bull', 1),\n             ('s', 11),\n             ('fourtime', 1),\n             ('world', 1),\n             ('champion', 2),\n             ('replaces', 1),\n             ('fernando', 1),\n             ('alonsoalonso', 1),\n             ('has', 10),\n             ('still', 2),\n             ('to', 26),\n             ('announce', 2),\n             ('which', 1),\n             ('team', 3),\n             ('he', 6),\n             ('will', 3),\n             ('drive', 1),\n             ('next', 1),\n             ('seasonmclaren', 1),\n             ('says', 7),\n             ('it', 3),\n             ('not', 2),\n             ('its', 4),\n             ('driver', 1),\n             ('lineup', 1),\n             ('until', 1),\n             ('december', 1),\n             ('new', 2),\n             ('kyle', 1),\n             ('white', 2),\n             ('without', 1),\n             ('there', 1),\n             ('would', 1),\n             ('be', 4),\n             ('no', 2),\n             ('medal', 1),\n             ('of', 20),\n             ('honornew', 1),\n             ('vows', 1),\n             ('live', 1),\n             ('up', 2),\n             ('responsibility', 2),\n             ('having', 1),\n             ('top', 1),\n             ('military', 2),\n             ('awardnew', 1),\n             ('obama', 1),\n             ('calls', 1),\n             ('soldier', 1),\n             ('who', 2),\n             ('embodies', 1),\n             ('courage', 1),\n             ('his', 4),\n             ('generationthe', 1),\n             ('army', 1),\n             ('vet', 1),\n             ('then', 1),\n             ('braved', 1),\n             ('enemy', 2),\n             ('fire', 1),\n             ('save', 1),\n             ('wounded', 1),\n             ('comrades', 1),\n             ('afghanistan', 1),\n             ('turkey', 1),\n             ('ruling', 1),\n             ('party', 1),\n             ('agrees', 1),\n             ('lift', 1),\n             ('ban', 1),\n             ('on', 8),\n             ('head', 2),\n             ('scarves', 1),\n             ('universitiesban', 1),\n             ('introduced', 1),\n             ('after', 4),\n             ('coup', 1),\n             ('as', 4),\n             ('seen', 1),\n             ('sign', 1),\n             ('religionturkey', 1),\n             ('is', 9),\n             ('secular', 2),\n             ('nation', 2),\n             ('but', 2),\n             ('population', 2),\n             ('mainly', 1),\n             ('muslimproposal', 1),\n             ('brought', 1),\n             ('protests', 1),\n             ('among', 1),\n             ('designer', 2),\n             ('isaac', 1),\n             ('mizrahi', 1),\n             ('moving', 1),\n             ('target', 1),\n             ('creative', 1),\n             ('director', 1),\n             ('liz', 1),\n             ('claibornemizrahi', 1),\n             ('believes', 1),\n             ('bad', 2),\n             ('hair', 1),\n             ('most', 1),\n             ('common', 2),\n             ('style', 2),\n             ('mistake', 1),\n             ('prefers', 1),\n             ('flowers', 1),\n             ('onesmizrahi', 1),\n             ('admits', 1),\n             ('claustrophobic', 1),\n             ('personality', 1),\n             ('boy', 1),\n             ('latest', 1),\n             ('victim', 1),\n             ('maneating', 2),\n             ('leopard', 2),\n             ('local', 1),\n             ('police', 1),\n             ('chief', 1),\n             ('sayshe', 1),\n             ('suspects', 1),\n             ('one', 1),\n             ('behind', 1),\n             ('deaths', 1),\n             ('people', 1),\n             ('past', 2),\n             ('monthsa', 1),\n             ('reward', 1),\n             ('been', 4),\n             ('offered', 1),\n             ('anyone', 1),\n             ('captures', 1),\n             ('or', 3),\n             ('kills', 1),\n             ('creatureleopards', 1),\n             ('are', 2),\n             ('low', 1),\n             ('mountain', 1),\n             ('areas', 1),\n             ('nepal', 1),\n             ('usually', 1),\n             ('eat', 1),\n             ('wild', 1),\n             ('prey', 1),\n             ('like', 1),\n             ('deer', 1),\n             ('us', 3),\n             ('embassy', 2),\n             ('condemns', 1),\n             ('senseless', 1),\n             ('targeting', 1),\n             ('christians', 2),\n             ('other', 3),\n             ('iraqisin', 1),\n             ('iraq', 2),\n             ('car', 1),\n             ('bomb', 1),\n             ('explodes', 1),\n             ('outside', 1),\n             ('church', 1),\n             ('christmas', 2),\n             ('services', 1),\n             ('another', 1),\n             ('targets', 1),\n             ('markettaliban', 1),\n             ('claim', 1),\n             ('attack', 1),\n             ('kabul', 1),\n             ('afghanistanmany', 1),\n             ('have', 2),\n             ('fled', 1),\n             ('over', 4),\n             ('decade', 1),\n             ('report', 2),\n             ('medieval', 1),\n             ('times', 1),\n             ('timbuktu', 1),\n             ('presentday', 1),\n             ('mali', 1),\n             ('important', 2),\n             ('intellectual', 1),\n             ('centerknown', 1),\n             ('great', 1),\n             ('mosques', 1),\n             ('trove', 1),\n             ('manuscripts', 1),\n             ('city', 2),\n             ('worldheritage', 1),\n             ('statusislamist', 1),\n             ('tuareg', 1),\n             ('rebels', 2),\n             ('occupied', 1),\n             ('recent', 1),\n             ('weeksunesco', 1),\n             ('fears', 1),\n             ('site', 1),\n             ('could', 2),\n             ('destroyed', 1),\n             ('looted', 1),\n             ('by', 2),\n             ('religion', 1),\n             ('professor', 1),\n             ('candida', 1),\n             ('moss', 1),\n             ('appears', 1),\n             ('each', 2),\n             ('episode', 1),\n             ('programmoss', 1),\n             ('part', 1),\n             ('original', 1),\n             ('study', 1),\n             ('determine', 1),\n             ('if', 2),\n             ('relics', 1),\n             ('found', 1),\n             ('bulgaria', 1),\n             ('bones', 1),\n             ('john', 1),\n             ('baptist', 1),\n             ('bermudan', 1),\n             ('premier', 1),\n             ('above', 1),\n             ('all', 2),\n             ('humanitarian', 1),\n             ('actuyghurs', 1),\n             ('native', 1),\n             ('chinese', 1),\n             ('muslims', 1),\n             ('detainees', 1),\n             ('apprehended', 1),\n             ('pakistanchina', 1),\n             ('urges', 1),\n             ('hand', 1),\n             ('uyghurs', 2),\n             ('held', 1),\n             ('guantanamo', 1),\n             ('bay', 1),\n             ('cubaofficial', 1),\n             ('negotiating', 1),\n             ('palau', 1),\n             ('take', 1),\n             ('remaining', 1),\n             ('former', 1),\n             ('spain', 2),\n             ('national', 1),\n             ('coach', 1),\n             ('luis', 1),\n             ('aragones', 1),\n             ('passes', 1),\n             ('away', 1),\n             ('madrid', 1),\n             ('saturdaythe', 1),\n             ('had', 1),\n             ('suffering', 1),\n             ('health', 2),\n             ('issues', 3),\n             ('since', 2),\n             ('retirement', 1),\n             ('last', 2),\n             ('yearhe', 1),\n             ('credited', 1),\n             ('restoring', 1),\n             ('fortunes', 1),\n             ('international', 1),\n             ('football', 1),\n             ('poweraragones', 1),\n             ('led', 1),\n             ('la', 1),\n             ('roja', 1),\n             ('european', 2),\n             ('title', 1),\n             ('revitalizing', 1),\n             ('play', 1),\n             ('north', 2),\n             ('central', 1),\n             ('west', 1),\n             ('virginia', 1),\n             ('airport', 1),\n             ('offers', 1),\n             ('free', 2),\n             ('sightseeing', 1),\n             ('flights', 2),\n             ('boost', 1),\n             ('numbersother', 1),\n             ('small', 1),\n             ('airports', 1),\n             ('offer', 1),\n             ('light', 1),\n             ('tours', 1),\n             ('increase', 1),\n             ('passenger', 1),\n             ('trafficrepublican', 1),\n             ('senator', 1),\n             ('wants', 1),\n             ('faa', 1),\n             ('government', 1),\n             ('accountability', 1),\n             ('office', 1),\n             ('investigate', 1),\n             ('red', 1),\n             ('cross', 1),\n             ('south', 2),\n             ('korea', 2),\n             ('channel', 1),\n             ('aid', 1),\n             ('koreathe', 1),\n             ('communist', 1),\n             ('replied', 1),\n             ('offerin', 1),\n             ('august', 1),\n             ('heavy', 1),\n             ('rains', 1),\n             ('caused', 1),\n             ('rivers', 1),\n             ('swell', 1),\n             ('fda', 2),\n             ('ordered', 1),\n             ('stop', 1),\n             ('sales', 3),\n             ('home', 1),\n             ('genetic', 1),\n             ('testing', 2),\n             ('kitssupporters', 1),\n             ('took', 1),\n             ('social', 1),\n             ('media', 1),\n             ('protest', 1),\n             ('decisiongenetic', 1),\n             ('can', 1),\n             ('powerful', 1),\n             ('tool', 1),\n             ('used', 1),\n             ('correctly', 1),\n             ('experts', 1),\n             ('union', 1),\n             ('announces', 1),\n             ('imposing', 1),\n             ('sanctions', 2),\n             ('peopledefense', 1),\n             ('ministers', 1),\n             ('united', 1),\n             ('states', 1),\n             ('russia', 1),\n             ('discuss', 1),\n             ('ukraineus', 1),\n             ('imposes', 1),\n             ('against', 1),\n             ('russians', 1),\n             ('companies', 1),\n             ('linked', 1),\n             ('putinthe', 1),\n             ('mayor', 1),\n             ('kharkiv', 1),\n             ('critical', 1),\n             ('condition', 1),\n             ('emergency', 1),\n             ('surgery', 2),\n             ('judge', 1),\n             ('sentences', 1),\n             ('joshua', 1),\n             ('komisarjevsky', 2),\n             ('deaththe', 1),\n             ('jury', 2),\n             ('voted', 1),\n             ('death', 2),\n             ('lethal', 1),\n             ('injection', 1),\n             ('six', 2),\n             ('countsanother', 1),\n             ('man', 1),\n             ('also', 1),\n             ('sentenced', 1),\n             ('casea', 1),\n             ('convicted', 1),\n             ('october', 1),\n             ('felony', 1),\n             ('charges', 1),\n             ('more', 1),\n             ('than', 1),\n             ('passengers', 1),\n             ('caribbean', 1),\n             ('princess', 1),\n             ('being', 1),\n             ('illa', 1),\n             ('spokeswoman', 1),\n             ('outbreak', 1),\n             ('confirmed', 1),\n             ('noroviruspassengers', 1),\n             ('crew', 1),\n             ('experience', 1),\n             ('vomiting', 1),\n             ('diarrhea', 1),\n             ('andy', 1),\n             ('murray', 3),\n             ('falls', 1),\n             ('jowilfried', 1),\n             ('tsonga', 1),\n             ('exhibition', 1),\n             ('tournament', 1),\n             ('abu', 1),\n             ('dhabiwimbledon', 1),\n             ('loses', 1),\n             ('hardhitting', 1),\n             ('frenchmanit', 1),\n             ('first', 2),\n             ('match', 2),\n             ('undergoing', 1),\n             ('back', 1),\n             ('septemberdavid', 1),\n             ('ferrer', 1),\n             ('defeats', 1),\n             ('stanislas', 1),\n             ('wawrinka', 1),\n             ('straight', 1),\n             ('sets', 1),\n             ('raul', 1),\n             ('reyes', 1),\n             ('seeking', 1),\n             ('latino', 1),\n             ('vote', 1),\n             ('marco', 1),\n             ('rubio', 1),\n             ('own', 1),\n             ('worst', 1),\n             ('two', 1),\n             ('key', 1),\n             ('immigration', 1),\n             ('reform', 1),\n             ('cuba', 1),\n             ('relationshe', 1),\n             ('care', 1),\n             ('climate', 1),\n             ('change', 1),\n             ('breaks', 1),\n             ('latinos', 1),\n             ('positions', 1),\n             ('polls', 1),\n             ('show', 1),\n             ('do', 1),\n             ('nt', 1),\n             ('favor', 1),\n             ('him', 1),\n             ('down', 1),\n             ('percent', 2),\n             ('across', 1),\n             ('companyelectronics', 1),\n             ('games', 1),\n             ('especially', 1),\n             ('hard', 1),\n             ('hit', 1),\n             ('falling', 1),\n             ('moresony', 1),\n             ('warned', 1),\n             ('week', 1),\n             ('close', 1),\n             ('out', 1),\n             ('fiscal', 1),\n             ('year', 1),\n             ('operating', 1),\n             ('loss', 1),\n             ('billion', 1),\n             ('soyuz', 2),\n             ('capsule', 2),\n             ('lands', 1),\n             ('hundreds', 1),\n             ('kilometers', 1),\n             ('offtargetcapsule', 1),\n             ('carrying', 1),\n             ('astronautlanding', 1),\n             ('second', 1),\n             ('time', 1),\n             ('gone', 1),\n             ('awry', 1),\n             ('china', 3),\n             ('quality', 1),\n             ('watchdog', 1),\n             ('resigns', 1),\n             ('tainted', 2),\n             ('baby', 1),\n             ('formula', 1),\n             ('scandalwho', 1),\n             ('representative', 1),\n             ('said', 1),\n             ('scandal', 1),\n             ('shows', 1),\n             ('flaws', 1),\n             ('food', 1),\n             ('supply', 1),\n             ('chainfour', 1),\n             ('infants', 1),\n             ('dead', 1),\n             ('reported', 1),\n             ('ill', 1),\n             ('milk', 1),\n             ('powder', 1)])"
     },
     "metadata": {},
     "execution_count": 24
    }
   ],
   "source": [
    "Y_tokenizer.word_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_to_idx(data: list) -> dict:\n",
    "    \"\"\"\n",
    "    Function that maps the data and return a dictionary of words corresponding to their index\n",
    "    it gets a list\n",
    "    return:\n",
    "        dict 1 idx to word\n",
    "        dict 2 word to idx\n",
    "    \"\"\"\n",
    "    total_letters = [letters for sublist in data for subsublist in sublist for letters in subsublist]\n",
    "    unique_letters =set(total_letters)\n",
    "    total_words = [word.replace(',','') for sublist in data for subsublist in sublist for word in subsublist.split()]\n",
    "    unique_words =list(set(total_words))\n",
    "\n",
    "    w_2_i = {unique_words[i]:i for i in range(len(unique_words))}\n",
    "    i_2_w= {i: unique_words[i] for i in range(len(unique_words))}\n",
    "    print(w_2_i)\n",
    "    input()\n",
    "    return (w_2_i, i_2_w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train= X_tokenizer.texts_to_sequences(cleaned_text) \n",
    "y_train  = Y_tokenizer.texts_to_sequences(highlights)\n",
    "X_train = pad_sequences(X_train,  maxlen = max_ar_length, padding = 'post')\n",
    "y_train =  pad_sequences(y_train,  maxlen = max_high_lengh, padding = 'post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "22"
     },
     "metadata": {},
     "execution_count": 27
    }
   ],
   "source": [
    "len(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "22"
     },
     "metadata": {},
     "execution_count": 28
    }
   ],
   "source": [
    "len(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "3000"
     },
     "metadata": {},
     "execution_count": 29
    }
   ],
   "source": [
    "X_tokenizer.num_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "3000"
     },
     "metadata": {},
     "execution_count": 30
    }
   ],
   "source": [
    "Y_tokenizer.num_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([2, 9, 3, ..., 0, 0, 0], dtype=int32)"
     },
     "metadata": {},
     "execution_count": 31
    }
   ],
   "source": [
    "X_train[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([  4, 128, 129, 130,   7, 131, 132,  13, 133, 134, 135,   9, 136,\n       137,  44, 138, 139, 140,  10,  45,   2,  46, 141,  30,  17,  31,\n       142,  16, 143, 144,  14,  32,  31,  47,  46,  22, 145, 146, 147,\n       148,   5,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n         0,   0,   0,   0], dtype=int32)"
     },
     "metadata": {},
     "execution_count": 32
    }
   ],
   "source": [
    "y_train[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_voc = X_tokenizer.num_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "str"
     },
     "metadata": {},
     "execution_count": 34
    }
   ],
   "source": [
    "type(highlights[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.array(y_train)\n",
    "x = np.array(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "(22, 56)"
     },
     "metadata": {},
     "execution_count": 36
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "(22, 1329)"
     },
     "metadata": {},
     "execution_count": 37
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([[   2,    9,    3, ...,    0,    0,    0],\n       [   2,    9,    3, ...,    0,    0,    0],\n       [   2,    9,    3, ...,    0,    0,    0],\n       ...,\n       [1252,    2,    9, ...,    0,    0,    0],\n       [ 689,  311,    2, ...,    0,    0,    0],\n       [ 447,   23,    2, ...,    0,    0,    0]], dtype=int32)"
     },
     "metadata": {},
     "execution_count": 38
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([[  4, 108, 109, ...,   0,   0,   0],\n       [  4, 128, 129, ...,   0,   0,   0],\n       [  4,  48, 149, ...,   5,   0,   0],\n       ...,\n       [  4,  37,  19, ...,   0,   0,   0],\n       [  4, 105, 106, ...,   0,   0,   0],\n       [  4,  56,   6, ...,   0,   0,   0]], dtype=int32)"
     },
     "metadata": {},
     "execution_count": 39
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "2022-01-21 19:11:29.388029: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\nModel: \"model\"\n__________________________________________________________________________________________________\n Layer (type)                   Output Shape         Param #     Connected to                     \n==================================================================================================\n input_1 (InputLayer)           [(None, 1329)]       0           []                               \n                                                                                                  \n input_2 (InputLayer)           [(None, 55)]         0           []                               \n                                                                                                  \n embedding (Embedding)          (None, 1329, 100)    300000      ['input_1[0][0]']                \n                                                                                                  \n embedding_1 (Embedding)        (None, 55, 100)      300000      ['input_2[0][0]']                \n                                                                                                  \n lstm (LSTM)                    [(None, 1329, 100),  80400       ['embedding[0][0]']              \n                                 (None, 100),                                                     \n                                 (None, 100)]                                                     \n                                                                                                  \n lstm_1 (LSTM)                  [(None, 55, 100),    80400       ['embedding_1[0][0]',            \n                                 (None, 100),                     'lstm[0][1]',                   \n                                 (None, 100)]                     'lstm[0][2]']                   \n                                                                                                  \n time_distributed (TimeDistribu  (None, 55, 3000)    303000      ['lstm_1[0][0]']                 \n ted)                                                                                             \n                                                                                                  \n==================================================================================================\nTotal params: 1,063,800\nTrainable params: 1,063,800\nNon-trainable params: 0\n__________________________________________________________________________________________________\nEpoch 1/20\n5/5 [==============================] - 39s 4s/step - loss: 5.8717 - accuracy: 0.0236\nEpoch 2/20\n5/5 [==============================] - 16s 3s/step - loss: 5.4412 - accuracy: 0.0372\nEpoch 3/20\n5/5 [==============================] - 18s 3s/step - loss: 4.8277 - accuracy: 0.0372\nEpoch 4/20\n5/5 [==============================] - 18s 4s/step - loss: 4.5881 - accuracy: 0.0338\nEpoch 5/20\n5/5 [==============================] - 18s 4s/step - loss: 4.4857 - accuracy: 0.0372\nEpoch 6/20\n5/5 [==============================] - 21s 4s/step - loss: 4.4304 - accuracy: 0.0372\nEpoch 7/20\n5/5 [==============================] - 17s 3s/step - loss: 4.3942 - accuracy: 0.0349\nEpoch 8/20\n5/5 [==============================] - 17s 3s/step - loss: 4.3681 - accuracy: 0.0372\nEpoch 9/20\n5/5 [==============================] - 18s 4s/step - loss: 4.3458 - accuracy: 0.0372\nEpoch 10/20\n5/5 [==============================] - 17s 3s/step - loss: 4.3261 - accuracy: 0.0383\nEpoch 11/20\n5/5 [==============================] - 17s 3s/step - loss: 4.3144 - accuracy: 0.0372\nEpoch 12/20\n5/5 [==============================] - 18s 4s/step - loss: 4.2999 - accuracy: 0.0372\nEpoch 13/20\n5/5 [==============================] - 17s 3s/step - loss: 4.2885 - accuracy: 0.0372\nEpoch 14/20\n5/5 [==============================] - 17s 3s/step - loss: 4.2763 - accuracy: 0.0372\nEpoch 15/20\n5/5 [==============================] - 17s 3s/step - loss: 4.2681 - accuracy: 0.0372\nEpoch 16/20\n5/5 [==============================] - 17s 3s/step - loss: 4.2573 - accuracy: 0.0360\nEpoch 17/20\n5/5 [==============================] - 17s 3s/step - loss: 4.2465 - accuracy: 0.0372\nEpoch 18/20\n5/5 [==============================] - 22s 5s/step - loss: 4.2343 - accuracy: 0.0372\nEpoch 19/20\n5/5 [==============================] - 18s 3s/step - loss: 4.2205 - accuracy: 0.0383\nEpoch 20/20\n5/5 [==============================] - 18s 4s/step - loss: 4.2065 - accuracy: 0.0405\n"
    }
   ],
   "source": [
    "\n",
    "from keras import backend as K \n",
    "K.clear_session()  #Resets all state generated by Keras\n",
    "\n",
    "latent_dim = 100\n",
    "embedding_dim = 100\n",
    "\n",
    "# Encoder\n",
    "encoder_inputs = Input(shape=(x.shape[1],))\n",
    "\n",
    "# TODO: understand how embedding works here; is this our own trained embedding? maybe we should just use word2vec\n",
    "#embedding layer\n",
    "enc_emb =  Embedding(X_voc, embedding_dim,trainable=True, mask_zero=True)(encoder_inputs)\n",
    "\n",
    "#encoder lstm\n",
    "# TODO: why are we not using an activation function? default is none, only for recurrent activation the default is sigmoid\n",
    "# TODO: why do we need the encoder_outputs? only for attention probably\n",
    "encoder_lstm = LSTM(latent_dim,return_sequences=True,return_state=True) # TODO: dropout=0.4,recurrent_dropout=0.4\n",
    "encoder_outputs, state_h, state_c = encoder_lstm(enc_emb)\n",
    "\n",
    "#Setting up the Decoder using 'encoder_states' as initial state\n",
    "# TODO: figure out why the shape is None? because we also use it later for our decoding when we only give one\n",
    "decoder_inputs = Input(shape=((y.shape[1] - 1),))\n",
    "\n",
    "#Embedding layer\n",
    "dec_emb_layer = Embedding(Y_voc, embedding_dim,trainable=True, mask_zero=True)\n",
    "dec_emb = dec_emb_layer(decoder_inputs)\n",
    "\n",
    "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True) # TODO: dropout=0.4,recurrent_dropout=0.4\n",
    "# TODO: is the LSTM bidirectional? why do we even need those two states?\n",
    "# TODO: why do the graphs say (None, 256) where 256 is the latent_dim (the length of the state vectors); shouldn't it be clear that it is (1, 256)?\n",
    "decoder_outputs ,decoder_fwd_state, decoder_back_state = decoder_lstm(dec_emb ,initial_state=[state_h, state_c])\n",
    "\n",
    "# #Attention layer; removed for now\n",
    "# attn_layer = AttentionLayer(name='attention_layer')\n",
    "# attn_out, attn_states = attn_layer([encoder_outputs, decoder_outputs])\n",
    "\n",
    "# #Concating Attention input and Decoder LSTM output\n",
    "# decoder_concat_input = Concatenate(axis=-1, name='concat_layer')([decoder_outputs, attn_out])\n",
    "\n",
    "#Dense layer\n",
    "# TODO: figure out what TimeDistributed does\n",
    "decoder_dense =  TimeDistributed(Dense(units = Y_voc, activation='softmax'))\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "#Defining the model \n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "\n",
    "model.summary()\n",
    "\n",
    "#Visualize the Model\n",
    "from tensorflow.keras.utils import plot_model\n",
    "\n",
    "plot_model(model, to_file='training_model_plot.png', show_shapes=True, show_layer_names=True)\n",
    "\n",
    "# TODO: understand this\n",
    "#Adding Metrics\n",
    "model.compile(optimizer='rmsprop' , loss='sparse_categorical_crossentropy' , metrics=['accuracy'])\n",
    "\n",
    "#Adding Callback\n",
    "# TODO: how exactly does this work? is there an internal mapping of the string \"val_loss\" to running a test on the specified validation_data?\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1)\n",
    "\n",
    "# Commented out IPython magic to ensure Python compatibility.\n",
    "#Training the Model\n",
    "# %tensorflow_version 1.x\n",
    "# indexing is clear, removing the eos token for the decoder inpu\n",
    "# ts and removing the sos token for the decoder outputs\n",
    "# TODO: think about how exactly this is working with calculating the loss etc., maybe that's the problem\n",
    "# TODO: fit only on one example and check if model actually works\n",
    "# this creates the third dimension we need to compare to the generated output of the decoder; basically creates \"one-hot encoding\" to generate the probability distribution\n",
    "history = model.fit(\n",
    "    x = [x,y[:,:-1]], \n",
    "    y = y.reshape(y.shape[0], y.shape[1], 1)[:, 1:, :],\n",
    "    epochs=200,\n",
    "    batch_size = 20\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[2 9 3 ... 0 0 0]\nWARNING:tensorflow:Model was constructed with shape (None, 55) for input KerasTensor(type_spec=TensorSpec(shape=(None, 55), dtype=tf.float32, name='input_2'), name='input_2', description=\"created by layer 'input_2'\"), but it was called on an input with incompatible shape (None, 1).\nPredicted summary:  the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the\n\n\n"
    }
   ],
   "source": [
    "#Building Dictionary for Source Vocabulary\n",
    "# TODO: what is happening here? we are just saving the vocabularies of the tokenizers (index -> word or word -> index maps)\n",
    "target_index_word=Y_tokenizer.index_word \n",
    "source_index_word=X_tokenizer.index_word \n",
    "target_word_index=Y_tokenizer.word_index\n",
    "\n",
    "#Testing phase\n",
    "#Encoding the input sequence to get the feature vector\n",
    "# TODO: is this what links this model to the trained model? that we use the same encoder_inputs Input object?\n",
    "encoder_model = Model(inputs=encoder_inputs,outputs=[encoder_outputs, state_h, state_c])\n",
    "\n",
    "#Decoder setup\n",
    "#These tensors will hold the states of the previous time step\n",
    "decoder_state_input_h = Input(shape=(latent_dim,))\n",
    "decoder_state_input_c = Input(shape=(latent_dim,))\n",
    "# TODO: remove this, probably only used for attention\n",
    "# decoder_hidden_state_input = Input(shape=(max_text_len,latent_dim))\n",
    "\n",
    "#Getting the embeddings of the decoder sequence\n",
    "dec_emb2= dec_emb_layer(decoder_inputs) \n",
    "\n",
    "#Setting the initial states to the states from the previous time step for better prediction\n",
    "decoder_outputs2, state_h2, state_c2 = decoder_lstm(dec_emb2, initial_state=[decoder_state_input_h, decoder_state_input_c])\n",
    "\n",
    "# TODO: remove this\n",
    "# #Attention inference\n",
    "# attn_out_inf, attn_states_inf = attn_layer([decoder_hidden_state_input, decoder_outputs2])\n",
    "# decoder_inf_concat = Concatenate(axis=-1, name='concat')([decoder_outputs2, attn_out_inf])\n",
    "\n",
    "#Adding Dense softmax layer to generate proability distribution over the target vocabulary\n",
    "decoder_outputs2 = decoder_dense(decoder_outputs2) \n",
    "\n",
    "#Final Decoder model\n",
    "# TODO: how is this all linked to what we trained before? where does the parameter sharing happen?\n",
    "# TODO: why are we appending the lists instead of just writing it in one list\n",
    "decoder_model = Model(\n",
    "    [decoder_inputs] + [decoder_state_input_h, decoder_state_input_c],\n",
    "    [decoder_outputs2] + [state_h2, state_c2])\n",
    "\n",
    "#Function defining the implementation of inference process\n",
    "def decode_sequence(input_seq):\n",
    "    #Encoding the input as state vectors\n",
    "    e_out, e_h, e_c = encoder_model.predict(input_seq)\n",
    "    \n",
    "    #Generating empty target sequence of length 1\n",
    "    target_seq = np.zeros((1,1))\n",
    "    \n",
    "    #Populating the first word of target sequence with the start word\n",
    "    target_seq[0, 0] = target_word_index['sos']\n",
    "\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "    while not stop_condition:\n",
    "        \n",
    "        output_tokens, h, c = decoder_model.predict([target_seq] + [e_h, e_c])\n",
    "\n",
    "        #Sampling a token\n",
    "        # TODO: understand the indexing: why -1 instead of 0 as well? shouldn't the output tokens be of shape (1, 1, num_words)?\n",
    "        # TODO: I think the neurons in the dense layer start at 0 but our dictionary starts at 1, despite a one-on-one mapping from\n",
    "        # dictionary index numbers to neurons if I understand correctly; doesn't make sense to make, programm will crash if arg_max returns zero\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_token = target_index_word[sampled_token_index]\n",
    "        \n",
    "        if(sampled_token!='eos'):\n",
    "            decoded_sentence += ' '+sampled_token\n",
    "\n",
    "        #Exit condition: either hit max length or find stop word\n",
    "        # TODO: it used to be >= (max_abstract_len - 1); why? doesn't make sense to me\n",
    "        if (sampled_token == 'eos'  or len(decoded_sentence.split()) == max_high_lengh):\n",
    "            stop_condition = True\n",
    "\n",
    "        #Updating the target sequence (of length 1)\n",
    "        target_seq = np.zeros((1,1))\n",
    "        target_seq[0, 0] = sampled_token_index\n",
    "\n",
    "        #Updating internal states\n",
    "        e_h, e_c = h, c\n",
    "\n",
    "    return decoded_sentence\n",
    "\n",
    "#Functions to convert an integer sequence to a word sequence for summary as well as reviews \n",
    "def seq2summary(input_seq):\n",
    "    newString=''\n",
    "    for i in input_seq:\n",
    "        if((i!=0 and i!=target_word_index['sos']) and i!=target_word_index['eos']):\n",
    "            newString=newString+target_index_word[i]+' '\n",
    "    return newString\n",
    "\n",
    "def seq2text(input_seq):\n",
    "    newString=''\n",
    "    for i in input_seq:\n",
    "        if(i!=0):\n",
    "            newString=newString+source_index_word[i]+' '\n",
    "    return newString\n",
    "\n",
    "#Summaries generated by the model\n",
    "\n",
    "print(X_train[0])\n",
    "print(\"Predicted summary:\",decode_sequence(X_train[0].reshape(1,-1)))\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1070,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "SyntaxError",
     "evalue": "invalid syntax (2442190599.py, line 1)",
     "traceback": [
      "\u001b[0;36m  Input \u001b[0;32mIn [1070]\u001b[0;36m\u001b[0m\n\u001b[0;31m    df = pd.DataFrame((cleaned_text, stories[]))\u001b[0m\n\u001b[0m                                             ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame((cleaned_text, stories[]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1071,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [1071]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mabstract\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mabstract\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x : \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msostok \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m+\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m eostok\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m#Splitting the Dataset twice to get 80% training data, 10% of validation data and 10% of test data\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m train_test_split\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "df['abstract'] = df['abstract'].apply(lambda x : 'sostok '+ x + ' eostok')\n",
    "\n",
    "#Splitting the Dataset twice to get 80% training data, 10% of validation data and 10% of test data\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train,X_val,y_train,y_val=train_test_split(np.array(df['article']),np.array(df['abstract']),test_size=0.2,random_state=0,shuffle=True)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_val, y_val, test_size = 0.5, random_state = 0, shuffle = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1072,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size =200\n",
    "src_txt_length =200\n",
    "sum_txt_length = 200\n",
    "# encoder input model\n",
    "inputs = Input(shape=(src_txt_length,))\n",
    "encoder1 = Embedding(vocab_size, 128)(inputs)\n",
    "encoder2 = LSTM(128)(encoder1)\n",
    "encoder3 = RepeatVector(sum_txt_length)(encoder2)\n",
    "# decoder output model\n",
    "decoder1 = LSTM(128, return_sequences=True)(encoder3)\n",
    "outputs = TimeDistributed(Dense(vocab_size, activation='softmax'))(decoder1)\n",
    "# tie it together\n",
    "model = Model(inputs=inputs, outputs=outputs)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1073,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'nn' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [1073]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mEncoderRNN\u001b[39;00m(\u001b[43mnn\u001b[49m\u001b[38;5;241m.\u001b[39mModule):\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, input_size, hidden_size):\n\u001b[1;32m      3\u001b[0m         \u001b[38;5;28msuper\u001b[39m(EncoderRNN, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'nn' is not defined"
     ]
    }
   ],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        embedded = self.embedding(input).view(1, 1, -1)\n",
    "        output = embedded\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1074,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'nn' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [1074]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mDecoderRNN\u001b[39;00m(\u001b[43mnn\u001b[49m\u001b[38;5;241m.\u001b[39mModule):\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, hidden_size, output_size):\n\u001b[1;32m      3\u001b[0m         \u001b[38;5;28msuper\u001b[39m(DecoderRNN, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'nn' is not defined"
     ]
    }
   ],
   "source": [
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        output = self.embedding(input).view(1, 1, -1)\n",
    "        output = F.relu(output)\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        output = self.softmax(self.out(output[0]))\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1075,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'nn' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [1075]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mAttnDecoderRNN\u001b[39;00m(\u001b[43mnn\u001b[49m\u001b[38;5;241m.\u001b[39mModule):\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, hidden_size, output_size, dropout_p\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m, max_length\u001b[38;5;241m=\u001b[39mMAX_LENGTH):\n\u001b[1;32m      3\u001b[0m         \u001b[38;5;28msuper\u001b[39m(AttnDecoderRNN, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'nn' is not defined"
     ]
    }
   ],
   "source": [
    "class AttnDecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size, dropout_p=0.1, max_length=MAX_LENGTH):\n",
    "        super(AttnDecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.dropout_p = dropout_p\n",
    "        self.max_length = max_length\n",
    "\n",
    "        self.embedding = nn.Embedding(self.output_size, self.hidden_size)\n",
    "        self.attn = nn.Linear(self.hidden_size * 2, self.max_length)\n",
    "        self.attn_combine = nn.Linear(self.hidden_size * 2, self.hidden_size)\n",
    "        self.dropout = nn.Dropout(self.dropout_p)\n",
    "        self.gru = nn.GRU(self.hidden_size, self.hidden_size)\n",
    "        self.out = nn.Linear(self.hidden_size, self.output_size)\n",
    "\n",
    "    def forward(self, input, hidden, encoder_outputs):\n",
    "        embedded = self.embedding(input).view(1, 1, -1)\n",
    "        embedded = self.dropout(embedded)\n",
    "\n",
    "        attn_weights = F.softmax(\n",
    "            self.attn(torch.cat((embedded[0], hidden[0]), 1)), dim=1)\n",
    "        attn_applied = torch.bmm(attn_weights.unsqueeze(0),\n",
    "                                 encoder_outputs.unsqueeze(0))\n",
    "\n",
    "        output = torch.cat((embedded[0], attn_applied[0]), 1)\n",
    "        output = self.attn_combine(output).unsqueeze(0)\n",
    "\n",
    "        output = F.relu(output)\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "\n",
    "        output = F.log_softmax(self.out(output[0]), dim=1)\n",
    "        return output, hidden, attn_weights\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e26081387c8990b949948d3d3fd7f7a4bc3bad723d6b8652b3d4ab339298fb31"
  },
  "kernelspec": {
   "display_name": "Python 3.8.0 64-bit",
   "language": "python",
   "name": "python38064bit8c5ff504779d4c0db4df09f0210e7c63"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0-final"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}