{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras \n",
    "from tensorflow.keras.layers import Input, LSTM, Attention, Embedding, Dense, Concatenate, TimeDistributed   #Layers required to implement the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np   #Package for scientific computing and dealing with arrays\n",
    "import pandas as pd  #Package providing fast, flexible and expressive data structures\n",
    "import re            #re stands for RegularExpression providing full support for Perl-like Regular Expressions in Python\n",
    "from bs4 import BeautifulSoup   #Package for pulling data out of HTML and XML files\n",
    "from keras.preprocessing.sequence import pad_sequences  #For Padding the seqences to same length\n",
    "from nltk.corpus import stopwords   #For removing filler words\n",
    "from tensorflow.keras.layers import Input, LSTM, Attention, Embedding, Dense, Concatenate, TimeDistributed   #Layers required to implement the model\n",
    "from tensorflow.keras.models import Model  #Helps in grouping the layers into an object with training and inference features\n",
    "from tensorflow.keras.callbacks import EarlyStopping  #Allows training the model on large no. of training epochs & stop once the performance stops improving on validation dataset\n",
    "from os import listdir\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded Stories 92578\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# load doc into memory\n",
    "def load_doc(filename):\n",
    "\t# open the file as read only\n",
    "\tfile = open(filename, encoding='utf-8')\n",
    "\t# read all text\n",
    "\ttext = file.read()\n",
    "\t# close the file\n",
    "\tfile.close()\n",
    "\treturn text\n",
    "\n",
    "# split a document into news story and highlights\n",
    "def split_story(doc):\n",
    "\t# find first highlight\n",
    "\tindex = doc.find('@highlight')\n",
    "\t# split into story and highlights\n",
    "\tstory, highlights = doc[:index], doc[index:].split('@highlight')\n",
    "\t# strip extra white space around each highlight\n",
    "\thighlights = [h.strip() for h in highlights if len(h) > 0]\n",
    "\treturn story, highlights\n",
    "\n",
    "# load all stories in a directory\n",
    "def load_stories(directory):\n",
    "\tstories = list()\n",
    "\tfor name in listdir(directory):\n",
    "\t\tfilename = directory + '/' + name\n",
    "\t\t# load document\n",
    "\t\tdoc = load_doc(filename)\n",
    "\t\t# split into story and highlights\n",
    "\t\tstory, highlights = split_story(doc)\n",
    "\t\t# store\n",
    "\t\tstories.append({'story':story, 'highlights':highlights})\n",
    "\treturn stories\n",
    "\n",
    "# clean a list of lines\n",
    "def clean_lines(lines):\n",
    "\tcleaned = list()\n",
    "\t# prepare a translation table to remove punctuation\n",
    "\ttable = str.maketrans('', '', string.punctuation)\n",
    "\tfor line in lines:\n",
    "\t\t# strip source cnn office if it exists\n",
    "\t\tindex = line.find('(CNN) -- ')\n",
    "\t\tif index > -1:\n",
    "\t\t\tline = line[index+len('(CNN)'):]\n",
    "\t\t# tokenize on white space\n",
    "\t\tline = line.split()\n",
    "\t\t# convert to lower case\n",
    "\t\tline = [word.lower() for word in line]\n",
    "\t\t# remove punctuation from each token\n",
    "\t\tline = [w.translate(table) for w in line]\n",
    "\t\t# remove tokens with numbers in them\n",
    "\t\tline = [word for word in line if word.isalpha()]\n",
    "\t\t# store as string\n",
    "\t\tcleaned.append(' '.join(line))\n",
    "\t# remove empty strings\n",
    "\tcleaned = [c for c in cleaned if len(c) > 0]\n",
    "\treturn cleaned\n",
    "\n",
    "# load stories\n",
    "directory = 'cnn/'\n",
    "stories = load_stories(directory)\n",
    "print('Loaded Stories %d' % len(stories))\n",
    "\n",
    "# clean stories\n",
    "for example in stories:\n",
    "\texample['story'] = clean_lines(example['story'].split('\\n'))\n",
    "\texample['highlights'] = clean_lines(example['highlights'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save to file\n",
    "from pickle import dump\n",
    "dump(stories, open('cnn_dataset.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded Stories 92578\n"
     ]
    }
   ],
   "source": [
    "from pickle import load\n",
    "\n",
    "# load from file\n",
    "stories = load(open('cnn_dataset.pkl', 'rb'))\n",
    "print('Loaded Stories %d' % len(stories))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer \n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import RepeatVector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['it s official us president barack obama wants lawmakers to weigh in on whether to use military force in syria',\n",
       " 'obama sent a letter to the heads of the house and senate on saturday night hours after announcing that he believes military action against syrian targets is the right step to take over the alleged use of chemical weapons',\n",
       " 'the proposed legislation from obama asks congress to approve the use of military force to deter disrupt prevent and degrade the potential for future uses of chemical weapons or other weapons of mass destruction',\n",
       " 'it s a step that is set to turn an international crisis into a fierce domestic political battle',\n",
       " 'there are key questions looming over the debate what did un weapons inspectors find in syria what happens if congress votes no and how will the syrian government react',\n",
       " 'in a televised address from the white house rose garden earlier saturday the president said he would take his case to congress not because he has to but because he wants to',\n",
       " 'while i believe i have the authority to carry out this military action without specific congressional authorization i know that the country will be stronger if we take this course and our actions will be even more effective he said we should have this debate because the issues are too big for business as usual',\n",
       " 'obama said top congressional leaders had agreed to schedule a debate when the body returns to washington on september the senate foreign relations committee will hold a hearing over the matter on tuesday sen robert menendez said',\n",
       " 'transcript read obama s full remarks',\n",
       " 'syrian crisis latest developments',\n",
       " 'un inspectors leave syria',\n",
       " 'obama s remarks came shortly after un inspectors left syria carrying evidence that will determine whether chemical weapons were used in an attack early last week in a damascus suburb',\n",
       " 'the aim of the game here the mandate is very clear and that is to ascertain whether chemical weapons were used and not by whom un spokesman martin nesirky told reporters on saturday',\n",
       " 'but who used the weapons in the reported toxic gas attack in a damascus suburb on august has been a key point of global debate over the syrian crisis',\n",
       " 'top us officials have said there s no doubt that the syrian government was behind it while syrian officials have denied responsibility and blamed jihadists fighting with the rebels',\n",
       " 'british and us intelligence reports say the attack involved chemical weapons but un officials have stressed the importance of waiting for an official report from inspectors',\n",
       " 'the inspectors will share their findings with un secretarygeneral ban kimoon ban who has said he wants to wait until the un team s final report is completed before presenting it to the un security council',\n",
       " 'the organization for the prohibition of chemical weapons which nine of the inspectors belong to said saturday that it could take up to three weeks to analyze the evidence they collected',\n",
       " 'it needs time to be able to analyze the information and the samples nesirky said',\n",
       " 'he noted that ban has repeatedly said there is no alternative to a political solution to the crisis in syria and that a military solution is not an option',\n",
       " 'bergen syria is a problem from hell for the us',\n",
       " 'obama this menace must be confronted',\n",
       " 'obama s senior advisers have debated the next steps to take and the president s comments saturday came amid mounting political pressure over the situation in syria some us lawmakers have called for immediate action while others warn of stepping into what could become a quagmire',\n",
       " 'some global leaders have expressed support but the british parliament s vote against military action earlier this week was a blow to obama s hopes of getting strong backing from key nato allies',\n",
       " 'on saturday obama proposed what he said would be a limited military action against syrian president bashar alassad any military attack would not be openended or include us ground forces he said',\n",
       " 'syria s alleged use of chemical weapons earlier this month is an assault on human dignity the president said',\n",
       " 'a failure to respond with force obama argued could lead to escalating use of chemical weapons or their proliferation to terrorist groups who would do our people harm in a world with many dangers this menace must be confronted',\n",
       " 'syria missile strike what would happen next',\n",
       " 'map us and allied assets around syria',\n",
       " 'obama decision came friday night',\n",
       " 'on friday night the president made a lastminute decision to consult lawmakers',\n",
       " 'what will happen if they vote no',\n",
       " 'it s unclear a senior administration official told cnn that obama has the authority to act without congress even if congress rejects his request for authorization to use force',\n",
       " 'obama on saturday continued to shore up support for a strike on the alassad government',\n",
       " 'he spoke by phone with french president francois hollande before his rose garden speech',\n",
       " 'the two leaders agreed that the international community must deliver a resolute message to the assad regime and others who would consider using chemical weapons that these crimes are unacceptable and those who violate this international norm will be held accountable by the world the white house said',\n",
       " 'meanwhile as uncertainty loomed over how congress would weigh in us military officials said they remained at the ready',\n",
       " 'key assertions us intelligence report on syria',\n",
       " 'syria who wants what after chemical weapons horror',\n",
       " 'reactions mixed to obama s speech',\n",
       " 'a spokesman for the syrian national coalition said that the opposition group was disappointed by obama s announcement',\n",
       " 'our fear now is that the lack of action could embolden the regime and they repeat his attacks in a more serious way said spokesman louay safi so we are quite concerned',\n",
       " 'some members of congress applauded obama s decision',\n",
       " 'house speaker john boehner majority leader eric cantor majority whip kevin mccarthy and conference chair cathy mcmorris rodgers issued a statement saturday praising the president',\n",
       " 'under the constitution the responsibility to declare war lies with congress the republican lawmakers said we are glad the president is seeking authorization for any military action in syria in response to serious substantive questions being raised',\n",
       " 'more than legislators including of obama s fellow democrats had signed letters calling for either a vote or at least a full debate before any us action',\n",
       " 'british prime minister david cameron whose own attempt to get lawmakers in his country to support military action in syria failed earlier this week responded to obama s speech in a twitter post saturday',\n",
       " 'i understand and support barack obama s position on syria cameron said',\n",
       " 'an influential lawmaker in russia which has stood by syria and criticized the united states had his own theory',\n",
       " 'the main reason obama is turning to the congress the military operation did not get enough support either in the world among allies of the us or in the united states itself alexei pushkov chairman of the internationalaffairs committee of the russian state duma said in a twitter post',\n",
       " 'in the united states scattered groups of antiwar protesters around the country took to the streets saturday',\n",
       " 'like many other americans we re just tired of the united states getting involved and invading and bombing other countries said robin rosecrans who was among hundreds at a los angeles demonstration',\n",
       " 'what do syria s neighbors think',\n",
       " 'why russia china iran stand by assad',\n",
       " 'syria s government unfazed',\n",
       " 'after obama s speech a military and political analyst on syrian state tv said obama is embarrassed that russia opposes military action against syria is crying for help for someone to come to his rescue and is facing two defeats on the political and military levels',\n",
       " 'syria s prime minister appeared unfazed by the saberrattling',\n",
       " 'the syrian army s status is on maximum readiness and fingers are on the trigger to confront all challenges wael nader alhalqi said during a meeting with a delegation of syrian expatriates from italy according to a banner on syria state tv that was broadcast prior to obama s address',\n",
       " 'an anchor on syrian state television said obama appeared to be preparing for an aggression on syria based on repeated lies',\n",
       " 'a top syrian diplomat told the state television network that obama was facing pressure to take military action from israel turkey some arabs and rightwing extremists in the united states',\n",
       " 'i think he has done well by doing what cameron did in terms of taking the issue to parliament said bashar jaafari syria s ambassador to the united nations',\n",
       " 'both obama and cameron he said climbed to the top of the tree and do nt know how to get down',\n",
       " 'the syrian government has denied that it used chemical weapons in the august attack saying that jihadists fighting with the rebels used them in an effort to turn global sentiments against it',\n",
       " 'british intelligence had put the number of people killed in the attack at more than',\n",
       " 'on saturday obama said all told well over people were murdered us secretary of state john kerry on friday cited a death toll of more than of them children no explanation was offered for the discrepancy',\n",
       " 'iran us military action in syria would spark disaster',\n",
       " 'opinion why strikes in syria are a bad idea']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(stories[0]['story'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preprocessing\n",
    "\n",
    "#This the dictionary used for expanding contractions\n",
    "contraction_mapping = {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"'cause\": \"because\", \"could've\": \"could have\", \"couldn't\": \"could not\",\n",
    "\n",
    "                           \"didn't\": \"did not\", \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \"hasn't\": \"has not\", \"haven't\": \"have not\",\n",
    "\n",
    "                           \"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\",\n",
    "\n",
    "                           \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \"i'd\": \"i would\",\n",
    "\n",
    "                           \"i'd've\": \"i would have\", \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\",\n",
    "\n",
    "                           \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\",\n",
    "\n",
    "                           \"mayn't\": \"may not\", \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\", \"must've\": \"must have\",\n",
    "\n",
    "                           \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\",\n",
    "\n",
    "                           \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\",\n",
    "\n",
    "                           \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\", \"she's\": \"she is\",\n",
    "\n",
    "                           \"should've\": \"should have\", \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\"so's\": \"so as\",\n",
    "\n",
    "                           \"this's\": \"this is\",\"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\", \"there'd\": \"there would\",\n",
    "\n",
    "                           \"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\",\"they'd\": \"they would\", \"they'd've\": \"they would have\",\n",
    "\n",
    "                           \"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\",\n",
    "\n",
    "                           \"wasn't\": \"was not\", \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\",\n",
    "\n",
    "                           \"we've\": \"we have\", \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\",\n",
    "\n",
    "                           \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\",\n",
    "\n",
    "                           \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\",\n",
    "\n",
    "                           \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \"won't've\": \"will not have\",\n",
    "\n",
    "                           \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \"y'all\": \"you all\",\n",
    "\n",
    "                           \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\n",
    "\n",
    "                           \"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\",\n",
    "\n",
    "                           \"you're\": \"you are\", \"you've\": \"you have\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#Text Cleaning\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "\n",
    "stop_words = set(stopwords.words('english')) \n",
    "def text_cleaner(text,num):\n",
    "    newString = text.lower()  #converts all uppercase characters in the string into lowercase characters and returns it\n",
    "    newString = BeautifulSoup(newString, \"lxml\").text #parses the string into an lxml.html \n",
    "    newString = re.sub(r'\\([^)]*\\)', '', newString) #used to replace a string that matches a regular expression instead of perfect match\n",
    "    newString = re.sub('\"','', newString)           \n",
    "    newString = ' '.join([contraction_mapping[t] if t in contraction_mapping else t for t in newString.split(\" \")]) #for expanding contractions using the contraction_mapping dictionary    \n",
    "    newString = re.sub(r\"'s\\b\",\"\",newString)\n",
    "    newString = re.sub(\"[^a-zA-Z]\", \" \", newString)\n",
    "    if(num==0): \n",
    "      tokens = [w for w in newString.split() if not w in stop_words]  #converting the strings into tokens\n",
    "    else :\n",
    "      tokens = newString.split()\n",
    "    long_words=[]\n",
    "    for i in tokens:\n",
    "        if len(i)>1:                  #removing short words\n",
    "            long_words.append(i)   \n",
    "    return (\" \".join(long_words)).strip()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'idan': 1}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ss  = edict()\n",
    "ss[\"idan\"] = 1\n",
    "ss[\"idan\"]+1\n",
    "ss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from easydict import EasyDict as edict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ackage stopwords is already up-to-date\n"
     ]
    }
   ],
   "source": [
    "sen = \"ackage stopwords is already up-to-date!\"\n",
    "print(sen.strip(\"!\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calling the function\n",
    "x_vocab = edict()\n",
    "y_vocab = edict()\n",
    "\n",
    "article_word_count = []\n",
    "abstract_word_count = []\n",
    "\n",
    "cleaned_text = []\n",
    "highlights = []\n",
    "\n",
    "max_ar_length = 0\n",
    "max_high_lengh = 0\n",
    "\n",
    "for file in stories:\n",
    "    t = file['story']\n",
    "    story =\"\"\n",
    "    highligh= \"\"\n",
    "    h = file['highlights']\n",
    "    for line in t:\n",
    "        story= story+line\n",
    "        for word in line: \n",
    "            if word not in x_vocab.keys():\n",
    "                x_vocab[word] =1\n",
    "            else: \n",
    "                x_vocab[word]+=1\n",
    "\n",
    "    if len(story.split())> max_ar_length: max_ar_length =len(story.split())\n",
    "    article_word_count.append(len(story.split()))\n",
    "    cleaned_text.append(text_cleaner(story,0))\n",
    "    for line in h:\n",
    "        highligh= highligh+line\n",
    "        if word not in y_vocab.keys():\n",
    "            y_vocab[word] =1\n",
    "        else: \n",
    "            y_vocab[word]+=1\n",
    "\n",
    "    if len(highligh.split())> max_high_lengh: max_high_lengh =len(highligh.split())\n",
    "    abstract_word_count.append(len(highligh.split()))\n",
    "    highlights.append(highligh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Max length sequence story: 2050\n",
      " Max length sequence story: 103\n"
     ]
    }
   ],
   "source": [
    "# print summer: \n",
    "print(f\" Max length sequence story: {max_ar_length}\")\n",
    "print(f\" Max length sequence story: {max_high_lengh}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "int"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(len(set(cleaned_text[1].split())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tokenizer = Tokenizer(len(set(cleaned_text[0].split()))+1)\n",
    "X_tokenizer.fit_on_texts(cleaned_text[0].split())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_tokenizer = Tokenizer(90) \n",
    "Y_tokenizer.fit_on_texts(highlights[0])\n",
    "Y_voc = Y_tokenizer.num_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_to_idx(data: list) -> dict:\n",
    "    \"\"\"\n",
    "    Function that maps the data and return a dictionary of words corresponding to their index\n",
    "    it gets a list\n",
    "    return:\n",
    "        dict 1 idx to word\n",
    "        dict 2 word to idx\n",
    "    \"\"\"\n",
    "    total_letters = [letters for sublist in data for subsublist in sublist for letters in subsublist]\n",
    "    unique_letters =set(total_letters)\n",
    "    total_words = [word.replace(',','') for sublist in data for subsublist in sublist for word in subsublist.split()]\n",
    "    unique_words =list(set(total_words))\n",
    "\n",
    "    w_2_i = {unique_words[i]:i for i in range(len(unique_words))}\n",
    "    i_2_w= {i: unique_words[i] for i in range(len(unique_words))}\n",
    "    print(w_2_i)\n",
    "    input()\n",
    "    return (w_2_i, i_2_w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train= X_tokenizer.texts_to_sequences(cleaned_text[0].split()) \n",
    "y_train  = Y_tokenizer.texts_to_sequences(highlights[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "249\n"
     ]
    }
   ],
   "source": [
    "print(X_tokenizer.num_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_voc = X_tokenizer.num_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(highlights[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.array([x for y in y_train for x in y])\n",
    "x =np.array([x for y in X_train for x in y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = x.reshape(1,-1)\n",
    "y  =y.reshape(1,-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "90"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_voc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "249"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_voc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 875)]        0           []                               \n",
      "                                                                                                  \n",
      " input_2 (InputLayer)           [(None, 228)]        0           []                               \n",
      "                                                                                                  \n",
      " embedding (Embedding)          (None, 875, 100)     100000      ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " embedding_1 (Embedding)        (None, 228, 100)     9100        ['input_2[0][0]']                \n",
      "                                                                                                  \n",
      " lstm (LSTM)                    [(None, 875, 100),   80400       ['embedding[0][0]']              \n",
      "                                 (None, 100),                                                     \n",
      "                                 (None, 100)]                                                     \n",
      "                                                                                                  \n",
      " lstm_1 (LSTM)                  [(None, 228, 100),   80400       ['embedding_1[0][0]',            \n",
      "                                 (None, 100),                     'lstm[0][1]',                   \n",
      "                                 (None, 100)]                     'lstm[0][2]']                   \n",
      "                                                                                                  \n",
      " time_distributed (TimeDistribu  (None, 228, 90)     9090        ['lstm_1[0][0]']                 \n",
      " ted)                                                                                             \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 278,990\n",
      "Trainable params: 278,990\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "('You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) ', 'for plot_model/model_to_dot to work.')\n",
      "Epoch 1/200\n",
      "1/1 [==============================] - ETA: 0s - loss: 4.5002 - accuracy: 0.0044WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
      "1/1 [==============================] - 5s 5s/step - loss: 4.5002 - accuracy: 0.0044\n",
      "Epoch 2/200\n",
      "1/1 [==============================] - ETA: 0s - loss: 4.4719 - accuracy: 0.2149WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
      "1/1 [==============================] - 0s 296ms/step - loss: 4.4719 - accuracy: 0.2149\n",
      "Epoch 3/200\n",
      "1/1 [==============================] - ETA: 0s - loss: 4.4405 - accuracy: 0.2632WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
      "1/1 [==============================] - 0s 299ms/step - loss: 4.4405 - accuracy: 0.2632\n",
      "Epoch 4/200\n",
      "1/1 [==============================] - ETA: 0s - loss: 4.3613 - accuracy: 0.1491WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
      "1/1 [==============================] - 0s 301ms/step - loss: 4.3613 - accuracy: 0.1491\n",
      "Epoch 5/200\n",
      "1/1 [==============================] - ETA: 0s - loss: 3.6961 - accuracy: 0.1009WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
      "1/1 [==============================] - 0s 299ms/step - loss: 3.6961 - accuracy: 0.1009\n",
      "Epoch 6/200\n",
      "1/1 [==============================] - ETA: 0s - loss: 3.2756 - accuracy: 0.1316WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
      "1/1 [==============================] - 0s 290ms/step - loss: 3.2756 - accuracy: 0.1316\n",
      "Epoch 7/200\n",
      "1/1 [==============================] - ETA: 0s - loss: 3.1137 - accuracy: 0.1360WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
      "1/1 [==============================] - 0s 291ms/step - loss: 3.1137 - accuracy: 0.1360\n",
      "Epoch 8/200\n",
      "1/1 [==============================] - ETA: 0s - loss: 3.0250 - accuracy: 0.1623WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
      "1/1 [==============================] - 0s 302ms/step - loss: 3.0250 - accuracy: 0.1623\n",
      "Epoch 9/200\n",
      "1/1 [==============================] - ETA: 0s - loss: 2.9723 - accuracy: 0.1535WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
      "1/1 [==============================] - 0s 311ms/step - loss: 2.9723 - accuracy: 0.1535\n",
      "Epoch 10/200\n",
      "1/1 [==============================] - ETA: 0s - loss: 2.9219 - accuracy: 0.1711WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
      "1/1 [==============================] - 0s 302ms/step - loss: 2.9219 - accuracy: 0.1711\n",
      "Epoch 11/200\n",
      "1/1 [==============================] - ETA: 0s - loss: 2.8940 - accuracy: 0.1974WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
      "1/1 [==============================] - 0s 293ms/step - loss: 2.8940 - accuracy: 0.1974\n",
      "Epoch 12/200\n",
      "1/1 [==============================] - ETA: 0s - loss: 2.8705 - accuracy: 0.1623WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
      "1/1 [==============================] - 0s 290ms/step - loss: 2.8705 - accuracy: 0.1623\n",
      "Epoch 13/200\n",
      "1/1 [==============================] - ETA: 0s - loss: 2.8480 - accuracy: 0.1667WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
      "1/1 [==============================] - 0s 295ms/step - loss: 2.8480 - accuracy: 0.1667\n",
      "Epoch 14/200\n",
      "1/1 [==============================] - ETA: 0s - loss: 2.8391 - accuracy: 0.1886WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
      "1/1 [==============================] - 0s 292ms/step - loss: 2.8391 - accuracy: 0.1886\n",
      "Epoch 15/200\n",
      "1/1 [==============================] - ETA: 0s - loss: 2.8191 - accuracy: 0.2193WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
      "1/1 [==============================] - 0s 306ms/step - loss: 2.8191 - accuracy: 0.2193\n",
      "Epoch 16/200\n",
      "1/1 [==============================] - ETA: 0s - loss: 2.8089 - accuracy: 0.1491WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
      "1/1 [==============================] - 0s 301ms/step - loss: 2.8089 - accuracy: 0.1491\n",
      "Epoch 17/200\n",
      "1/1 [==============================] - ETA: 0s - loss: 2.7970 - accuracy: 0.1886WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
      "1/1 [==============================] - 0s 296ms/step - loss: 2.7970 - accuracy: 0.1886\n",
      "Epoch 18/200\n",
      "1/1 [==============================] - ETA: 0s - loss: 2.7869 - accuracy: 0.1535WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
      "1/1 [==============================] - 0s 304ms/step - loss: 2.7869 - accuracy: 0.1535\n",
      "Epoch 19/200\n",
      "1/1 [==============================] - ETA: 0s - loss: 2.7728 - accuracy: 0.1798WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
      "1/1 [==============================] - 0s 304ms/step - loss: 2.7728 - accuracy: 0.1798\n",
      "Epoch 20/200\n",
      "1/1 [==============================] - ETA: 0s - loss: 2.7682 - accuracy: 0.1667WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
      "1/1 [==============================] - 0s 291ms/step - loss: 2.7682 - accuracy: 0.1667\n",
      "Epoch 21/200\n",
      "1/1 [==============================] - ETA: 0s - loss: 2.7470 - accuracy: 0.2149WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
      "1/1 [==============================] - 0s 289ms/step - loss: 2.7470 - accuracy: 0.2149\n",
      "Epoch 22/200\n",
      "1/1 [==============================] - ETA: 0s - loss: 2.7503 - accuracy: 0.1930WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
      "1/1 [==============================] - 0s 299ms/step - loss: 2.7503 - accuracy: 0.1930\n",
      "Epoch 23/200\n",
      "1/1 [==============================] - ETA: 0s - loss: 2.7454 - accuracy: 0.1930WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
      "1/1 [==============================] - 0s 291ms/step - loss: 2.7454 - accuracy: 0.1930\n",
      "Epoch 24/200\n",
      "1/1 [==============================] - ETA: 0s - loss: 2.7385 - accuracy: 0.2544WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
      "1/1 [==============================] - 0s 296ms/step - loss: 2.7385 - accuracy: 0.2544\n",
      "Epoch 25/200\n",
      "1/1 [==============================] - ETA: 0s - loss: 2.7376 - accuracy: 0.1974WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
      "1/1 [==============================] - 0s 293ms/step - loss: 2.7376 - accuracy: 0.1974\n",
      "Epoch 26/200\n",
      "1/1 [==============================] - ETA: 0s - loss: 2.7125 - accuracy: 0.2105WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
      "1/1 [==============================] - 0s 290ms/step - loss: 2.7125 - accuracy: 0.2105\n",
      "Epoch 27/200\n",
      "1/1 [==============================] - ETA: 0s - loss: 2.6978 - accuracy: 0.1842WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
      "1/1 [==============================] - 0s 337ms/step - loss: 2.6978 - accuracy: 0.1842\n",
      "Epoch 28/200\n",
      "1/1 [==============================] - ETA: 0s - loss: 2.6903 - accuracy: 0.1930WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
      "1/1 [==============================] - 0s 342ms/step - loss: 2.6903 - accuracy: 0.1930\n",
      "Epoch 29/200\n",
      "1/1 [==============================] - ETA: 0s - loss: 2.6799 - accuracy: 0.2456WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
      "1/1 [==============================] - 0s 290ms/step - loss: 2.6799 - accuracy: 0.2456\n",
      "Epoch 30/200\n",
      "1/1 [==============================] - ETA: 0s - loss: 2.6686 - accuracy: 0.1974WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
      "1/1 [==============================] - 0s 293ms/step - loss: 2.6686 - accuracy: 0.1974\n",
      "Epoch 31/200\n",
      "1/1 [==============================] - ETA: 0s - loss: 2.6584 - accuracy: 0.2500WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
      "1/1 [==============================] - 0s 301ms/step - loss: 2.6584 - accuracy: 0.2500\n",
      "Epoch 32/200\n",
      "1/1 [==============================] - ETA: 0s - loss: 2.6843 - accuracy: 0.1579WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
      "1/1 [==============================] - 0s 301ms/step - loss: 2.6843 - accuracy: 0.1579\n",
      "Epoch 33/200\n",
      "1/1 [==============================] - ETA: 0s - loss: 2.6455 - accuracy: 0.3816WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
      "1/1 [==============================] - 0s 297ms/step - loss: 2.6455 - accuracy: 0.3816\n",
      "Epoch 34/200\n",
      "1/1 [==============================] - ETA: 0s - loss: 2.6243 - accuracy: 0.1974WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
      "1/1 [==============================] - 0s 316ms/step - loss: 2.6243 - accuracy: 0.1974\n",
      "Epoch 35/200\n",
      "1/1 [==============================] - ETA: 0s - loss: 2.6065 - accuracy: 0.3114WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
      "1/1 [==============================] - 0s 352ms/step - loss: 2.6065 - accuracy: 0.3114\n",
      "Epoch 36/200\n",
      "1/1 [==============================] - ETA: 0s - loss: 2.6063 - accuracy: 0.2675WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
      "1/1 [==============================] - 0s 342ms/step - loss: 2.6063 - accuracy: 0.2675\n",
      "Epoch 37/200\n",
      "1/1 [==============================] - ETA: 0s - loss: 2.5856 - accuracy: 0.3553WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
      "1/1 [==============================] - 0s 309ms/step - loss: 2.5856 - accuracy: 0.3553\n",
      "Epoch 38/200\n",
      "1/1 [==============================] - ETA: 0s - loss: 2.5963 - accuracy: 0.1930WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
      "1/1 [==============================] - 0s 314ms/step - loss: 2.5963 - accuracy: 0.1930\n",
      "Epoch 39/200\n",
      "1/1 [==============================] - ETA: 0s - loss: 2.5558 - accuracy: 0.3553WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
      "1/1 [==============================] - 0s 296ms/step - loss: 2.5558 - accuracy: 0.3553\n",
      "Epoch 40/200\n",
      "1/1 [==============================] - ETA: 0s - loss: 2.5567 - accuracy: 0.2982WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
      "1/1 [==============================] - 0s 310ms/step - loss: 2.5567 - accuracy: 0.2982\n",
      "Epoch 41/200\n",
      "1/1 [==============================] - ETA: 0s - loss: 2.5241 - accuracy: 0.3991WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
      "1/1 [==============================] - 0s 305ms/step - loss: 2.5241 - accuracy: 0.3991\n",
      "Epoch 42/200\n",
      "1/1 [==============================] - ETA: 0s - loss: 2.5143 - accuracy: 0.3158WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
      "1/1 [==============================] - 0s 313ms/step - loss: 2.5143 - accuracy: 0.3158\n",
      "Epoch 43/200\n",
      "1/1 [==============================] - ETA: 0s - loss: 2.5088 - accuracy: 0.3640WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
      "1/1 [==============================] - 0s 309ms/step - loss: 2.5088 - accuracy: 0.3640\n",
      "Epoch 44/200\n",
      "1/1 [==============================] - ETA: 0s - loss: 2.5062 - accuracy: 0.2281WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
      "1/1 [==============================] - 0s 303ms/step - loss: 2.5062 - accuracy: 0.2281\n",
      "Epoch 45/200\n",
      "1/1 [==============================] - ETA: 0s - loss: 2.4622 - accuracy: 0.3991WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
      "1/1 [==============================] - 0s 305ms/step - loss: 2.4622 - accuracy: 0.3991\n",
      "Epoch 46/200\n",
      "1/1 [==============================] - ETA: 0s - loss: 2.4487 - accuracy: 0.3816WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
      "1/1 [==============================] - 0s 298ms/step - loss: 2.4487 - accuracy: 0.3816\n",
      "Epoch 47/200\n",
      "1/1 [==============================] - ETA: 0s - loss: 2.4325 - accuracy: 0.4386WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
      "1/1 [==============================] - 0s 311ms/step - loss: 2.4325 - accuracy: 0.4386\n",
      "Epoch 48/200\n",
      "1/1 [==============================] - ETA: 0s - loss: 2.4127 - accuracy: 0.3728WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
      "1/1 [==============================] - 0s 327ms/step - loss: 2.4127 - accuracy: 0.3728\n",
      "Epoch 49/200\n",
      "1/1 [==============================] - ETA: 0s - loss: 2.4061 - accuracy: 0.4474WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
      "1/1 [==============================] - 0s 310ms/step - loss: 2.4061 - accuracy: 0.4474\n",
      "Epoch 50/200\n",
      "1/1 [==============================] - ETA: 0s - loss: 2.3786 - accuracy: 0.4167WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
      "1/1 [==============================] - 0s 320ms/step - loss: 2.3786 - accuracy: 0.4167\n",
      "Epoch 51/200\n",
      "1/1 [==============================] - ETA: 0s - loss: 2.3794 - accuracy: 0.4123WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
      "1/1 [==============================] - 0s 309ms/step - loss: 2.3794 - accuracy: 0.4123\n",
      "Epoch 52/200\n",
      "1/1 [==============================] - ETA: 0s - loss: 2.3600 - accuracy: 0.4430WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
      "1/1 [==============================] - 0s 312ms/step - loss: 2.3600 - accuracy: 0.4430\n",
      "Epoch 53/200\n",
      "1/1 [==============================] - ETA: 0s - loss: 2.3583 - accuracy: 0.4035WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
      "1/1 [==============================] - 0s 296ms/step - loss: 2.3583 - accuracy: 0.4035\n",
      "Epoch 54/200\n",
      "1/1 [==============================] - ETA: 0s - loss: 2.3148 - accuracy: 0.5000WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
      "1/1 [==============================] - 0s 310ms/step - loss: 2.3148 - accuracy: 0.5000\n",
      "Epoch 55/200\n",
      "1/1 [==============================] - ETA: 0s - loss: 2.2927 - accuracy: 0.4912WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
      "1/1 [==============================] - 0s 320ms/step - loss: 2.2927 - accuracy: 0.4912\n",
      "Epoch 56/200\n",
      "1/1 [==============================] - ETA: 0s - loss: 2.2625 - accuracy: 0.5219WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
      "1/1 [==============================] - 0s 323ms/step - loss: 2.2625 - accuracy: 0.5219\n",
      "Epoch 57/200\n",
      "1/1 [==============================] - ETA: 0s - loss: 2.2414 - accuracy: 0.5000WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
      "1/1 [==============================] - 0s 325ms/step - loss: 2.2414 - accuracy: 0.5000\n",
      "Epoch 58/200\n",
      "1/1 [==============================] - ETA: 0s - loss: 2.2181 - accuracy: 0.5219WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
      "1/1 [==============================] - 0s 309ms/step - loss: 2.2181 - accuracy: 0.5219\n",
      "Epoch 59/200\n",
      "1/1 [==============================] - ETA: 0s - loss: 2.2045 - accuracy: 0.5000WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
      "1/1 [==============================] - 0s 327ms/step - loss: 2.2045 - accuracy: 0.5000\n",
      "Epoch 60/200\n",
      "1/1 [==============================] - ETA: 0s - loss: 2.1919 - accuracy: 0.5175WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
      "1/1 [==============================] - 0s 310ms/step - loss: 2.1919 - accuracy: 0.5175\n",
      "Epoch 61/200\n",
      "1/1 [==============================] - ETA: 0s - loss: 2.1739 - accuracy: 0.5132WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
      "1/1 [==============================] - 0s 327ms/step - loss: 2.1739 - accuracy: 0.5132\n",
      "Epoch 62/200\n",
      "1/1 [==============================] - ETA: 0s - loss: 2.1635 - accuracy: 0.5132WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
      "1/1 [==============================] - 0s 332ms/step - loss: 2.1635 - accuracy: 0.5132\n",
      "Epoch 63/200\n",
      "1/1 [==============================] - ETA: 0s - loss: 2.1329 - accuracy: 0.5439WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
      "1/1 [==============================] - 0s 326ms/step - loss: 2.1329 - accuracy: 0.5439\n",
      "Epoch 64/200\n",
      "1/1 [==============================] - ETA: 0s - loss: 2.1311 - accuracy: 0.5307WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
      "1/1 [==============================] - 0s 317ms/step - loss: 2.1311 - accuracy: 0.5307\n",
      "Epoch 65/200\n",
      "1/1 [==============================] - ETA: 0s - loss: 2.1018 - accuracy: 0.5395WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
      "1/1 [==============================] - 0s 305ms/step - loss: 2.1018 - accuracy: 0.5395\n",
      "Epoch 66/200\n",
      "1/1 [==============================] - ETA: 0s - loss: 2.0854 - accuracy: 0.5526WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
      "1/1 [==============================] - 0s 377ms/step - loss: 2.0854 - accuracy: 0.5526\n",
      "Epoch 67/200\n",
      "1/1 [==============================] - ETA: 0s - loss: 2.0584 - accuracy: 0.5263WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
      "1/1 [==============================] - 0s 325ms/step - loss: 2.0584 - accuracy: 0.5263\n",
      "Epoch 68/200\n",
      "1/1 [==============================] - ETA: 0s - loss: 2.0659 - accuracy: 0.6009WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
      "1/1 [==============================] - 0s 321ms/step - loss: 2.0659 - accuracy: 0.6009\n",
      "Epoch 69/200\n",
      "1/1 [==============================] - ETA: 0s - loss: 2.0289 - accuracy: 0.5614WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
      "1/1 [==============================] - 0s 306ms/step - loss: 2.0289 - accuracy: 0.5614\n",
      "Epoch 70/200\n",
      "1/1 [==============================] - ETA: 0s - loss: 2.0026 - accuracy: 0.5877WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
      "1/1 [==============================] - 0s 298ms/step - loss: 2.0026 - accuracy: 0.5877\n",
      "Epoch 71/200\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.9774 - accuracy: 0.5877WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
      "1/1 [==============================] - 0s 305ms/step - loss: 1.9774 - accuracy: 0.5877\n",
      "Epoch 72/200\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.9733 - accuracy: 0.5833WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
      "1/1 [==============================] - 0s 302ms/step - loss: 1.9733 - accuracy: 0.5833\n",
      "Epoch 73/200\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.9527 - accuracy: 0.6096WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
      "1/1 [==============================] - 0s 301ms/step - loss: 1.9527 - accuracy: 0.6096\n",
      "Epoch 74/200\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.9169 - accuracy: 0.5921WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
      "1/1 [==============================] - 0s 297ms/step - loss: 1.9169 - accuracy: 0.5921\n",
      "Epoch 75/200\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.9005 - accuracy: 0.6053WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
      "1/1 [==============================] - 0s 310ms/step - loss: 1.9005 - accuracy: 0.6053\n",
      "Epoch 76/200\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.8820 - accuracy: 0.6228WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
      "1/1 [==============================] - 0s 304ms/step - loss: 1.8820 - accuracy: 0.6228\n",
      "Epoch 77/200\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.8532 - accuracy: 0.6096WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
      "1/1 [==============================] - 0s 315ms/step - loss: 1.8532 - accuracy: 0.6096\n",
      "Epoch 78/200\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.8341 - accuracy: 0.6096WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
      "1/1 [==============================] - 0s 314ms/step - loss: 1.8341 - accuracy: 0.6096\n",
      "Epoch 79/200\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.8186 - accuracy: 0.6228WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
      "1/1 [==============================] - 0s 316ms/step - loss: 1.8186 - accuracy: 0.6228\n",
      "Epoch 80/200\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.7760 - accuracy: 0.6228WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
      "1/1 [==============================] - 0s 311ms/step - loss: 1.7760 - accuracy: 0.6228\n",
      "Epoch 81/200\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.7775 - accuracy: 0.6184WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
      "1/1 [==============================] - 0s 328ms/step - loss: 1.7775 - accuracy: 0.6184\n",
      "Epoch 82/200\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.7463 - accuracy: 0.6316WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
      "1/1 [==============================] - 0s 328ms/step - loss: 1.7463 - accuracy: 0.6316\n",
      "Epoch 83/200\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.7195 - accuracy: 0.6491WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
      "1/1 [==============================] - 0s 329ms/step - loss: 1.7195 - accuracy: 0.6491\n",
      "Epoch 84/200\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.7033 - accuracy: 0.6272WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
      "1/1 [==============================] - 0s 320ms/step - loss: 1.7033 - accuracy: 0.6272\n",
      "Epoch 85/200\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.6750 - accuracy: 0.6404WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
      "1/1 [==============================] - 0s 342ms/step - loss: 1.6750 - accuracy: 0.6404\n",
      "Epoch 86/200\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.6475 - accuracy: 0.6579WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
      "1/1 [==============================] - 0s 318ms/step - loss: 1.6475 - accuracy: 0.6579\n",
      "Epoch 87/200\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.6275 - accuracy: 0.6667WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
      "1/1 [==============================] - 0s 327ms/step - loss: 1.6275 - accuracy: 0.6667\n",
      "Epoch 88/200\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.5977 - accuracy: 0.6491WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
      "1/1 [==============================] - 0s 308ms/step - loss: 1.5977 - accuracy: 0.6491\n",
      "Epoch 89/200\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.5902 - accuracy: 0.6667WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
      "1/1 [==============================] - 0s 321ms/step - loss: 1.5902 - accuracy: 0.6667\n",
      "Epoch 90/200\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.5464 - accuracy: 0.6667WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
      "1/1 [==============================] - 0s 307ms/step - loss: 1.5464 - accuracy: 0.6667\n",
      "Epoch 91/200\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.5337 - accuracy: 0.6711WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
      "1/1 [==============================] - 0s 293ms/step - loss: 1.5337 - accuracy: 0.6711\n",
      "Epoch 92/200\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.5195 - accuracy: 0.6886WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
      "1/1 [==============================] - 0s 313ms/step - loss: 1.5195 - accuracy: 0.6886\n",
      "Epoch 93/200\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.4949 - accuracy: 0.6930WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
      "1/1 [==============================] - 0s 305ms/step - loss: 1.4949 - accuracy: 0.6930\n",
      "Epoch 94/200\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.4705 - accuracy: 0.6754WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
      "1/1 [==============================] - 0s 314ms/step - loss: 1.4705 - accuracy: 0.6754\n",
      "Epoch 95/200\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.4535 - accuracy: 0.7281WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
      "1/1 [==============================] - 0s 314ms/step - loss: 1.4535 - accuracy: 0.7281\n",
      "Epoch 96/200\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.4287 - accuracy: 0.6711WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
      "1/1 [==============================] - 0s 302ms/step - loss: 1.4287 - accuracy: 0.6711\n",
      "Epoch 97/200\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.4226 - accuracy: 0.7193WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
      "1/1 [==============================] - 0s 301ms/step - loss: 1.4226 - accuracy: 0.7193\n",
      "Epoch 98/200\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.3892 - accuracy: 0.7018WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
      "1/1 [==============================] - 0s 302ms/step - loss: 1.3892 - accuracy: 0.7018\n",
      "Epoch 99/200\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.3624 - accuracy: 0.7149WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
      "1/1 [==============================] - 0s 309ms/step - loss: 1.3624 - accuracy: 0.7149\n",
      "Epoch 100/200\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.3361 - accuracy: 0.7105WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
      "1/1 [==============================] - 0s 302ms/step - loss: 1.3361 - accuracy: 0.7105\n",
      "Epoch 101/200\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.3071 - accuracy: 0.7237WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
      "1/1 [==============================] - 0s 311ms/step - loss: 1.3071 - accuracy: 0.7237\n",
      "Epoch 102/200\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.2900 - accuracy: 0.7368WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
      "1/1 [==============================] - 0s 304ms/step - loss: 1.2900 - accuracy: 0.7368\n",
      "Epoch 103/200\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.2608 - accuracy: 0.7544WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
      "1/1 [==============================] - 0s 303ms/step - loss: 1.2608 - accuracy: 0.7544\n",
      "Epoch 104/200\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.2626 - accuracy: 0.7456WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
      "1/1 [==============================] - 0s 297ms/step - loss: 1.2626 - accuracy: 0.7456\n",
      "Epoch 105/200\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.2365 - accuracy: 0.7675WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
      "1/1 [==============================] - 0s 305ms/step - loss: 1.2365 - accuracy: 0.7675\n",
      "Epoch 106/200\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.2102 - accuracy: 0.7675WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
      "1/1 [==============================] - 0s 306ms/step - loss: 1.2102 - accuracy: 0.7675\n",
      "Epoch 107/200\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.1921 - accuracy: 0.7939WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
      "1/1 [==============================] - 0s 307ms/step - loss: 1.1921 - accuracy: 0.7939\n",
      "Epoch 108/200\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.1578 - accuracy: 0.7675WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
      "1/1 [==============================] - 0s 296ms/step - loss: 1.1578 - accuracy: 0.7675\n",
      "Epoch 109/200\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.1421 - accuracy: 0.7851WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
      "1/1 [==============================] - 0s 304ms/step - loss: 1.1421 - accuracy: 0.7851\n",
      "Epoch 110/200\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.1164 - accuracy: 0.8114WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
      "1/1 [==============================] - 0s 299ms/step - loss: 1.1164 - accuracy: 0.8114\n",
      "Epoch 111/200\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.0956 - accuracy: 0.7982WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
      "1/1 [==============================] - 0s 308ms/step - loss: 1.0956 - accuracy: 0.7982\n",
      "Epoch 112/200\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.0811 - accuracy: 0.8114WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
      "1/1 [==============================] - 0s 303ms/step - loss: 1.0811 - accuracy: 0.8114\n",
      "Epoch 113/200\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.0614 - accuracy: 0.8026WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
      "1/1 [==============================] - 0s 294ms/step - loss: 1.0614 - accuracy: 0.8026\n",
      "Epoch 114/200\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.0528 - accuracy: 0.8377WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
      "1/1 [==============================] - 0s 314ms/step - loss: 1.0528 - accuracy: 0.8377\n",
      "Epoch 115/200\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.0366 - accuracy: 0.8158WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
      "1/1 [==============================] - 0s 305ms/step - loss: 1.0366 - accuracy: 0.8158\n",
      "Epoch 116/200\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.0146 - accuracy: 0.8421WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
      "1/1 [==============================] - 0s 290ms/step - loss: 1.0146 - accuracy: 0.8421\n",
      "Epoch 117/200\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.9868 - accuracy: 0.8333WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
      "1/1 [==============================] - 0s 300ms/step - loss: 0.9868 - accuracy: 0.8333\n",
      "Epoch 118/200\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.9690 - accuracy: 0.8553WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
      "1/1 [==============================] - 0s 300ms/step - loss: 0.9690 - accuracy: 0.8553\n",
      "Epoch 119/200\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.9477 - accuracy: 0.8465WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
      "1/1 [==============================] - 0s 304ms/step - loss: 0.9477 - accuracy: 0.8465\n",
      "Epoch 120/200\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.9315 - accuracy: 0.8684WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
      "1/1 [==============================] - 0s 300ms/step - loss: 0.9315 - accuracy: 0.8684\n",
      "Epoch 121/200\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.9096 - accuracy: 0.8553WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
      "1/1 [==============================] - 0s 302ms/step - loss: 0.9096 - accuracy: 0.8553\n",
      "Epoch 122/200\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.8893 - accuracy: 0.8684WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
      "1/1 [==============================] - 0s 374ms/step - loss: 0.8893 - accuracy: 0.8684\n",
      "Epoch 123/200\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.8777 - accuracy: 0.8772WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
      "1/1 [==============================] - 0s 319ms/step - loss: 0.8777 - accuracy: 0.8772\n",
      "Epoch 124/200\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.8603 - accuracy: 0.8728WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
      "1/1 [==============================] - 0s 317ms/step - loss: 0.8603 - accuracy: 0.8728\n",
      "Epoch 125/200\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.8338 - accuracy: 0.8860WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
      "1/1 [==============================] - 0s 304ms/step - loss: 0.8338 - accuracy: 0.8860\n",
      "Epoch 126/200\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.8267 - accuracy: 0.8860WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
      "1/1 [==============================] - 0s 303ms/step - loss: 0.8267 - accuracy: 0.8860\n",
      "Epoch 127/200\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.7949 - accuracy: 0.9035WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
      "1/1 [==============================] - 0s 297ms/step - loss: 0.7949 - accuracy: 0.9035\n",
      "Epoch 128/200\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.7920 - accuracy: 0.8991WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
      "1/1 [==============================] - 0s 296ms/step - loss: 0.7920 - accuracy: 0.8991\n",
      "Epoch 129/200\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.7627 - accuracy: 0.9079WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
      "1/1 [==============================] - 0s 288ms/step - loss: 0.7627 - accuracy: 0.9079\n",
      "Epoch 130/200\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.7434 - accuracy: 0.9123WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
      "1/1 [==============================] - 0s 306ms/step - loss: 0.7434 - accuracy: 0.9123\n",
      "Epoch 131/200\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.7376 - accuracy: 0.9211WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
      "1/1 [==============================] - 0s 291ms/step - loss: 0.7376 - accuracy: 0.9211\n",
      "Epoch 132/200\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.7154 - accuracy: 0.8991WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
      "1/1 [==============================] - 0s 308ms/step - loss: 0.7154 - accuracy: 0.8991\n",
      "Epoch 133/200\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.6903 - accuracy: 0.9167WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
      "1/1 [==============================] - 0s 303ms/step - loss: 0.6903 - accuracy: 0.9167\n",
      "Epoch 134/200\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.6854 - accuracy: 0.9342WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
      "1/1 [==============================] - 0s 324ms/step - loss: 0.6854 - accuracy: 0.9342\n",
      "Epoch 135/200\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.6617 - accuracy: 0.9474WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
      "1/1 [==============================] - 0s 313ms/step - loss: 0.6617 - accuracy: 0.9474\n",
      "Epoch 136/200\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.6550 - accuracy: 0.9079WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
      "1/1 [==============================] - 0s 317ms/step - loss: 0.6550 - accuracy: 0.9079\n",
      "Epoch 137/200\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.6386 - accuracy: 0.9386WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
      "1/1 [==============================] - 0s 315ms/step - loss: 0.6386 - accuracy: 0.9386\n",
      "Epoch 138/200\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.6158 - accuracy: 0.9298WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
      "1/1 [==============================] - 0s 301ms/step - loss: 0.6158 - accuracy: 0.9298\n",
      "Epoch 139/200\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.6093 - accuracy: 0.9518WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
      "1/1 [==============================] - 0s 314ms/step - loss: 0.6093 - accuracy: 0.9518\n",
      "Epoch 140/200\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.5972 - accuracy: 0.9342WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
      "1/1 [==============================] - 0s 331ms/step - loss: 0.5972 - accuracy: 0.9342\n",
      "Epoch 141/200\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.5853 - accuracy: 0.9342WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
      "1/1 [==============================] - 0s 327ms/step - loss: 0.5853 - accuracy: 0.9342\n",
      "Epoch 142/200\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.5755 - accuracy: 0.9342WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
      "1/1 [==============================] - 0s 321ms/step - loss: 0.5755 - accuracy: 0.9342\n",
      "Epoch 143/200\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.5454 - accuracy: 0.9605WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
      "1/1 [==============================] - 0s 326ms/step - loss: 0.5454 - accuracy: 0.9605\n",
      "Epoch 144/200\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.5276 - accuracy: 0.9518WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
      "1/1 [==============================] - 0s 312ms/step - loss: 0.5276 - accuracy: 0.9518\n",
      "Epoch 145/200\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.5239 - accuracy: 0.9737WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
      "1/1 [==============================] - 0s 328ms/step - loss: 0.5239 - accuracy: 0.9737\n",
      "Epoch 146/200\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.5132 - accuracy: 0.9649WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
      "1/1 [==============================] - 0s 326ms/step - loss: 0.5132 - accuracy: 0.9649\n",
      "Epoch 147/200\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.4896 - accuracy: 0.9737WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
      "1/1 [==============================] - 0s 322ms/step - loss: 0.4896 - accuracy: 0.9737\n",
      "Epoch 148/200\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.4861 - accuracy: 0.9605WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
      "1/1 [==============================] - 0s 346ms/step - loss: 0.4861 - accuracy: 0.9605\n",
      "Epoch 149/200\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.4798 - accuracy: 0.9693WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
      "1/1 [==============================] - 0s 340ms/step - loss: 0.4798 - accuracy: 0.9693\n",
      "Epoch 150/200\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.4627 - accuracy: 0.9825WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
      "1/1 [==============================] - 0s 355ms/step - loss: 0.4627 - accuracy: 0.9825\n",
      "Epoch 151/200\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.4515 - accuracy: 0.9781WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
      "1/1 [==============================] - 0s 329ms/step - loss: 0.4515 - accuracy: 0.9781\n",
      "Epoch 152/200\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.4373 - accuracy: 0.9781WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
      "1/1 [==============================] - 0s 328ms/step - loss: 0.4373 - accuracy: 0.9781\n",
      "Epoch 153/200\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.4243 - accuracy: 0.9781WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
      "1/1 [==============================] - 0s 321ms/step - loss: 0.4243 - accuracy: 0.9781\n",
      "Epoch 154/200\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.4002 - accuracy: 0.9868WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
      "1/1 [==============================] - 0s 335ms/step - loss: 0.4002 - accuracy: 0.9868\n",
      "Epoch 155/200\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.4047 - accuracy: 0.9825WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
      "1/1 [==============================] - 0s 310ms/step - loss: 0.4047 - accuracy: 0.9825\n",
      "Epoch 156/200\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3928 - accuracy: 0.9825WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
      "1/1 [==============================] - 0s 323ms/step - loss: 0.3928 - accuracy: 0.9825\n",
      "Epoch 157/200\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3801 - accuracy: 0.9868WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
      "1/1 [==============================] - 0s 329ms/step - loss: 0.3801 - accuracy: 0.9868\n",
      "Epoch 158/200\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3686 - accuracy: 0.9825WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
      "1/1 [==============================] - 0s 314ms/step - loss: 0.3686 - accuracy: 0.9825\n",
      "Epoch 159/200\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3577 - accuracy: 1.0000WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
      "1/1 [==============================] - 0s 315ms/step - loss: 0.3577 - accuracy: 1.0000\n",
      "Epoch 160/200\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3451 - accuracy: 0.9912WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
      "1/1 [==============================] - 0s 306ms/step - loss: 0.3451 - accuracy: 0.9912\n",
      "Epoch 161/200\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3407 - accuracy: 0.9825WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
      "1/1 [==============================] - 0s 310ms/step - loss: 0.3407 - accuracy: 0.9825\n",
      "Epoch 162/200\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3255 - accuracy: 0.9912WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
      "1/1 [==============================] - 0s 300ms/step - loss: 0.3255 - accuracy: 0.9912\n",
      "Epoch 163/200\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3274 - accuracy: 0.9912WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
      "1/1 [==============================] - 0s 296ms/step - loss: 0.3274 - accuracy: 0.9912\n",
      "Epoch 164/200\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3150 - accuracy: 0.9912WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
      "1/1 [==============================] - 0s 280ms/step - loss: 0.3150 - accuracy: 0.9912\n",
      "Epoch 165/200\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2848 - accuracy: 1.0000WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
      "1/1 [==============================] - 0s 302ms/step - loss: 0.2848 - accuracy: 1.0000\n",
      "Epoch 166/200\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2879 - accuracy: 0.9956WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
      "1/1 [==============================] - 0s 293ms/step - loss: 0.2879 - accuracy: 0.9956\n",
      "Epoch 167/200\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3032 - accuracy: 0.9825WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
      "1/1 [==============================] - 0s 297ms/step - loss: 0.3032 - accuracy: 0.9825\n",
      "Epoch 168/200\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2935 - accuracy: 0.9912WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
      "1/1 [==============================] - 0s 286ms/step - loss: 0.2935 - accuracy: 0.9912\n",
      "Epoch 169/200\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2696 - accuracy: 0.9956WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
      "1/1 [==============================] - 0s 297ms/step - loss: 0.2696 - accuracy: 0.9956\n",
      "Epoch 170/200\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2628 - accuracy: 0.9956WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
      "1/1 [==============================] - 0s 292ms/step - loss: 0.2628 - accuracy: 0.9956\n",
      "Epoch 171/200\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2499 - accuracy: 0.9956WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
      "1/1 [==============================] - 0s 303ms/step - loss: 0.2499 - accuracy: 0.9956\n",
      "Epoch 172/200\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2494 - accuracy: 1.0000WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
      "1/1 [==============================] - 0s 297ms/step - loss: 0.2494 - accuracy: 1.0000\n",
      "Epoch 173/200\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2341 - accuracy: 1.0000WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
      "1/1 [==============================] - 0s 293ms/step - loss: 0.2341 - accuracy: 1.0000\n",
      "Epoch 174/200\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2300 - accuracy: 1.0000WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
      "1/1 [==============================] - 0s 293ms/step - loss: 0.2300 - accuracy: 1.0000\n",
      "Epoch 175/200\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2267 - accuracy: 1.0000WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
      "1/1 [==============================] - 0s 290ms/step - loss: 0.2267 - accuracy: 1.0000\n",
      "Epoch 176/200\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2148 - accuracy: 1.0000WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
      "1/1 [==============================] - 0s 290ms/step - loss: 0.2148 - accuracy: 1.0000\n",
      "Epoch 177/200\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2104 - accuracy: 1.0000WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
      "1/1 [==============================] - 0s 302ms/step - loss: 0.2104 - accuracy: 1.0000\n",
      "Epoch 178/200\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2009 - accuracy: 1.0000WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
      "1/1 [==============================] - 0s 297ms/step - loss: 0.2009 - accuracy: 1.0000\n",
      "Epoch 179/200\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1927 - accuracy: 1.0000WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
      "1/1 [==============================] - 0s 283ms/step - loss: 0.1927 - accuracy: 1.0000\n",
      "Epoch 180/200\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1990 - accuracy: 1.0000WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
      "1/1 [==============================] - 0s 299ms/step - loss: 0.1990 - accuracy: 1.0000\n",
      "Epoch 181/200\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1796 - accuracy: 1.0000WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
      "1/1 [==============================] - 0s 295ms/step - loss: 0.1796 - accuracy: 1.0000\n",
      "Epoch 182/200\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1808 - accuracy: 1.0000WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
      "1/1 [==============================] - 0s 311ms/step - loss: 0.1808 - accuracy: 1.0000\n",
      "Epoch 183/200\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1716 - accuracy: 1.0000WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
      "1/1 [==============================] - 0s 290ms/step - loss: 0.1716 - accuracy: 1.0000\n",
      "Epoch 184/200\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1672 - accuracy: 1.0000WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
      "1/1 [==============================] - 0s 310ms/step - loss: 0.1672 - accuracy: 1.0000\n",
      "Epoch 185/200\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1576 - accuracy: 1.0000WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
      "1/1 [==============================] - 0s 308ms/step - loss: 0.1576 - accuracy: 1.0000\n",
      "Epoch 186/200\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1613 - accuracy: 1.0000WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
      "1/1 [==============================] - 0s 291ms/step - loss: 0.1613 - accuracy: 1.0000\n",
      "Epoch 187/200\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1540 - accuracy: 1.0000WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
      "1/1 [==============================] - 0s 302ms/step - loss: 0.1540 - accuracy: 1.0000\n",
      "Epoch 188/200\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1417 - accuracy: 1.0000WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
      "1/1 [==============================] - 0s 291ms/step - loss: 0.1417 - accuracy: 1.0000\n",
      "Epoch 189/200\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1411 - accuracy: 1.0000WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
      "1/1 [==============================] - 0s 304ms/step - loss: 0.1411 - accuracy: 1.0000\n",
      "Epoch 190/200\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1453 - accuracy: 1.0000WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
      "1/1 [==============================] - 0s 308ms/step - loss: 0.1453 - accuracy: 1.0000\n",
      "Epoch 191/200\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1325 - accuracy: 1.0000WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
      "1/1 [==============================] - 0s 304ms/step - loss: 0.1325 - accuracy: 1.0000\n",
      "Epoch 192/200\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1287 - accuracy: 1.0000WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
      "1/1 [==============================] - 0s 308ms/step - loss: 0.1287 - accuracy: 1.0000\n",
      "Epoch 193/200\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1262 - accuracy: 1.0000WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
      "1/1 [==============================] - 0s 290ms/step - loss: 0.1262 - accuracy: 1.0000\n",
      "Epoch 194/200\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1195 - accuracy: 1.0000WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
      "1/1 [==============================] - 0s 306ms/step - loss: 0.1195 - accuracy: 1.0000\n",
      "Epoch 195/200\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1185 - accuracy: 1.0000WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
      "1/1 [==============================] - 0s 288ms/step - loss: 0.1185 - accuracy: 1.0000\n",
      "Epoch 196/200\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1147 - accuracy: 1.0000WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
      "1/1 [==============================] - 0s 292ms/step - loss: 0.1147 - accuracy: 1.0000\n",
      "Epoch 197/200\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1101 - accuracy: 1.0000WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
      "1/1 [==============================] - 0s 295ms/step - loss: 0.1101 - accuracy: 1.0000\n",
      "Epoch 198/200\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1065 - accuracy: 1.0000WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
      "1/1 [==============================] - 0s 287ms/step - loss: 0.1065 - accuracy: 1.0000\n",
      "Epoch 199/200\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1049 - accuracy: 1.0000WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
      "1/1 [==============================] - 0s 304ms/step - loss: 0.1049 - accuracy: 1.0000\n",
      "Epoch 200/200\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1017 - accuracy: 1.0000WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
      "1/1 [==============================] - 0s 293ms/step - loss: 0.1017 - accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from keras import backend as K \n",
    "K.clear_session()  #Resets all state generated by Keras\n",
    "\n",
    "latent_dim = 100\n",
    "embedding_dim = 100\n",
    "\n",
    "# Encoder\n",
    "encoder_inputs = Input(shape=(x.shape[1],))\n",
    "\n",
    "# TODO: understand how embedding works here; is this our own trained embedding? maybe we should just use word2vec\n",
    "#embedding layer\n",
    "enc_emb =  Embedding(1000, embedding_dim,trainable=True, mask_zero = True)(encoder_inputs)\n",
    "\n",
    "#encoder lstm\n",
    "# TODO: why are we not using an activation function? default is none, only for recurrent activation the default is sigmoid\n",
    "# TODO: why do we need the encoder_outputs? only for attention probably\n",
    "encoder_lstm = LSTM(latent_dim,return_sequences=True,return_state=True) #add dropout\n",
    "encoder_outputs, state_h, state_c = encoder_lstm(enc_emb)\n",
    "\n",
    "#Setting up the Decoder using 'encoder_states' as initial state\n",
    "# TODO: figure out why the shape is None? because we also use it later for our decoding when we only give one\n",
    "decoder_inputs = Input(shape=(y.shape[1],))\n",
    "\n",
    "#Embedding layer\n",
    "dec_emb_layer = Embedding(Y_voc+1, embedding_dim,trainable=True, mask_zero = True)\n",
    "dec_emb = dec_emb_layer(decoder_inputs)\n",
    "\n",
    "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True,dropout=0.4,recurrent_dropout=0.2)\n",
    "# TODO: is the LSTM bidirectional? why do we even need those two states?\n",
    "# TODO: why do the graphs say (None, 256) where 256 is the latent_dim (the length of the state vectors); shouldn't it be clear that it is (1, 256)?\n",
    "decoder_outputs ,decoder_fwd_state, decoder_back_state = decoder_lstm(dec_emb,initial_state=[state_h, state_c])\n",
    "\n",
    "# #Attention layer; removed for now\n",
    "# attn_layer = AttentionLayer(name='attention_layer')\n",
    "# attn_out, attn_states = attn_layer([encoder_outputs, decoder_outputs])\n",
    "\n",
    "# #Concating Attention input and Decoder LSTM output\n",
    "# decoder_concat_input = Concatenate(axis=-1, name='concat_layer')([decoder_outputs, attn_out])\n",
    "\n",
    "#Dense layer\n",
    "# TODO: figure out what TimeDistributed does\n",
    "decoder_dense =  TimeDistributed(Dense(units = Y_voc, activation='softmax'))\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "#Defining the model \n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "\n",
    "model.summary()\n",
    "\n",
    "#Visualize the Model\n",
    "from tensorflow.keras.utils import plot_model\n",
    "\n",
    "plot_model(model, to_file='training_model_plot.png', show_shapes=True, show_layer_names=True)\n",
    "\n",
    "# TODO: understand this\n",
    "#Adding Metrics\n",
    "model.compile(optimizer='rmsprop' , loss='sparse_categorical_crossentropy' , metrics=['accuracy'])\n",
    "\n",
    "#Adding Callback\n",
    "# TODO: how exactly does this work? is there an internal mapping of the string \"val_loss\" to running a test on the specified validation_data?\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1)\n",
    "\n",
    "# Commented out IPython magic to ensure Python compatibility.\n",
    "#Training the Model\n",
    "# %tensorflow_version 1.x\n",
    "# indexing is clear, removing the eos token for the decoder inpu\n",
    "# ts and removing the sos token for the decoder outputs\n",
    "# TODO: think about how exactly this is working with calculating the loss etc., maybe that's the problem\n",
    "# TODO: why is y 3-dimensional\n",
    "# TODO: fit only on one example and check if model actually works\n",
    "history = model.fit(x = [x,y], y = y.reshape(1,-1,1) ,epochs=200,callbacks=[es],batch_size = 1)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict = model.predict([x,y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 228, 90)"
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in predict:\n",
    "    print(np.argmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-230-c1baa48f23d5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     89\u001b[0m \u001b[1;31m#for i in range(0,20):\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     90\u001b[0m \u001b[1;31m#print(\"Article:\",seq2text(X_train[1]))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 91\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Original summary:\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mseq2summary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     92\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"article to be decoded:\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m \u001b[1;31m#print(X_train[i].reshape(1,MAX_ARTICLE_LEN))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-230-c1baa48f23d5>\u001b[0m in \u001b[0;36mseq2summary\u001b[1;34m(input_seq)\u001b[0m\n\u001b[0;32m     73\u001b[0m     \u001b[0mnewString\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m''\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0minput_seq\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 75\u001b[1;33m         \u001b[1;32mif\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m!=\u001b[0m\u001b[1;36m0\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m!=\u001b[0m\u001b[0mtarget_word_index\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'sostok'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m!=\u001b[0m\u001b[0mtarget_word_index\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'eostok'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     76\u001b[0m             \u001b[0mnewString\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnewString\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mtarget_index_word\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m' '\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     77\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mnewString\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()"
     ]
    }
   ],
   "source": [
    "\n",
    "encoder_model = Model(inputs=encoder_inputs,outputs=[encoder_outputs, state_h, state_c])\n",
    "\n",
    "decoder_state_input_h = Input(shape=(latent_dim,))\n",
    "decoder_state_input_c = Input(shape=(latent_dim,))\n",
    "\n",
    "dec_emb2= dec_emb_layer(decoder_inputs) \n",
    "\n",
    "#Setting the initial states to the states from the previous time step for better prediction\n",
    "decoder_outputs2, state_h2, state_c2 = decoder_lstm(dec_emb2, initial_state=[decoder_state_input_h, decoder_state_input_c])\n",
    "\n",
    "# TODO: remove this\n",
    "# #Attention inference\n",
    "# attn_out_inf, attn_states_inf = attn_layer([decoder_hidden_state_input, decoder_outputs2])\n",
    "# decoder_inf_concat = Concatenate(axis=-1, name='concat')([decoder_outputs2, attn_out_inf])\n",
    "\n",
    "#Adding Dense softmax layer to generate proability distribution over the target vocabulary\n",
    "decoder_outputs2 = decoder_dense(decoder_outputs2) \n",
    "\n",
    "#Final Decoder model\n",
    "# TODO: how is this all linked to what we trained before? where does the parameter sharing happen?\n",
    "# TODO: why are we appending the lists instead of just writing it in one list\n",
    "decoder_model = Model(\n",
    "    [decoder_inputs] + [decoder_state_input_h, decoder_state_input_c],\n",
    "    [decoder_outputs2] + [state_h2, state_c2])\n",
    "\n",
    "#Function defining the implementation of inference process\n",
    "def decode_sequence(input_seq):\n",
    "    #Encoding the input as state vectors\n",
    "    # TODO: plotting to see if this is actually the trained encoder and it is, but why?\n",
    "    plot_model(encoder_model, to_file='encoder_model_plot.png', show_shapes=True, show_layer_names=True)\n",
    "    e_out, e_h, e_c = encoder_model.predict(input_seq)\n",
    "    \n",
    "    #Generating empty target sequence of length 1\n",
    "    target_seq = np.zeros((1,1))\n",
    "    \n",
    "    #Populating the first word of target sequence with the start word\n",
    "    target_seq[0, 0] = target_word_index['sostok']\n",
    "\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "    while not stop_condition:\n",
    "        \n",
    "        output_tokens, h, c = decoder_model.predict([target_seq] + [e_h, e_c])\n",
    "\n",
    "        #Sampling a token\n",
    "        # TODO: understand the indexing: why -1 instead of 0 as well? should the output tokens be of shape (1, 1, num_words)?\n",
    "        # TODO: I added the plus one because I think the neurons in the dense layer start at 0 but our dictionary starts at 1, despite a one-on-one mapping from\n",
    "        # dictionary index numbers to neurons if I understand correctly\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        print(sampled_token)\n",
    "        input()\n",
    "        sampled_token = target_index_word[sampled_token_index+1]\n",
    "        \n",
    "        if(sampled_token!='eostok'):\n",
    "            decoded_sentence += ' '+sampled_token\n",
    "\n",
    "        #Exit condition: either hit max length or find stop word\n",
    "        # TODO: it used to be >= (max_abstract_len - 1); why? doesn't make sense to me\n",
    "        if (sampled_token == 'eostok'  or len(decoded_sentence.split()) == MAX_ABSTRACT_LEN):\n",
    "            stop_condition = True\n",
    "\n",
    "        #Updating the target sequence (of length 1)\n",
    "        target_seq = np.zeros((1,1))\n",
    "        target_seq[0, 0] = sampled_token_index\n",
    "\n",
    "        #Updating internal states\n",
    "        e_h, e_c = h, c\n",
    "\n",
    "    return decoded_sentence\n",
    "\n",
    "#Functions to convert an integer sequence to a word sequence for summary as well as reviews \n",
    "def seq2summary(input_seq):\n",
    "    newString=''\n",
    "    for i in input_seq:\n",
    "        if((i!=0 and i!=target_word_index['sostok']) and i!=target_word_index['eostok']):\n",
    "            newString=newString+target_index_word[i]+' '\n",
    "    return newString\n",
    "\n",
    "def seq2text(input_seq):\n",
    "    newString=''\n",
    "    for i in input_seq:\n",
    "        if(i!=0):\n",
    "            newString=newString+source_index_word[i]+' '\n",
    "    return newString\n",
    "\n",
    "#Summaries generated by the model\n",
    "\n",
    "# TODO: change this, will crash when there is less than 20\n",
    "#for i in range(0,20):\n",
    "#print(\"Article:\",seq2text(X_train[1]))\n",
    "print(\"Original summary:\",seq2summary(y))\n",
    "print(\"article to be decoded:\")\n",
    "#print(X_train[i].reshape(1,MAX_ARTICLE_LEN))\n",
    "print(\"Predicted summary:\",decode_sequence(x))\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-32-2313bfa3cb78>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'abstract'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'abstract'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m:\u001b[0m \u001b[1;34m'sostok '\u001b[0m\u001b[1;33m+\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' eostok'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m#Splitting the Dataset twice to get 80% training data, 10% of validation data and 10% of test data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mX_val\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_val\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrain_test_split\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'article'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'abstract'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtest_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "df['abstract'] = df['abstract'].apply(lambda x : 'sostok '+ x + ' eostok')\n",
    "\n",
    "#Splitting the Dataset twice to get 80% training data, 10% of validation data and 10% of test data\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train,X_val,y_train,y_val=train_test_split(np.array(df['article']),np.array(df['abstract']),test_size=0.2,random_state=0,shuffle=True)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_val, y_val, test_size = 0.5, random_state = 0, shuffle = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size =200\n",
    "src_txt_length =200\n",
    "sum_txt_length = 200\n",
    "# encoder input model\n",
    "inputs = Input(shape=(src_txt_length,))\n",
    "encoder1 = Embedding(vocab_size, 128)(inputs)\n",
    "encoder2 = LSTM(128)(encoder1)\n",
    "encoder3 = RepeatVector(sum_txt_length)(encoder2)\n",
    "# decoder output model\n",
    "decoder1 = LSTM(128, return_sequences=True)(encoder3)\n",
    "outputs = TimeDistributed(Dense(vocab_size, activation='softmax'))(decoder1)\n",
    "# tie it together\n",
    "model = Model(inputs=inputs, outputs=outputs)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        embedded = self.embedding(input).view(1, 1, -1)\n",
    "        output = embedded\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        output = self.embedding(input).view(1, 1, -1)\n",
    "        output = F.relu(output)\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        output = self.softmax(self.out(output[0]))\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'MAX_LENGTH' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-37-0b46da400ec9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mclass\u001b[0m \u001b[0mAttnDecoderRNN\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhidden_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdropout_p\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mMAX_LENGTH\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m         \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mAttnDecoderRNN\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhidden_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhidden_size\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutput_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moutput_size\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-37-0b46da400ec9>\u001b[0m in \u001b[0;36mAttnDecoderRNN\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mAttnDecoderRNN\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[1;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhidden_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdropout_p\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mMAX_LENGTH\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m         \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mAttnDecoderRNN\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhidden_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhidden_size\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutput_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moutput_size\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'MAX_LENGTH' is not defined"
     ]
    }
   ],
   "source": [
    "class AttnDecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size, dropout_p=0.1, max_length=MAX_LENGTH):\n",
    "        super(AttnDecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.dropout_p = dropout_p\n",
    "        self.max_length = max_length\n",
    "\n",
    "        self.embedding = nn.Embedding(self.output_size, self.hidden_size)\n",
    "        self.attn = nn.Linear(self.hidden_size * 2, self.max_length)\n",
    "        self.attn_combine = nn.Linear(self.hidden_size * 2, self.hidden_size)\n",
    "        self.dropout = nn.Dropout(self.dropout_p)\n",
    "        self.gru = nn.GRU(self.hidden_size, self.hidden_size)\n",
    "        self.out = nn.Linear(self.hidden_size, self.output_size)\n",
    "\n",
    "    def forward(self, input, hidden, encoder_outputs):\n",
    "        embedded = self.embedding(input).view(1, 1, -1)\n",
    "        embedded = self.dropout(embedded)\n",
    "\n",
    "        attn_weights = F.softmax(\n",
    "            self.attn(torch.cat((embedded[0], hidden[0]), 1)), dim=1)\n",
    "        attn_applied = torch.bmm(attn_weights.unsqueeze(0),\n",
    "                                 encoder_outputs.unsqueeze(0))\n",
    "\n",
    "        output = torch.cat((embedded[0], attn_applied[0]), 1)\n",
    "        output = self.attn_combine(output).unsqueeze(0)\n",
    "\n",
    "        output = F.relu(output)\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "\n",
    "        output = F.log_softmax(self.out(output[0]), dim=1)\n",
    "        return output, hidden, attn_weights\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e26081387c8990b949948d3d3fd7f7a4bc3bad723d6b8652b3d4ab339298fb31"
  },
  "kernelspec": {
   "display_name": "Python 3.7.9 64-bit (conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
