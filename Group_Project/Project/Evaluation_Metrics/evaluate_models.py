import json
import pandas as pd

from summ_eval.data_stats_metric      import DataStatsMetric
from summ_eval.rouge_metric           import RougeMetric
from summ_eval.rouge_we_metric        import RougeWeMetric
# Edited version of the summ_eval.meteor_metric file
from meteor_metric                    import MeteorMetric


DATA_PATH = "."
MODEL_OUTPUT_FILE_PATH = DATA_PATH + "/predictions.json"

COMPARE_TO_ORIGINAL_TEXT = 1
COMPARE_TO_REFERENCES = 0

def main():
    
    # Read data into dataframe
    # model_output_data = pd.read_json(MODEL_OUTPUT_FILE_PATH, lines = True)

    # Alternative way to read data into dataframe
    with open(MODEL_OUTPUT_FILE_PATH, "rb") as file:
        rawcontent = file.read()

    content = rawcontent.decode("unicode_escape")[1:-1]     # Get rid of "..."
    lines = content.split("\n")                             # Split the different jsons
    jsons = list(map(lambda line: json.loads(line), lines)) # Load all json strings into json objects
    model_output_data = pd.DataFrame.from_records(jsons)    # Dataframe from json records
    
    # Summaries, References & Original Texts of the Test instances
    summaries      = model_output_data["decoded"].to_list()   # Summaries generated by our model
    original_texts = model_output_data["article"].to_list()   # Full text retrieved by pairing data with original texts
    references     = model_output_data["reference"].to_list() # Reference summaries of this original text also retrieved by pairing data

    # Placeholders
    #summaries      = ["This is the summary for text 1 right", "This is the summary for text 2"]
    #references     = [["This is the reference summary for text 1", "This is another reference summary for text 1"], ["This is the reference summary for text 2"]]
    #original_texts = ["This is the original text 1", "This is the original text 2"]

    # Evaluate the summaries
    evaluate_summaries(summaries, references, original_texts)

def evaluate_summaries(summaries, references, original_texts):
    
    # List the different metrics according to (name, extra info, metric, compare to references/original text)
    metrics = \
        [
            ("DataStatsMetric", "n_gram = 1, 2, 3", DataStatsMetric(n_gram = 3), COMPARE_TO_ORIGINAL_TEXT),
            ("MeteorMetric", "", MeteorMetric(), COMPARE_TO_REFERENCES),
            ("RougeMetric", "", RougeMetric(), COMPARE_TO_REFERENCES),
            ("RougeWeMetric", "n_gram = 1", RougeWeMetric(n_gram = 1), COMPARE_TO_REFERENCES),
            ("RougeWeMetric", "n_gram = 2", RougeWeMetric(n_gram = 2), COMPARE_TO_REFERENCES),
            ("RougeWeMetric", "n_gram = 3", RougeWeMetric(n_gram = 3), COMPARE_TO_REFERENCES)
        ]

    # Initialize output
    metric_outputs = []

    # Loop over the different metrics
    for metric_name, extra_info, metric, compare_to in metrics:
        print("Starting for the metric: " + metric_name)

        # Evaluate metric on the summaries w.r.t. original texts or references
        if compare_to == COMPARE_TO_ORIGINAL_TEXT:
            metric_output = metric.evaluate_batch(summaries, original_texts)
        elif compare_to == COMPARE_TO_REFERENCES:
            metric_output = metric.evaluate_batch(summaries, references)

        # Add to output
        metric_outputs = metric_outputs + [[metric_name, extra_info, metric_output]]
    

    # Write output to file
    print("Writing output to file...")
    metric_outputs_df = pd.DataFrame(data = metric_outputs, columns = ["Metric", "Extra Info", "Output"])
    metric_outputs_df.to_csv("Metrics_output.csv")
    print("Done!")


if __name__ == "__main__":
    main()